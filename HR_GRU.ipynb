{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n"
      ],
      "metadata": {
        "id": "zqckVh8BjsIe"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    # For CUDA\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    # Ensures deterministic behavior for cuDNN\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "lINqfhr_jShn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import re\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import pickle  # --- IMPORT PICKLE ---\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n"
      ],
      "metadata": {
        "id": "7Fa_jdL9rKjP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDcRlHYArX0H",
        "outputId": "c85dea15-de94-4997-a855-d2ad6185ffa6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ema = pd.read_csv(\"/content/drive/MyDrive/CraveMultimodal/data/EMA/CombineEMA.csv\")\n",
        "ema['User_ID'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHUJGOX0rm3X",
        "outputId": "54e6427f-0505-4b3c-d31e-d6829812eac6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['CRS022', 'CRS026', 'CRS020', 'CRS018', 'CRS023', 'CRS021',\n",
              "       'CRS019', 'CRS024', 'CRS017', 'CRS015', 'CRS014', 'CRS016',\n",
              "       'CRS012', 'CRS013', 'CRS011', 'CRS010', 'CRS008', 'CRS007',\n",
              "       'CRS009', 'CRS002', 'CRS001', 'CRS006', 'CRS005', 'CRS003',\n",
              "       'CR021', 'CR010', 'CR001', 'CR008', 'CR006', 'CR004', 'CR016',\n",
              "       'CR005', 'CR022', 'CR025', 'CR003', 'CR020', 'CR002', 'CR018',\n",
              "       'CR027', 'CR015', 'CR009', 'CR011', 'CR012', 'CR031', 'CR032',\n",
              "       'CR035', 'CR023'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ema[\"User_ID\"] = ema[\"User_ID\"].str.lower()"
      ],
      "metadata": {
        "id": "oT5rR_A2Q8tZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_fitbit_hr = pd.read_csv(\"/content/drive/MyDrive/CraveMultimodal/data/Combined_Fitbit/Combined_Fitbit_HR.csv\")"
      ],
      "metadata": {
        "id": "_C8BSMxugAE0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_fitbit_hr[\"date\"] = pd.to_datetime(df_fitbit_hr[\"date\"])\n",
        "df_fitbit_hr['subject']=df_fitbit_hr['subject'].str.lower()\n",
        "df_fitbit_hr.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "QtSsiHy0gduw",
        "outputId": "57dc8c86-3900-41ce-8b60-21717411038d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  subject       date      time  value\n",
              "0   cr001 2021-07-22  00:00:00     64\n",
              "1   cr001 2021-07-22  00:01:00     57\n",
              "2   cr001 2021-07-22  00:02:00     59\n",
              "3   cr001 2021-07-22  00:03:00     96\n",
              "4   cr001 2021-07-22  00:04:00    115"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-abde4610-be57-402c-b398-78faa4d15b96\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>subject</th>\n",
              "      <th>date</th>\n",
              "      <th>time</th>\n",
              "      <th>value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cr001</td>\n",
              "      <td>2021-07-22</td>\n",
              "      <td>00:00:00</td>\n",
              "      <td>64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>cr001</td>\n",
              "      <td>2021-07-22</td>\n",
              "      <td>00:01:00</td>\n",
              "      <td>57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>cr001</td>\n",
              "      <td>2021-07-22</td>\n",
              "      <td>00:02:00</td>\n",
              "      <td>59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>cr001</td>\n",
              "      <td>2021-07-22</td>\n",
              "      <td>00:03:00</td>\n",
              "      <td>96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>cr001</td>\n",
              "      <td>2021-07-22</td>\n",
              "      <td>00:04:00</td>\n",
              "      <td>115</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-abde4610-be57-402c-b398-78faa4d15b96')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-abde4610-be57-402c-b398-78faa4d15b96 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-abde4610-be57-402c-b398-78faa4d15b96');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-49ed4581-0d22-4691-a539-fbab0fe819bc\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-49ed4581-0d22-4691-a539-fbab0fe819bc')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-49ed4581-0d22-4691-a539-fbab0fe819bc button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_fitbit_hr"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_or_trim(x, T):\n",
        "    if len(x) >= T:\n",
        "        return x[:T]\n",
        "    else:\n",
        "        return np.pad(x, (0, T - len(x)), mode='constant')"
      ],
      "metadata": {
        "id": "qucxDcS7_A6D"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_day_sequences = []\n",
        "T = 1420   # The fixed length for the time series (1000 minutes = ~16.7 hours)\n",
        "\n",
        "\n",
        "#  Group the data first by Subject, then by Date\n",
        "subject_day_groups = df_fitbit_hr.groupby(['subject', 'date'])\n",
        "\n",
        "#  Iterate through the groups, extract the sequence, and pad/trim it\n",
        "for (subj, date), group_df in subject_day_groups:\n",
        "    # 'group_df' is the DataFrame slice for one subject on one date\n",
        "\n",
        "    # Extract the heart rate values as a NumPy array\n",
        "    hr_seq = group_df['value'].values\n",
        "\n",
        "    # Apply the padding/trimming function\n",
        "    hr_seq_fixed = pad_or_trim(hr_seq, T)\n",
        "\n",
        "    # Append the fixed-length sequence\n",
        "    all_day_sequences.append(hr_seq_fixed)\n",
        "\n",
        "#  Convert the list of sequences into the final 2D NumPy array\n",
        "all_day_sequences = np.array(all_day_sequences)\n",
        "\n",
        "print(f\"Total number of daily sequences (N_days): {all_day_sequences.shape[0]}\")\n",
        "print(f\"Length of each sequence (T): {all_day_sequences.shape[1]}\")\n",
        "print(f\"Final array shape: {all_day_sequences.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxfyVN3y5JaD",
        "outputId": "94793932-7f57-4e96-96c0-0a8f095b767e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of daily sequences (N_days): 759\n",
            "Length of each sequence (T): 1420\n",
            "Final array shape: (759, 1420)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "class HR_Autoencoder(nn.Module):\n",
        "    def __init__(self, seq_len):\n",
        "        super().__init__()\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        # ---- Encoder ----\n",
        "        self.encoder_conv = nn.Sequential(\n",
        "            nn.Conv1d(1, 16, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2),          # T -> T/2\n",
        "\n",
        "            nn.Conv1d(16, 32, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2),          # T/2 -> T/4\n",
        "        )\n",
        "\n",
        "        # figure out encoded length\n",
        "        with torch.no_grad():\n",
        "            dummy = torch.zeros(1, 1, seq_len)\n",
        "            enc_out = self.encoder_conv(dummy)\n",
        "        self.enc_channels = enc_out.shape[1]   # 32\n",
        "        self.enc_len = enc_out.shape[2]        # T/4\n",
        "\n",
        "        # compress to latent vector\n",
        "        self.fc_enc = nn.Linear(self.enc_channels * self.enc_len, 64)  # latent dim = 64\n",
        "        self.fc_dec = nn.Linear(64, self.enc_channels * self.enc_len)\n",
        "\n",
        "        #  Decoder\n",
        "        self.decoder_conv = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2, mode=\"nearest\"),  # T/4 -> T/2\n",
        "            nn.Conv1d(32, 16, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Upsample(scale_factor=2, mode=\"nearest\"),  # T/2 -> T\n",
        "            nn.Conv1d(16, 1, kernel_size=5, padding=2),\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        # x: (B, 1, T)\n",
        "        h = self.encoder_conv(x)                  # (B, C, L_enc)\n",
        "        h_flat = h.view(h.size(0), -1)            # (B, C * L_enc)\n",
        "        z = self.fc_enc(h_flat)                   # (B, 32)\n",
        "        return z\n",
        "\n",
        "    def decode(self, z):\n",
        "        # z: (B, 32)\n",
        "        h_flat = self.fc_dec(z)                   # (B, C * L_enc)\n",
        "        h = h_flat.view(-1, self.enc_channels, self.enc_len)  # (B, C, L_enc)\n",
        "        x_recon = self.decoder_conv(h)            # (B, 1, ~T)\n",
        "\n",
        "        # ensure output length exactly seq_len (crop or pad)\n",
        "        if x_recon.shape[-1] > self.seq_len:\n",
        "            x_recon = x_recon[..., :self.seq_len]\n",
        "        elif x_recon.shape[-1] < self.seq_len:\n",
        "            pad_len = self.seq_len - x_recon.shape[-1]\n",
        "            x_recon = F.pad(x_recon, (0, pad_len))\n",
        "        return x_recon\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encode(x)\n",
        "        x_recon = self.decode(z)\n",
        "        return x_recon, z\n"
      ],
      "metadata": {
        "id": "4OJFwvQphFDP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class HRDayDataset(Dataset):\n",
        "    def __init__(self, sequences):\n",
        "        # sequences: (N_days, T)\n",
        "        self.sequences = torch.tensor(sequences, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.sequences[idx]      # (T,)\n",
        "        x = x.unsqueeze(0)           # (1, T)  channel dim\n",
        "        return x, x                  # autoencoder: target = input\n"
      ],
      "metadata": {
        "id": "NPVbV0mwiAlD"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_autoencoder(model, dataloader, epochs=20, lr=1e-3, device=\"cpu\"):\n",
        "    model.to(device)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        for batch_x, batch_y in dataloader:\n",
        "            batch_x = batch_x.to(device)  # (B, 1, T)\n",
        "            batch_y = batch_y.to(device)\n",
        "\n",
        "            opt.zero_grad()\n",
        "            x_recon, _ = model(batch_x)\n",
        "            loss = loss_fn(x_recon, batch_y)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "            total_loss += loss.item() * batch_x.size(0)\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader.dataset)\n",
        "        print(f\"Epoch {ep+1}/{epochs}, Loss: {avg_loss:.6f}\")\n"
      ],
      "metadata": {
        "id": "Ihj5Yfo7iIdQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train\n",
        "T = 1420  # or whatever length you padded/trimmed to\n",
        "dataset = HRDayDataset(all_day_sequences)   # shape (N_days, T)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "model = HR_Autoencoder(seq_len=T)\n",
        "train_autoencoder(model, dataloader, epochs=20, lr=1e-3, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8Y0_QIvDq4V",
        "outputId": "11bf40c9-d299-42c0-f210-776fa3f5bad6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 1259.433433\n",
            "Epoch 2/20, Loss: 335.407886\n",
            "Epoch 3/20, Loss: 252.969108\n",
            "Epoch 4/20, Loss: 204.329902\n",
            "Epoch 5/20, Loss: 164.528525\n",
            "Epoch 6/20, Loss: 136.072326\n",
            "Epoch 7/20, Loss: 114.343474\n",
            "Epoch 8/20, Loss: 98.987007\n",
            "Epoch 9/20, Loss: 89.209131\n",
            "Epoch 10/20, Loss: 80.998277\n",
            "Epoch 11/20, Loss: 76.716793\n",
            "Epoch 12/20, Loss: 73.289837\n",
            "Epoch 13/20, Loss: 66.105903\n",
            "Epoch 14/20, Loss: 63.532742\n",
            "Epoch 15/20, Loss: 59.861984\n",
            "Epoch 16/20, Loss: 58.624354\n",
            "Epoch 17/20, Loss: 57.814247\n",
            "Epoch 18/20, Loss: 56.098222\n",
            "Epoch 19/20, Loss: 53.572229\n",
            "Epoch 20/20, Loss: 52.245825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "day_embeddings = {}\n",
        "T = 1420 # Your fixed length\n",
        "\n",
        "#  PyTorch Setup\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "#  Group the data by Subject and then by Date\n",
        "# This efficiently yields the daily sequences you need.\n",
        "subject_day_groups = df_fitbit_hr.groupby(['subject', 'date'])\n",
        "\n",
        "# Iterate through the groups\n",
        "for (subj, date), group_df in subject_day_groups:\n",
        "\n",
        "    # Initialize the inner dictionary for the subject if it's the first time seeing them\n",
        "    if subj not in day_embeddings:\n",
        "        day_embeddings[subj] = {}\n",
        "\n",
        "    #  Extract the heart rate sequence\n",
        "    hr_seq = group_df['value'].values\n",
        "\n",
        "    #  Pad/Trim the sequence to the fixed length T\n",
        "    hr_seq_fixed = pad_or_trim(hr_seq, T)\n",
        "\n",
        "    # Convert sequence to a PyTorch tensor\n",
        "    # .unsqueeze(0).unsqueeze(0) converts (T,) -> (1, 1, T) for a typical 1D CNN/RNN input\n",
        "    x = torch.tensor(hr_seq_fixed, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
        "\n",
        "    #  Run the model to get the embedding (z)\n",
        "    with torch.no_grad():\n",
        "        # Assuming model(x) returns (output, embedding)\n",
        "        _, z = model(x)          # z: (1, 32) (batch size 1, embedding dim 32)\n",
        "\n",
        "    # Store the resulting embedding\n",
        "    # .cpu() moves to CPU, .numpy() converts to numpy array, .squeeze(0) removes the batch dimension\n",
        "    day_embeddings[subj][date] = z.cpu().numpy().squeeze(0)  # Stores the embedding as a (32,) array\n",
        "\n",
        "print(f\"Finished generating embeddings for {len(day_embeddings)} subjects.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOeGcndoinmj",
        "outputId": "f775d052-02b1-4ec5-d40c-1a65b3410209"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished generating embeddings for 35 subjects.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "column_extract = [\"User_ID\",\"Start\",\"End\",\"Positive\",\"Negative\"]\n",
        "df_ema = ema[column_extract]\n",
        "df_ema[\"Start\"] = pd.to_datetime(df_ema[\"Start\"], format=\"mixed\", errors='coerce')\n",
        "df_ema[\"End\"] = pd.to_datetime(df_ema[\"End\"], format= \"mixed\",errors = 'coerce')\n",
        "df_ema.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJmkgu6HFLtk",
        "outputId": "ed5be986-3c89-498e-8bdc-b703013ff4b9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3562 entries, 0 to 3561\n",
            "Data columns (total 5 columns):\n",
            " #   Column    Non-Null Count  Dtype         \n",
            "---  ------    --------------  -----         \n",
            " 0   User_ID   3562 non-null   object        \n",
            " 1   Start     3562 non-null   datetime64[ns]\n",
            " 2   End       3562 non-null   datetime64[ns]\n",
            " 3   Positive  3562 non-null   float64       \n",
            " 4   Negative  3562 non-null   float64       \n",
            "dtypes: datetime64[ns](2), float64(2), object(1)\n",
            "memory usage: 139.3+ KB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3332197160.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_ema[\"Start\"] = pd.to_datetime(df_ema[\"Start\"], format=\"mixed\", errors='coerce')\n",
            "/tmp/ipython-input-3332197160.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_ema[\"End\"] = pd.to_datetime(df_ema[\"End\"], format= \"mixed\",errors = 'coerce')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_ema[\"Date_only\"] = df_ema[\"Start\"].dt.date\n",
        "df_ema[\"Date_only\"] = pd.to_datetime(df_ema[\"Date_only\"])\n",
        "df_ema = df_ema[[\"User_ID\", \"Date_only\",\"Positive\",\"Negative\"]]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBka3yhtFkDU",
        "outputId": "07ff4298-506f-41fa-cec1-8e6611989ad9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2322932470.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_ema[\"Date_only\"] = df_ema[\"Start\"].dt.date\n",
            "/tmp/ipython-input-2322932470.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_ema[\"Date_only\"] = pd.to_datetime(df_ema[\"Date_only\"])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_ema_new = df_ema.copy()\n",
        "\n",
        "def ema_multiclass(row):\n",
        "    p = int(row[\"Positive\"])\n",
        "    n = int(row[\"Negative\"])\n",
        "    if p == 0 and n == 0:\n",
        "        return 0  # neither\n",
        "    elif p == 1 and n == 0:\n",
        "        return 1  # positive only\n",
        "    elif p == 0 and n == 1:\n",
        "        return 2  # negative only\n",
        "    elif p == 1 and n == 1:\n",
        "        return 3  # both\n",
        "    else:\n",
        "        return None  # just in case of weird values\n",
        "\n",
        "df_ema_new[\"EMA_class\"] = df_ema_new.apply(ema_multiclass, axis=1)\n",
        "\n",
        "# Keep only positive-only (1) and negative-only (2)\n",
        "mask_pos_only = df_ema_new[\"EMA_class\"] == 1\n",
        "mask_neg_only = df_ema_new[\"EMA_class\"] == 2\n",
        "\n",
        "df_ema_bin = df_ema_new[mask_pos_only | mask_neg_only].copy()\n",
        "\n",
        "# New binary label:\n",
        "# 1 = positive only, 0 = negative only\n",
        "df_ema_bin[\"EMA_bin\"] = np.where(df_ema_bin[\"EMA_class\"] == 1, 1, 0)\n",
        "df_ema_bin.head()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "co0SFPbIFzhr",
        "outputId": "2213cf1f-1abb-4a18-e180-92941f775c64"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  User_ID  Date_only  Positive  Negative  EMA_class  EMA_bin\n",
              "0  crs022 2025-07-16       1.0       0.0          1        1\n",
              "2  crs022 2025-07-22       1.0       0.0          1        1\n",
              "3  crs022 2025-07-06       1.0       0.0          1        1\n",
              "4  crs022 2025-07-06       1.0       0.0          1        1\n",
              "6  crs022 2025-07-08       1.0       0.0          1        1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e9119184-147d-4bbf-bbda-ed38c130962e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>User_ID</th>\n",
              "      <th>Date_only</th>\n",
              "      <th>Positive</th>\n",
              "      <th>Negative</th>\n",
              "      <th>EMA_class</th>\n",
              "      <th>EMA_bin</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>crs022</td>\n",
              "      <td>2025-07-16</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>crs022</td>\n",
              "      <td>2025-07-22</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>crs022</td>\n",
              "      <td>2025-07-06</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>crs022</td>\n",
              "      <td>2025-07-06</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>crs022</td>\n",
              "      <td>2025-07-08</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e9119184-147d-4bbf-bbda-ed38c130962e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e9119184-147d-4bbf-bbda-ed38c130962e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e9119184-147d-4bbf-bbda-ed38c130962e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-dccc9c91-199d-4eb0-9411-6efcbd2adba4\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dccc9c91-199d-4eb0-9411-6efcbd2adba4')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-dccc9c91-199d-4eb0-9411-6efcbd2adba4 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_ema_bin",
              "summary": "{\n  \"name\": \"df_ema_bin\",\n  \"rows\": 2723,\n  \"fields\": [\n    {\n      \"column\": \"User_ID\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 45,\n        \"samples\": [\n          \"cr015\",\n          \"cr010\",\n          \"cr001\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Date_only\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2021-06-22 00:00:00\",\n        \"max\": \"2025-09-26 00:00:00\",\n        \"num_unique_values\": 618,\n        \"samples\": [\n          \"2025-05-18 00:00:00\",\n          \"2021-08-25 00:00:00\",\n          \"2025-04-21 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Positive\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4944741237543345,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Negative\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4944741237543345,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"EMA_class\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          2,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"EMA_bin\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Build a (subject, date) class mapping\n",
        "\n",
        "#ema_labels_multi = {}\n",
        "ema_labels_bin = {}\n",
        "\n",
        "for subj, df in df_ema_bin.groupby(\"User_ID\"):    #change\n",
        "  ema_labels_bin[subj] = {}                      #change\n",
        "  for _, row in df.iterrows():\n",
        "    date = pd.to_datetime(row[\"Date_only\"]).date()\n",
        "    ema_labels_bin[subj][date] = int(row[\"EMA_bin\"]) #change"
      ],
      "metadata": {
        "id": "mG_2xVM8IHoU"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import date\n",
        "\n",
        "\n",
        "\n",
        "new_day_embeddings = {}\n",
        "\n",
        "#  Iterate through each subject in the outer dictionary\n",
        "for subj, date_data in day_embeddings.items():\n",
        "\n",
        "    # k is the Timestamp, v is the NumPy array\n",
        "    # k.date() extracts the datetime.date object from the Timestamp\n",
        "    converted_date_data = {\n",
        "        k.date(): v\n",
        "        for k, v in date_data.items()\n",
        "    }\n",
        "\n",
        "    # Store the result in the new dictionary\n",
        "    new_day_embeddings[subj] = converted_date_data\n",
        "\n",
        "# The original variable name\n",
        "day_embeddings = new_day_embeddings"
      ],
      "metadata": {
        "id": "PD7xmWWc8YaL"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Binary Prediction using gru with fixed H"
      ],
      "metadata": {
        "id": "Zr2nqIRAfxur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM = 32\n",
        "ZERO_EMBEDDING = np.zeros(EMBEDDING_DIM, dtype=np.float32)\n",
        "\n",
        "def build_dataset_multiclass(day_embeddings, ema_labels_bin, H=7): #change\n",
        "    X_list, y_list, subj_list = [], [], []\n",
        "\n",
        "    for subj in day_embeddings:\n",
        "        if subj not in ema_labels_bin: #change\n",
        "            continue\n",
        "\n",
        "\n",
        "\n",
        "        days = sorted(day_embeddings[subj].keys())\n",
        "\n",
        "        for i in range(H, len(days)):\n",
        "            day_n = days[i]\n",
        "            prev_days = days[i-H:i]\n",
        "\n",
        "            # Target Label Check (Uses the fixed dictionary)\n",
        "            if day_n not in ema_labels_bin[subj]: #change\n",
        "                continue\n",
        "\n",
        "\n",
        "            seq = np.vstack([day_embeddings[subj][d]for d in prev_days])\n",
        "\n",
        "            X_list.append(seq)\n",
        "\n",
        "            y_list.append(ema_labels_bin[subj][day_n]) #change\n",
        "\n",
        "            subj_list.append(subj)\n",
        "\n",
        "    return np.array(X_list), np.array(y_list), np.array(subj_list)"
      ],
      "metadata": {
        "id": "j7mVrR7hSts0"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_mc, y_mc, subj_mc = build_dataset_multiclass(day_embeddings, ema_labels_bin, H=7)\n",
        "print(X_mc.shape, y_mc.shape)\n",
        "input_dim = X_mc.shape[1] * X_mc.shape[2]\n",
        "print(input_dim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpDx8WC2LWZk",
        "outputId": "c36d8dff-d478-4972-9a77-b692106b9143"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(203, 7, 64) (203,)\n",
            "448\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class AffectSequenceDatasetBin(Dataset):\n",
        "    \"\"\"\n",
        "    X: (N, H, 32)  -> per item: (H, 32)\n",
        "    y: (N,) binary labels (0/1)\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)   # (N, H, 32)\n",
        "        self.y = torch.tensor(y, dtype=torch.long)      # (N,)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.X[idx]          # (H, 32), no flatten\n",
        "        y = self.y[idx]\n",
        "        return x, y\n"
      ],
      "metadata": {
        "id": "Vyh26t8liSE5"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GRUAffectClassifier(nn.Module):\n",
        "    def __init__(self, latent_dim=64, hidden_dim=64, num_layers=1, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=latent_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (batch, H, latent_dim)\n",
        "        \"\"\"\n",
        "        out, _ = self.gru(x)         # (batch, H, hidden_dim)\n",
        "        h_last = out[:, -1, :]       # last time step  (batch, hidden_dim)\n",
        "        logits = self.fc(h_last)     # (batch, num_classes)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "8Gw1sqVEiZP9"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    f1_score,\n",
        "    recall_score,\n",
        "    precision_score,\n",
        "    confusion_matrix,\n",
        "    roc_auc_score\n",
        ")\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def evaluate_loso_gru_binary(\n",
        "    X, y, subj_ids,\n",
        "    latent_dim=64,\n",
        "    hidden_dim=64,\n",
        "    num_layers=1,\n",
        "    epochs=30,\n",
        "    batch_size=16,\n",
        "    device=None\n",
        "):\n",
        "    if device is None:\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    unique_subjs = np.unique(subj_ids)\n",
        "    results = []\n",
        "\n",
        "    for test_subj in unique_subjs:\n",
        "        test_mask = (subj_ids == test_subj)\n",
        "        train_mask = ~test_mask\n",
        "\n",
        "        X_train, y_train = X[train_mask], y[train_mask]\n",
        "        X_test,  y_test  = X[test_mask],  y[test_mask]\n",
        "\n",
        "        # we skip if test subject has < 2 samples OR only 1 class (AUC would be degenerate)\n",
        "        if len(y_test) < 2: #or len(np.unique(y_test)) < 2\n",
        "            continue\n",
        "\n",
        "        N_train,H, D = X_train.shape\n",
        "        N_test = X_test.shape[0]\n",
        "\n",
        "        X_train_flat = X_train.reshape(N_train, -1)\n",
        "        X_test_flat = X_test.reshape(N_test, -1)\n",
        "\n",
        "        scalar= StandardScaler()\n",
        "        X_train_scaled = scalar.fit_transform(X_train_flat)\n",
        "        X_test_scaled = scalar.transform(X_test_flat)\n",
        "\n",
        "        #reshape back to (N, H, latent_dim)\n",
        "        X_train = X_train_scaled.reshape(N_train, H, D)\n",
        "        X_test = X_test_scaled.reshape(N_test, H, D)\n",
        "\n",
        "        train_ds = AffectSequenceDatasetBin(X_train, y_train)\n",
        "        test_ds  = AffectSequenceDatasetBin(X_test,  y_test)\n",
        "\n",
        "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "        test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        # class weights (to handle imbalance)\n",
        "        class_counts = np.bincount(y_train, minlength=2).astype(float)\n",
        "        class_counts[class_counts == 0.0] = 1.0\n",
        "        class_weights = 1.0 / class_counts\n",
        "        class_weights = class_weights * (2 / class_weights.sum())\n",
        "        weight_tensor = torch.tensor(class_weights, dtype=torch.float32, device=device)\n",
        "\n",
        "        model = GRUAffectClassifier(\n",
        "            latent_dim=latent_dim,\n",
        "            hidden_dim=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            num_classes=2\n",
        "        ).to(device)\n",
        "\n",
        "        opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "        loss_fn = nn.CrossEntropyLoss(weight=weight_tensor)\n",
        "\n",
        "        # Train\n",
        "        for ep in range(epochs):\n",
        "            model.train()\n",
        "            for bx, by in train_loader:\n",
        "                bx, by = bx.to(device), by.to(device)\n",
        "                opt.zero_grad()\n",
        "                logits = model(bx)              # (batch, 2)\n",
        "                loss = loss_fn(logits, by)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "        # Evaluate\n",
        "        model.eval()\n",
        "        all_logits = []\n",
        "        all_truth = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for bx, by in test_loader:\n",
        "                bx = bx.to(device)\n",
        "                logits = model(bx)\n",
        "                all_logits.append(logits.cpu())\n",
        "                all_truth.append(by)\n",
        "\n",
        "        all_logits = torch.cat(all_logits, dim=0)        # (N_test, 2)\n",
        "        all_truth = torch.cat(all_truth, dim=0).numpy()  # (N_test,)\n",
        "        probs = F.softmax(all_logits, dim=1).numpy()     # (N_test, 2)\n",
        "        preds = probs.argmax(axis=1)                     # (N_test,)\n",
        "        pos_probs = probs[:, 1]                          # for AUC\n",
        "\n",
        "        # Metrics\n",
        "        acc = accuracy_score(all_truth, preds)\n",
        "        f1_macro = f1_score(all_truth, preds, average=\"macro\", zero_division=0)\n",
        "        prec_macro = precision_score(all_truth, preds, average=\"macro\", zero_division=0)\n",
        "        rec_macro = recall_score(all_truth, preds, average=\"macro\", zero_division=0)\n",
        "\n",
        "        # per-class recall [0, 1]\n",
        "        label_list = [0, 1]\n",
        "        rec_per_class = recall_score(\n",
        "            all_truth, preds,\n",
        "            average=None,\n",
        "            labels=label_list,\n",
        "            zero_division=0\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            auc = roc_auc_score(all_truth, pos_probs)\n",
        "        except ValueError:\n",
        "            auc = np.nan\n",
        "\n",
        "        # Confusion matrix\n",
        "        cm = confusion_matrix(all_truth, preds, labels=label_list)\n",
        "\n",
        "        results.append({\n",
        "            \"test_subject\": test_subj,\n",
        "            \"n_test\": len(all_truth),\n",
        "            \"accuracy\": acc,\n",
        "            \"precision_macro\": prec_macro,\n",
        "            \"recall_macro\": rec_macro,\n",
        "            \"f1_macro\": f1_macro,\n",
        "            \"auc\": auc,\n",
        "            \"recall_per_class\": rec_per_class.tolist(),\n",
        "            \"confusion_matrix\": cm.tolist(),\n",
        "            \"y_test_unique\": list(np.unique(all_truth)),\n",
        "            \"class_counts_train\": class_counts.tolist(),\n",
        "            \"class_weights\": class_weights.tolist(),\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results)\n"
      ],
      "metadata": {
        "id": "mtz1PMZaiicB"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "latent_dim = X_mc.shape[2]   # 64\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "loso_gru = evaluate_loso_gru_binary(\n",
        "    X_mc, y_mc, subj_mc,\n",
        "    latent_dim=latent_dim,\n",
        "    hidden_dim=64,\n",
        "    num_layers=1,\n",
        "    epochs=30,\n",
        "    batch_size=16,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(loso_gru)\n",
        "print(\"\\nAverage metrics across subjects:\")\n",
        "print(loso_gru[[\"accuracy\", \"precision_macro\", \"recall_macro\", \"f1_macro\", \"auc\"]].mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbSslw0KiutZ",
        "outputId": "6a63570d-77fe-45a9-f7a2-cae7416fa801"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   test_subject  n_test  accuracy  precision_macro  recall_macro  f1_macro  \\\n",
            "0         cr001      21  0.476190         0.416667      0.263158  0.322581   \n",
            "1         cr003       7  0.285714         0.142857      0.500000  0.222222   \n",
            "2         cr004       2  1.000000         1.000000      1.000000  1.000000   \n",
            "3         cr005      15  1.000000         1.000000      1.000000  1.000000   \n",
            "4         cr006      14  0.642857         0.346154      0.450000  0.391304   \n",
            "5         cr011       7  0.285714         0.500000      0.142857  0.222222   \n",
            "6         cr016      10  0.900000         0.500000      0.450000  0.473684   \n",
            "7         cr031      19  0.684211         0.601190      0.677083  0.592857   \n",
            "8         cr032      11  0.818182         0.500000      0.409091  0.450000   \n",
            "9        crs002      17  0.352941         0.500000      0.176471  0.260870   \n",
            "10       crs007      15  0.000000         0.000000      0.000000  0.000000   \n",
            "11       crs010      20  0.400000         0.600000      0.647059  0.393939   \n",
            "12       crs011      16  0.812500         0.500000      0.406250  0.448276   \n",
            "13       crs014      14  0.642857         0.500000      0.321429  0.391304   \n",
            "14       crs015       8  0.625000         0.566667      0.583333  0.563636   \n",
            "15       crs019       6  0.500000         0.375000      0.300000  0.333333   \n",
            "\n",
            "         auc              recall_per_class   confusion_matrix y_test_unique  \\\n",
            "0   0.289474     [0.0, 0.5263157894736842]  [[0, 2], [9, 10]]        [0, 1]   \n",
            "1   0.600000                    [1.0, 0.0]   [[2, 0], [5, 0]]        [0, 1]   \n",
            "2   1.000000                    [1.0, 1.0]   [[1, 0], [0, 1]]        [0, 1]   \n",
            "3        NaN                    [0.0, 1.0]  [[0, 0], [0, 15]]           [1]   \n",
            "4   0.275000                    [0.0, 0.9]   [[0, 4], [1, 9]]        [0, 1]   \n",
            "5        NaN     [0.2857142857142857, 0.0]   [[2, 5], [0, 0]]           [0]   \n",
            "6        NaN                    [0.0, 0.9]   [[0, 0], [1, 9]]           [1]   \n",
            "7   0.645833  [0.6875, 0.6666666666666666]  [[11, 5], [1, 2]]        [0, 1]   \n",
            "8        NaN     [0.0, 0.8181818181818182]   [[0, 0], [2, 9]]           [1]   \n",
            "9        NaN    [0.35294117647058826, 0.0]  [[6, 11], [0, 0]]           [0]   \n",
            "10       NaN                    [0.0, 0.0]  [[0, 0], [15, 0]]           [1]   \n",
            "11  0.588235    [1.0, 0.29411764705882354]  [[3, 0], [12, 5]]        [0, 1]   \n",
            "12       NaN                 [0.8125, 0.0]  [[13, 3], [0, 0]]           [0]   \n",
            "13       NaN     [0.6428571428571429, 0.0]   [[9, 5], [0, 0]]           [0]   \n",
            "14  0.500000     [0.6666666666666666, 0.5]   [[4, 2], [1, 1]]        [0, 1]   \n",
            "15  0.600000                    [0.0, 0.6]   [[0, 1], [2, 3]]        [0, 1]   \n",
            "\n",
            "   class_counts_train                             class_weights  \n",
            "0        [87.0, 95.0]   [1.043956043956044, 0.9560439560439562]  \n",
            "1       [87.0, 109.0]  [1.1122448979591837, 0.8877551020408163]  \n",
            "2       [88.0, 113.0]  [1.1243781094527363, 0.8756218905472636]  \n",
            "3        [89.0, 99.0]    [1.053191489361702, 0.946808510638298]  \n",
            "4       [85.0, 104.0]  [1.1005291005291005, 0.8994708994708995]  \n",
            "5       [82.0, 114.0]    [1.163265306122449, 0.836734693877551]  \n",
            "6       [89.0, 104.0]    [1.077720207253886, 0.922279792746114]  \n",
            "7       [73.0, 111.0]  [1.2065217391304348, 0.7934782608695653]  \n",
            "8       [89.0, 103.0]  [1.0729166666666667, 0.9270833333333334]  \n",
            "9       [72.0, 114.0]  [1.2258064516129032, 0.7741935483870968]  \n",
            "10       [89.0, 99.0]    [1.053191489361702, 0.946808510638298]  \n",
            "11       [86.0, 97.0]  [1.0601092896174864, 0.9398907103825138]  \n",
            "12      [73.0, 114.0]  [1.2192513368983957, 0.7807486631016043]  \n",
            "13      [75.0, 114.0]  [1.2063492063492063, 0.7936507936507935]  \n",
            "14      [83.0, 112.0]  [1.1487179487179489, 0.8512820512820513]  \n",
            "15      [88.0, 109.0]  [1.1065989847715736, 0.8934010152284264]  \n",
            "\n",
            "Average metrics across subjects:\n",
            "accuracy           0.589135\n",
            "precision_macro    0.503033\n",
            "recall_macro       0.457921\n",
            "f1_macro           0.441639\n",
            "auc                0.562318\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Binary prediction using cumulative n-1 days of HR data only"
      ],
      "metadata": {
        "id": "ykKWGmpGFlup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_dataset_binary_cumulative_hr(day_embeddings, ema_labels_bin):\n",
        "    \"\"\"\n",
        "    HR-only cumulative history.\n",
        "\n",
        "    For each subject and each day_n (from the 2nd day onward):\n",
        "\n",
        "      - Input X_seq: embeddings for ALL past days up to day_{n-1}\n",
        "            shape (T_i, D), where T_i = number of previous days\n",
        "\n",
        "      - Target y: EMA_bin at day_n (0/1)\n",
        "\n",
        "    Returns:\n",
        "      X_list:  length-N object array, each element is array(T_i, D)\n",
        "      y:       (N,) int array (0/1)\n",
        "      subj_ids:(N,) array of subject IDs\n",
        "    \"\"\"\n",
        "    X_list = []\n",
        "    y_list = []\n",
        "    subj_list = []\n",
        "\n",
        "    for subj in day_embeddings:\n",
        "        if subj not in ema_labels_bin:\n",
        "            continue\n",
        "\n",
        "        days = sorted(day_embeddings[subj].keys())\n",
        "\n",
        "        # start at 7 (need at least seven previous day)\n",
        "        for i in range(7, len(days)):\n",
        "            day_n = days[i]           # prediction day\n",
        "            day_prev = days[i-1]      # previous day\n",
        "            history_days = days[:i]  # ALL previous days 0..i-1\n",
        "\n",
        "            # need EMA label for day_n\n",
        "            if day_n not in ema_labels_bin[subj]:\n",
        "                continue\n",
        "\n",
        "            if day_prev not in ema_labels_bin[subj]:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                seq = np.vstack([day_embeddings[subj][d] for d in history_days])\n",
        "                # seq shape: (T_i, D)\n",
        "            except KeyError:\n",
        "                continue\n",
        "\n",
        "            X_list.append(seq)\n",
        "            y_list.append(ema_labels_bin[subj][day_n])\n",
        "            subj_list.append(subj)\n",
        "\n",
        "    X_array = np.array(X_list, dtype=object)       # variable-length\n",
        "    y = np.array(y_list, dtype=int)\n",
        "    subj_ids = np.array(subj_list)\n",
        "\n",
        "    return X_array, y, subj_ids\n"
      ],
      "metadata": {
        "id": "rxhrQ5YaCre5"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class AffectSequenceDatasetBin_Var(Dataset):\n",
        "    \"\"\"\n",
        "    Variable-length HR-only dataset.\n",
        "\n",
        "    X_list: object array/list, each element array(T_i, D)\n",
        "    y:      (N,) 0/1\n",
        "    \"\"\"\n",
        "    def __init__(self, X_list, y):\n",
        "        self.X = [torch.tensor(x, dtype=torch.float32) for x in X_list]\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)  # float for BCE\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "\n",
        "def collate_varlen_bin(batch):\n",
        "    \"\"\"\n",
        "    batch: list of (seq, y_scalar)\n",
        "      seq: (T_i, D)\n",
        "      y_scalar: scalar tensor\n",
        "\n",
        "    Returns:\n",
        "      padded:   (B, T_max, D)  left-padded with zeros\n",
        "      y_batch:  (B,)\n",
        "      lengths:  (B,) original T_i\n",
        "    \"\"\"\n",
        "    seqs, ys = zip(*batch)\n",
        "    lengths = [s.shape[0] for s in seqs]\n",
        "    B = len(seqs)\n",
        "    D = seqs[0].shape[1]\n",
        "    T_max = max(lengths)\n",
        "\n",
        "    padded = torch.zeros(B, T_max, D, dtype=torch.float32)\n",
        "    for i, s in enumerate(seqs):\n",
        "        T_i = s.shape[0]\n",
        "        # left-pad so the most recent day is at the end\n",
        "        padded[i, T_max - T_i:, :] = s\n",
        "\n",
        "    y_batch = torch.stack(ys, dim=0)      # (B,)\n",
        "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
        "\n",
        "    return padded, y_batch, lengths\n"
      ],
      "metadata": {
        "id": "PtzKEgiYCxNN"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "class GRUAffectBinary(nn.Module):\n",
        "    def __init__(self, latent_dim=64, hidden_dim=64, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=latent_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(hidden_dim, 1)  # single logit for BCE\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        \"\"\"\n",
        "        x:       (B, T_max, D)\n",
        "        lengths: (B,)\n",
        "        \"\"\"\n",
        "        packed = pack_padded_sequence(\n",
        "            x, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
        "        )\n",
        "        packed_out, _ = self.gru(packed)\n",
        "        out, _ = pad_packed_sequence(packed_out, batch_first=True)  # (B, T_eff, H)\n",
        "\n",
        "        # last valid timestep\n",
        "        idx = (lengths - 1).unsqueeze(1).unsqueeze(2).expand(-1, 1, out.size(2))\n",
        "        last_h = out.gather(1, idx).squeeze(1)  # (B, H)\n",
        "\n",
        "        last_h = self.dropout(last_h)\n",
        "        logit = self.fc(last_h).squeeze(1)      # (B,)\n",
        "        return logit\n"
      ],
      "metadata": {
        "id": "aFAbMmDVC1yx"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_best_threshold(y_true, y_prob, steps=17):\n",
        "    \"\"\"\n",
        "    y_true: (N,) binary 0/1\n",
        "    y_prob: (N,) predicted probability of class 1\n",
        "    Returns: best_threshold, best_f1\n",
        "    \"\"\"\n",
        "    best_t = 0.5\n",
        "    best_f1 = -1.0\n",
        "    for t in np.linspace(0.1, 0.9, steps):\n",
        "        y_pred = (y_prob >= t).astype(int)\n",
        "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            best_t = t\n",
        "    return best_t, best_f1\n"
      ],
      "metadata": {
        "id": "Nw_t2tbHC9HF"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    f1_score,\n",
        "    recall_score,\n",
        "    precision_score,\n",
        "    roc_auc_score,\n",
        "    confusion_matrix\n",
        ")\n",
        "\n",
        "def evaluate_loso_gru_binary_cumulative_hr(\n",
        "    X_list, y, subj_ids,\n",
        "    latent_dim=64,\n",
        "    hidden_dim=64,\n",
        "    num_layers=1,\n",
        "    epochs=30,\n",
        "    batch_size=16,\n",
        "    device=None\n",
        "):\n",
        "    \"\"\"\n",
        "    X_list: object array/list of length N, each element (T_i, D)\n",
        "    y:      (N,) 0/1\n",
        "    subj_ids: (N,)\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    unique_subjs = np.unique(subj_ids)\n",
        "    results = []\n",
        "\n",
        "    for test_subj in unique_subjs:\n",
        "        test_mask = (subj_ids == test_subj)\n",
        "        train_mask = ~test_mask\n",
        "\n",
        "        X_train_list = X_list[train_mask]\n",
        "        X_test_list  = X_list[test_mask]\n",
        "        y_train      = y[train_mask]\n",
        "        y_test       = y[test_mask]\n",
        "\n",
        "        # need at least 2 test samples\n",
        "        if len(y_test) < 2:\n",
        "            continue\n",
        "\n",
        "        # NORMALIZATION (on training only)\n",
        "        # compute mean/std over all timepoints and dims in training\n",
        "        all_train_frames = np.vstack(X_train_list)  # (sum T_i, D)\n",
        "        mean = all_train_frames.mean(axis=0, keepdims=True)  # (1, D)\n",
        "        std = all_train_frames.std(axis=0, keepdims=True)\n",
        "        std[std == 0] = 1.0\n",
        "\n",
        "        def normalize_seq_list(XL):\n",
        "            out = []\n",
        "            for arr in XL:\n",
        "                arr_norm = (arr - mean) / std\n",
        "                out.append(arr_norm.astype(np.float32))\n",
        "            return np.array(out, dtype=object)\n",
        "\n",
        "        X_train_list_norm = normalize_seq_list(X_train_list)\n",
        "        X_test_list_norm  = normalize_seq_list(X_test_list)\n",
        "\n",
        "\n",
        "        # datasets/loaders\n",
        "        train_ds = AffectSequenceDatasetBin_Var(X_train_list_norm, y_train)\n",
        "        test_ds  = AffectSequenceDatasetBin_Var(X_test_list_norm,  y_test)\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            train_ds, batch_size=batch_size, shuffle=True,\n",
        "            collate_fn=collate_varlen_bin\n",
        "        )\n",
        "        test_loader = DataLoader(\n",
        "            test_ds, batch_size=batch_size, shuffle=False,\n",
        "            collate_fn=collate_varlen_bin\n",
        "        )\n",
        "\n",
        "        # class imbalance for BCE pos_weight\n",
        "        class_counts = np.bincount(y_train.astype(int), minlength=2).astype(float)\n",
        "        if class_counts[1] == 0:\n",
        "            pos_weight_val = 1.0\n",
        "        else:\n",
        "            pos_weight_val = class_counts[0] / class_counts[1]\n",
        "        pos_weight = torch.tensor([pos_weight_val], dtype=torch.float32, device=device)\n",
        "\n",
        "        # latent_dim from first sequence\n",
        "        latent_dim_here = X_train_list_norm[0].shape[1]\n",
        "\n",
        "        model = GRUAffectBinary(\n",
        "            latent_dim=latent_dim_here,\n",
        "            hidden_dim=hidden_dim,\n",
        "            num_layers=num_layers\n",
        "        ).to(device)\n",
        "\n",
        "        opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "        loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "        # TRAIN\n",
        "        for ep in range(epochs):\n",
        "            model.train()\n",
        "            for bx, by, lengths in train_loader:\n",
        "                bx = bx.to(device)         # (B, T_max, D)\n",
        "                by = by.to(device)         # (B,) float\n",
        "                lengths = lengths.to(device)\n",
        "\n",
        "                opt.zero_grad()\n",
        "                logits = model(bx, lengths)  # (B,)\n",
        "                loss = loss_fn(logits, by)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "        # TRAIN PREDICTIONS FOR THRESHOLD\n",
        "        model.eval()\n",
        "        train_logits_all = []\n",
        "        train_truth_all  = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for bx, by, lengths in train_loader:\n",
        "                bx = bx.to(device)\n",
        "                lengths = lengths.to(device)\n",
        "                logits = model(bx, lengths)\n",
        "                train_logits_all.append(logits.cpu())\n",
        "                train_truth_all.append(by)\n",
        "\n",
        "        train_logits_all = torch.cat(train_logits_all, dim=0).numpy()\n",
        "        train_truth_all  = torch.cat(train_truth_all, dim=0).numpy()\n",
        "        train_probs_all  = 1.0 / (1.0 + np.exp(-train_logits_all))  # sigmoid\n",
        "\n",
        "        best_thr, best_f1_train = find_best_threshold(train_truth_all, train_probs_all)\n",
        "\n",
        "        # TEST PREDICTIONS\n",
        "        test_logits_all = []\n",
        "        test_truth_all  = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for bx, by, lengths in test_loader:\n",
        "                bx = bx.to(device)\n",
        "                lengths = lengths.to(device)\n",
        "                logits = model(bx, lengths)\n",
        "                test_logits_all.append(logits.cpu())\n",
        "                test_truth_all.append(by)\n",
        "\n",
        "        test_logits_all = torch.cat(test_logits_all, dim=0).numpy()\n",
        "        test_truth_all  = torch.cat(test_truth_all, dim=0).numpy()\n",
        "        test_probs_all  = 1.0 / (1.0 + np.exp(-test_logits_all))\n",
        "\n",
        "        preds = (test_probs_all >= best_thr).astype(int)\n",
        "        y_int = test_truth_all.astype(int)\n",
        "\n",
        "        # metrics\n",
        "        acc = accuracy_score(y_int, preds)\n",
        "        f1_macro = f1_score(y_int, preds, average=\"macro\", zero_division=0)\n",
        "        prec_macro = precision_score(y_int, preds, average=\"macro\", zero_division=0)\n",
        "        rec_macro = recall_score(y_int, preds, average=\"macro\", zero_division=0)\n",
        "\n",
        "        rec_per_class = recall_score(\n",
        "            y_int, preds,\n",
        "            average=None,\n",
        "            labels=[0, 1],\n",
        "            zero_division=0\n",
        "        )\n",
        "        prec_per_class = precision_score(\n",
        "            y_int, preds,\n",
        "            average=None,\n",
        "            labels=[0, 1],\n",
        "            zero_division=0\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            auc = roc_auc_score(y_int, test_probs_all)\n",
        "        except ValueError:\n",
        "            auc = np.nan\n",
        "\n",
        "        cm = confusion_matrix(y_int, preds, labels=[0, 1])\n",
        "        cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
        "        cm_norm = np.nan_to_num(cm_norm)\n",
        "\n",
        "        results.append({\n",
        "            \"test_subject\": test_subj,\n",
        "            \"n_test\": len(y_int),\n",
        "            \"accuracy\": acc,\n",
        "            \"precision_macro\": prec_macro,\n",
        "            \"recall_macro\": rec_macro,\n",
        "            \"f1_macro\": f1_macro,\n",
        "            \"auc\": auc,\n",
        "            \"recall_per_class\": rec_per_class.tolist(),\n",
        "            \"precision_per_class\": prec_per_class.tolist(),\n",
        "            \"confusion_matrix\": cm_norm.tolist(),\n",
        "            \"y_test_unique\": list(np.unique(y_int)),\n",
        "            \"class_counts_train\": class_counts.tolist(),\n",
        "            \"pos_weight\": float(pos_weight_val),\n",
        "            \"best_threshold\": float(best_thr),\n",
        "            \"train_best_f1\": float(best_f1_train),\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results)\n"
      ],
      "metadata": {
        "id": "INlFgCNzDDcZ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build cumulative HR-only dataset\n",
        "X_hr_cum, y_hr_cum, subj_hr_cum = build_dataset_binary_cumulative_hr(\n",
        "    day_embeddings, ema_labels_bin\n",
        ")\n",
        "\n",
        "set_seed(42)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "loso_hr_cum = evaluate_loso_gru_binary_cumulative_hr(\n",
        "    X_hr_cum, y_hr_cum, subj_hr_cum,\n",
        "    latent_dim=X_hr_cum[0].shape[1],   # D from first sequence\n",
        "    hidden_dim=64,\n",
        "    num_layers=1,\n",
        "    epochs=30,\n",
        "    batch_size=16,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(loso_hr_cum)\n",
        "print(\"\\nAverage metrics:\")\n",
        "print(loso_hr_cum[[\"accuracy\", \"precision_macro\", \"recall_macro\", \"f1_macro\", \"auc\"]].mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwqpCliUDKjf",
        "outputId": "0bbabae6-5c86-49f5-9a11-2799c6903ca5"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-55763866.py:171: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-55763866.py:171: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-55763866.py:171: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-55763866.py:171: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-55763866.py:171: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-55763866.py:171: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-55763866.py:171: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-55763866.py:171: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-55763866.py:171: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   test_subject  n_test  accuracy  precision_macro  recall_macro  f1_macro  \\\n",
            "0         cr001      19  0.315789         0.566667      0.617647  0.308123   \n",
            "1         cr003       7  0.428571         0.458333      0.450000  0.416667   \n",
            "2         cr005      14  0.357143         0.500000      0.178571  0.263158   \n",
            "3         cr006       5  0.400000         0.200000      0.500000  0.285714   \n",
            "4         cr011       4  0.000000         0.000000      0.000000  0.000000   \n",
            "5         cr016       4  1.000000         1.000000      1.000000  1.000000   \n",
            "6         cr031       8  0.250000         0.500000      0.125000  0.200000   \n",
            "7         cr032       9  1.000000         1.000000      1.000000  1.000000   \n",
            "8        crs002      14  0.785714         0.500000      0.392857  0.440000   \n",
            "9        crs007      15  0.266667         0.500000      0.133333  0.210526   \n",
            "10       crs010      17  0.529412         0.600000      0.733333  0.484848   \n",
            "11       crs011      13  0.153846         0.500000      0.076923  0.133333   \n",
            "12       crs014      14  0.214286         0.500000      0.107143  0.176471   \n",
            "13       crs015       4  0.500000         0.250000      0.500000  0.333333   \n",
            "14       crs019       6  0.833333         0.416667      0.500000  0.454545   \n",
            "\n",
            "         auc            recall_per_class         precision_per_class  \\\n",
            "0   0.441176  [1.0, 0.23529411764705882]  [0.13333333333333333, 1.0]   \n",
            "1   0.700000                  [0.5, 0.4]  [0.25, 0.6666666666666666]   \n",
            "2        NaN  [0.0, 0.35714285714285715]                  [0.0, 1.0]   \n",
            "3   0.500000                  [0.0, 1.0]                  [0.0, 0.4]   \n",
            "4        NaN                  [0.0, 0.0]                  [0.0, 0.0]   \n",
            "5        NaN                  [0.0, 1.0]                  [0.0, 1.0]   \n",
            "6        NaN                 [0.25, 0.0]                  [1.0, 0.0]   \n",
            "7        NaN                  [0.0, 1.0]                  [0.0, 1.0]   \n",
            "8        NaN   [0.7857142857142857, 0.0]                  [1.0, 0.0]   \n",
            "9        NaN  [0.0, 0.26666666666666666]                  [0.0, 1.0]   \n",
            "10  0.800000   [1.0, 0.4666666666666667]                  [0.2, 1.0]   \n",
            "11       NaN  [0.15384615384615385, 0.0]                  [1.0, 0.0]   \n",
            "12       NaN  [0.21428571428571427, 0.0]                  [1.0, 0.0]   \n",
            "13  1.000000                  [1.0, 0.0]                  [0.5, 0.0]   \n",
            "14  0.800000                  [0.0, 1.0]   [0.0, 0.8333333333333334]   \n",
            "\n",
            "                                     confusion_matrix y_test_unique  \\\n",
            "0   [[1.0, 0.0], [0.7647058823529411, 0.2352941176...        [0, 1]   \n",
            "1                            [[0.5, 0.5], [0.6, 0.4]]        [0, 1]   \n",
            "2   [[0.0, 0.0], [0.6428571428571429, 0.3571428571...           [1]   \n",
            "3                            [[0.0, 1.0], [0.0, 1.0]]        [0, 1]   \n",
            "4                            [[0.0, 1.0], [0.0, 0.0]]           [0]   \n",
            "5                            [[0.0, 0.0], [0.0, 1.0]]           [1]   \n",
            "6                          [[0.25, 0.75], [0.0, 0.0]]           [0]   \n",
            "7                            [[0.0, 0.0], [0.0, 1.0]]           [1]   \n",
            "8   [[0.7857142857142857, 0.21428571428571427], [0...           [0]   \n",
            "9   [[0.0, 0.0], [0.7333333333333333, 0.2666666666...           [1]   \n",
            "10  [[1.0, 0.0], [0.5333333333333333, 0.4666666666...        [0, 1]   \n",
            "11  [[0.15384615384615385, 0.8461538461538461], [0...           [0]   \n",
            "12  [[0.21428571428571427, 0.7857142857142857], [0...           [0]   \n",
            "13                           [[1.0, 0.0], [1.0, 0.0]]        [0, 1]   \n",
            "14                           [[0.0, 1.0], [0.0, 1.0]]        [0, 1]   \n",
            "\n",
            "   class_counts_train  pos_weight  best_threshold  train_best_f1  \n",
            "0        [63.0, 72.0]    0.875000            0.20       0.782609  \n",
            "1        [63.0, 84.0]    0.750000            0.40       0.823529  \n",
            "2        [65.0, 75.0]    0.866667            0.25       0.797872  \n",
            "3        [62.0, 87.0]    0.712644            0.10       0.816901  \n",
            "4        [61.0, 89.0]    0.685393            0.25       0.827907  \n",
            "5        [65.0, 85.0]    0.764706            0.25       0.809524  \n",
            "6        [57.0, 89.0]    0.640449            0.30       0.859903  \n",
            "7        [65.0, 80.0]    0.812500            0.50       0.800000  \n",
            "8        [51.0, 89.0]    0.573034            0.30       0.839623  \n",
            "9        [65.0, 74.0]    0.878378            0.40       0.783069  \n",
            "10       [63.0, 74.0]    0.851351            0.30       0.791444  \n",
            "11       [52.0, 89.0]    0.584270            0.35       0.842105  \n",
            "12       [51.0, 89.0]    0.573034            0.25       0.851675  \n",
            "13       [63.0, 87.0]    0.724138            0.40       0.834951  \n",
            "14       [64.0, 84.0]    0.761905            0.25       0.811594  \n",
            "\n",
            "Average metrics:\n",
            "accuracy           0.468984\n",
            "precision_macro    0.499444\n",
            "recall_macro       0.420987\n",
            "f1_macro           0.380448\n",
            "auc                0.706863\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GRU With Previous EMA as input feature with cumulative n-1 days\n"
      ],
      "metadata": {
        "id": "8Yrjbf52legH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def build_dataset_binary_cumulative_with_prev_ema(day_embeddings, ema_labels_bin):\n",
        "    \"\"\"\n",
        "    For each subject and each day_n (from the 2nd day onward):\n",
        "\n",
        "      - Input X_seq: embeddings for ALL past days up to day_{n-1}\n",
        "            shape (T_i, D), where T_i = number of previous days\n",
        "\n",
        "      - Extra feature prev_ema: EMA_bin at day_{n-1}  (0/1)\n",
        "\n",
        "      - Target y: EMA_bin at day_n (0/1)\n",
        "\n",
        "    Returns:\n",
        "      X_list:  length-N list of arrays, each of shape (T_i, D)\n",
        "      prev_ema: (N,) float array (0/1)\n",
        "      y:        (N,) int array (0/1)\n",
        "      subj_ids: (N,) array of subject IDs\n",
        "    \"\"\"\n",
        "    X_list = []\n",
        "    prev_ema_list = []\n",
        "    y_list = []\n",
        "    subj_list = []\n",
        "\n",
        "    for subj in day_embeddings:\n",
        "        # must have EMA labels for this subject\n",
        "        if subj not in ema_labels_bin:\n",
        "            continue\n",
        "\n",
        "        # sorted days for which we have embeddings\n",
        "        days = sorted(day_embeddings[subj].keys())\n",
        "\n",
        "        # start from i = 1 (second day), since we need a previous day\n",
        "        for i in range(7, len(days)):\n",
        "            day_n = days[i]        # prediction day\n",
        "            day_prev = days[i-1]   # immediate previous day\n",
        "            history_days = days[:i]  # ALL days before day_n: days[0..i-1]\n",
        "\n",
        "            # need EMA label at day_n and day_prev\n",
        "            if day_n not in ema_labels_bin[subj]:\n",
        "                continue\n",
        "            if day_prev not in ema_labels_bin[subj]:\n",
        "                continue\n",
        "\n",
        "            # build sequence of embeddings for all previous days\n",
        "            try:\n",
        "                seq = np.vstack([day_embeddings[subj][d] for d in history_days])\n",
        "                # seq shape: (T_i, D)\n",
        "            except KeyError:\n",
        "                # if some day is missing in embeddings, skip this sample\n",
        "                continue\n",
        "\n",
        "            X_list.append(seq)\n",
        "            y_list.append(ema_labels_bin[subj][day_n])      # label for day_n\n",
        "            prev_ema_list.append(ema_labels_bin[subj][day_prev])  # EMA of day_{n-1}\n",
        "            subj_list.append(subj)\n",
        "\n",
        "    # variable-length sequences  keep X_list as a Python list (or object array)\n",
        "    X_array = np.array(X_list, dtype=object)  # each element is (T_i, D)\n",
        "    prev_ema = np.array(prev_ema_list, dtype=np.float32)\n",
        "    y = np.array(y_list, dtype=int)\n",
        "    subj_ids = np.array(subj_list)\n",
        "\n",
        "    return X_array, prev_ema, y, subj_ids\n"
      ],
      "metadata": {
        "id": "OYtx6wSnmUIk"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AffectSequenceDatasetBinWithEMA_Var(Dataset):\n",
        "    \"\"\"\n",
        "    Variable-length version.\n",
        "\n",
        "    X_list: list (or array) of length N,\n",
        "            each element is a NumPy array of shape (T_i, D).\n",
        "\n",
        "    prev_ema: (N,) array of 0/1 (yesterday's EMA)\n",
        "    y:        (N,) array of 0/1 (today's EMA)\n",
        "    \"\"\"\n",
        "    def __init__(self, X_list, prev_ema, y):\n",
        "        # store sequences as a list of tensors with varying T_i\n",
        "        self.X = [torch.tensor(x, dtype=torch.float32) for x in X_list]\n",
        "        self.prev_ema = torch.tensor(prev_ema, dtype=torch.float32)  # (N,)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)                # (N,)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # returns (seq, prev_ema_scalar, label)\n",
        "        return self.X[idx], self.prev_ema[idx], self.y[idx]\n"
      ],
      "metadata": {
        "id": "WbZBlac8mlnW"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_varlen_bin_with_ema(batch):\n",
        "    \"\"\"\n",
        "    batch: list of (seq, prev_ema_scalar, y_scalar)\n",
        "      seq: (T_i, D)\n",
        "      prev_ema_scalar: scalar tensor\n",
        "      y_scalar: scalar tensor\n",
        "\n",
        "    Returns:\n",
        "      padded:        (B, T_max, D)  left-padded with zeros\n",
        "      prev_ema_batch:(B,)\n",
        "      y_batch:       (B,)\n",
        "      lengths:       (B,) original T_i\n",
        "    \"\"\"\n",
        "    import torch\n",
        "\n",
        "    seqs, prev_emas, ys = zip(*batch)\n",
        "    lengths = [s.shape[0] for s in seqs]\n",
        "    B = len(seqs)\n",
        "    D = seqs[0].shape[1]\n",
        "    T_max = max(lengths)\n",
        "\n",
        "    padded = torch.zeros(B, T_max, D, dtype=torch.float32)\n",
        "    for i, s in enumerate(seqs):\n",
        "        T_i = s.shape[0]\n",
        "        # left-pad so that the most recent day is at the END of the sequence\n",
        "        padded[i, T_max - T_i:, :] = s\n",
        "\n",
        "    prev_ema_batch = torch.stack(prev_emas, dim=0)  # (B,)\n",
        "    y_batch = torch.stack(ys, dim=0)                # (B,)\n",
        "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
        "\n",
        "    return padded, prev_ema_batch, y_batch, lengths\n"
      ],
      "metadata": {
        "id": "wj9wxnFgnQ4b"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import torch\n",
        "\n",
        "class GRUAffectBinaryWithEMA(nn.Module):\n",
        "    def __init__(self, latent_dim=64, hidden_dim=64, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=latent_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(hidden_dim + 1, 1)  # +1 for prev_ema\n",
        "\n",
        "    def forward(self, x, lengths, prev_ema):\n",
        "        # x: (B, T_max, D)\n",
        "        # lengths: (B,)\n",
        "        # prev_ema: (B,)\n",
        "        packed = pack_padded_sequence(\n",
        "            x, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
        "        )\n",
        "        packed_out, _ = self.gru(packed)\n",
        "        out, _ = pad_packed_sequence(packed_out, batch_first=True)  # (B, T_eff, H)\n",
        "\n",
        "        # last valid timestep\n",
        "        idx = (lengths - 1).unsqueeze(1).unsqueeze(2).expand(-1, 1, out.size(2))\n",
        "        last_h = out.gather(1, idx).squeeze(1)  # (B, H)\n",
        "\n",
        "        last_h = self.dropout(last_h)\n",
        "        prev_ema = prev_ema.unsqueeze(1)        # (B, 1)\n",
        "        h_cat = torch.cat([last_h, prev_ema], dim=1)  # (B, H+1)\n",
        "\n",
        "        logit = self.fc(h_cat).squeeze(1)       # (B,)\n",
        "        return logit\n",
        "\n"
      ],
      "metadata": {
        "id": "gsGGropFnb6s"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def find_best_threshold(y_true, y_prob, steps=17):\n",
        "    \"\"\"\n",
        "    y_true: (N,) binary 0/1\n",
        "    y_prob: (N,) predicted probability of class 1\n",
        "    steps: number of thresholds between 0.1 and 0.9 to scan\n",
        "\n",
        "    Returns: best_threshold, best_f1\n",
        "    \"\"\"\n",
        "    best_t = 0.5\n",
        "    best_f1 = -1.0\n",
        "    for t in np.linspace(0.1, 0.9, steps):\n",
        "        y_pred = (y_prob >= t).astype(int)\n",
        "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            best_t = t\n",
        "    return best_t, best_f1\n"
      ],
      "metadata": {
        "id": "SiX3MXIR4qBJ"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, precision_score, recall_score,\n",
        "    roc_auc_score, confusion_matrix\n",
        ")\n",
        "\n",
        "def evaluate_loso_gru_binary_with_prev_ema_threshold_varlen(\n",
        "    X_list, prev_ema, y, subj_ids,\n",
        "    latent_dim=64,\n",
        "    hidden_dim=64,\n",
        "    num_layers=1,\n",
        "    epochs=30,\n",
        "    batch_size=16,\n",
        "    device=None\n",
        "):\n",
        "    \"\"\"\n",
        "    X_list: np.array (dtype=object) of length N, each element array(T_i, D)\n",
        "    prev_ema: (N,)\n",
        "    y: (N,)  0/1\n",
        "    subj_ids: (N,)\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    unique_subjs = np.unique(subj_ids)\n",
        "    results = []\n",
        "\n",
        "    for test_subj in unique_subjs:\n",
        "        test_mask = (subj_ids == test_subj)\n",
        "        train_mask = ~test_mask\n",
        "\n",
        "        X_train_list = X_list[train_mask]\n",
        "        prev_train   = prev_ema[train_mask]\n",
        "        y_train      = y[train_mask]\n",
        "\n",
        "        X_test_list  = X_list[test_mask]\n",
        "        prev_test    = prev_ema[test_mask]\n",
        "        y_test       = y[test_mask]\n",
        "\n",
        "        # need at least 2 test samples\n",
        "        if len(y_test) < 2:\n",
        "            continue\n",
        "\n",
        "        # datasets / loaders\n",
        "        train_ds = AffectSequenceDatasetBinWithEMA_Var(X_train_list, prev_train, y_train)\n",
        "        test_ds  = AffectSequenceDatasetBinWithEMA_Var(X_test_list,  prev_test,  y_test)\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            train_ds, batch_size=batch_size, shuffle=True,\n",
        "            collate_fn=collate_varlen_bin_with_ema\n",
        "        )\n",
        "        test_loader  = DataLoader(\n",
        "            test_ds, batch_size=batch_size, shuffle=False,\n",
        "            collate_fn=collate_varlen_bin_with_ema\n",
        "        )\n",
        "\n",
        "        # class imbalance (pos_weight for BCE)\n",
        "        class_counts = np.bincount(y_train.astype(int), minlength=2).astype(float)\n",
        "        # class_counts[0] = #neg, class_counts[1] = #pos\n",
        "        if class_counts[1] == 0:\n",
        "            pos_weight_val = 1.0\n",
        "        else:\n",
        "            pos_weight_val = class_counts[0] / class_counts[1]\n",
        "\n",
        "        pos_weight = torch.tensor([pos_weight_val], dtype=torch.float32, device=device)\n",
        "\n",
        "        # model / loss\n",
        "        # infer latent_dim from first sequence\n",
        "        if isinstance(X_list[0], np.ndarray):\n",
        "            latent_dim_here = X_list[0].shape[1]\n",
        "        else:\n",
        "            latent_dim_here = latent_dim\n",
        "\n",
        "        model = GRUAffectBinaryWithEMA(\n",
        "            latent_dim=latent_dim_here,\n",
        "            hidden_dim=hidden_dim,\n",
        "            num_layers=num_layers\n",
        "        ).to(device)\n",
        "\n",
        "        opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "        loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "        #to track loss per epoch for subject\n",
        "        train_loss_per_epoch = []\n",
        "        test_loss_per_epoch = []\n",
        "\n",
        "        # TRAIN\n",
        "        for ep in range(epochs):\n",
        "            model.train()\n",
        "            total_train_loss = 0.0\n",
        "            n_train = 0\n",
        "\n",
        "            for bx, bprev, by, lengths in train_loader:\n",
        "                bx = bx.to(device)             # (B, T_max, D)\n",
        "                bprev = bprev.to(device)       # (B,)\n",
        "                by = by.to(device)             # (B,) float 0/1\n",
        "                lengths = lengths.to(device)\n",
        "\n",
        "                opt.zero_grad()\n",
        "                logits = model(bx, lengths, bprev)  # (B,)\n",
        "                loss = loss_fn(logits, by)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "                total_train_loss += loss.item() * by.size(0)\n",
        "                n_train += by.size(0)\n",
        "\n",
        "\n",
        "            avg_train_loss = total_train_loss / max(n_train, 1)\n",
        "            train_loss_per_epoch.append(avg_train_loss)\n",
        "\n",
        "            # TEST (VAL) LOSS\n",
        "            model.eval()\n",
        "            total_test_loss = 0.0\n",
        "            n_test_loss = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for bx, bprev, by, lengths in test_loader:\n",
        "                    bx = bx.to(device)\n",
        "                    bprev = bprev.to(device)\n",
        "                    lengths = lengths.to(device)\n",
        "                    by = by.to(device)\n",
        "\n",
        "                    logits = model(bx, lengths, bprev)\n",
        "                    loss = loss_fn(logits, by)\n",
        "\n",
        "                    total_test_loss += loss.item() * by.size(0)\n",
        "                    n_test_loss += by.size(0)\n",
        "\n",
        "            avg_test_loss = total_test_loss / max(n_test_loss, 1)\n",
        "            test_loss_per_epoch.append(avg_test_loss)\n",
        "\n",
        "\n",
        "        # TRAIN PREDICTIONS FOR THRESHOLD TUNING\n",
        "        model.eval()\n",
        "        train_logits_all = []\n",
        "        train_truth_all  = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for bx, bprev, by, lengths in train_loader:\n",
        "                bx = bx.to(device)\n",
        "                bprev = bprev.to(device)\n",
        "                lengths = lengths.to(device)\n",
        "                logits = model(bx, lengths, bprev)  # (B,)\n",
        "                train_logits_all.append(logits.cpu())\n",
        "                train_truth_all.append(by)\n",
        "\n",
        "        train_logits_all = torch.cat(train_logits_all, dim=0).numpy()   # (N_train,)\n",
        "        train_truth_all  = torch.cat(train_truth_all, dim=0).numpy()    # (N_train,)\n",
        "        train_probs_all  = 1.0 / (1.0 + np.exp(-train_logits_all))      # sigmoid\n",
        "\n",
        "        best_thr, best_f1_train = find_best_threshold(train_truth_all, train_probs_all)\n",
        "\n",
        "        # TEST PREDICTIONS\n",
        "        test_logits_all = []\n",
        "        test_truth_all  = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for bx, bprev, by, lengths in test_loader:\n",
        "                bx = bx.to(device)\n",
        "                bprev = bprev.to(device)\n",
        "                lengths = lengths.to(device)\n",
        "                logits = model(bx, lengths, bprev)\n",
        "                test_logits_all.append(logits.cpu())\n",
        "                test_truth_all.append(by)\n",
        "\n",
        "        test_logits_all = torch.cat(test_logits_all, dim=0).numpy()  # (N_test,)\n",
        "        test_truth_all  = torch.cat(test_truth_all, dim=0).numpy()   # (N_test,)\n",
        "        test_probs_all  = 1.0 / (1.0 + np.exp(-test_logits_all))\n",
        "\n",
        "        preds = (test_probs_all >= best_thr).astype(int)\n",
        "        y_int = test_truth_all.astype(int)\n",
        "\n",
        "        # metrics\n",
        "        acc = accuracy_score(y_int, preds)\n",
        "        f1_macro = f1_score(y_int, preds, average=\"macro\", zero_division=0)\n",
        "        prec_macro = precision_score(y_int, preds, average=\"macro\", zero_division=0)\n",
        "        rec_macro = recall_score(y_int, preds, average=\"macro\", zero_division=0)\n",
        "\n",
        "        rec_per_class = recall_score(\n",
        "            y_int, preds,\n",
        "            average=None,\n",
        "            labels=[0, 1],\n",
        "            zero_division=0\n",
        "        )\n",
        "        prec_per_class = precision_score(\n",
        "            y_int, preds,\n",
        "            average=None,\n",
        "            labels=[0, 1],\n",
        "            zero_division=0\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            auc = roc_auc_score(y_int, test_probs_all)\n",
        "        except ValueError:\n",
        "            auc = np.nan\n",
        "\n",
        "        cm = confusion_matrix(y_int, preds, labels=[0, 1])\n",
        "        cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
        "        cm_norm = np.nan_to_num(cm_norm)\n",
        "\n",
        "        results.append({\n",
        "            \"test_subject\": test_subj,\n",
        "            \"n_test\": len(y_int),\n",
        "            \"accuracy\": acc,\n",
        "            \"precision_macro\": prec_macro,\n",
        "            \"recall_macro\": rec_macro,\n",
        "            \"f1_macro\": f1_macro,\n",
        "            \"auc\": auc,\n",
        "            \"recall_per_class\": rec_per_class.tolist(),\n",
        "            \"precision_per_class\": prec_per_class.tolist(),\n",
        "            \"confusion_matrix\": cm_norm.tolist(),\n",
        "            \"y_test_unique\": list(np.unique(y_int)),\n",
        "            \"class_counts_train\": class_counts.tolist(),\n",
        "            \"pos_weight\": float(pos_weight_val),\n",
        "            \"best_threshold\": float(best_thr),\n",
        "            \"train_best_f1\": float(best_f1_train),\n",
        "            \"train_loss_curve\": train_loss_per_epoch,\n",
        "            \"test_loss_curve\": test_loss_per_epoch,\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wtmenJlOt2Cy"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "X_bin, prev_bin, y_bin, subj_bin = build_dataset_binary_cumulative_with_prev_ema(\n",
        "    day_embeddings, ema_labels_bin\n",
        ")\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "loso_bin_cum = evaluate_loso_gru_binary_with_prev_ema_threshold_varlen(\n",
        "    X_bin, prev_bin, y_bin, subj_bin,\n",
        "    latent_dim=X_bin[0].shape[1],   # D from first sequence\n",
        "    hidden_dim=64,\n",
        "    num_layers=1,\n",
        "    epochs=30,\n",
        "    batch_size=16,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(loso_bin_cum)\n",
        "\n",
        "print(\"\\nAverage metrics:\")\n",
        "print(loso_bin_cum[[\"accuracy\", \"precision_macro\", \"recall_macro\", \"f1_macro\", \"auc\"]].mean())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeL0GqCGt9xS",
        "outputId": "6e8150bf-979e-4c14-c395-19699856a45a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-3135434405.py:199: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-3135434405.py:199: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-3135434405.py:199: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-3135434405.py:199: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-3135434405.py:199: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-3135434405.py:199: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-3135434405.py:199: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-3135434405.py:199: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-3135434405.py:199: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   test_subject  n_test  accuracy  precision_macro  recall_macro  f1_macro  \\\n",
            "0         cr001      19  0.315789         0.566667      0.617647  0.308123   \n",
            "1         cr003       7  0.571429         0.333333      0.400000  0.363636   \n",
            "2         cr005      14  0.500000         0.500000      0.250000  0.333333   \n",
            "3         cr006       5  0.600000         0.583333      0.583333  0.583333   \n",
            "4         cr011       4  0.250000         0.500000      0.125000  0.200000   \n",
            "5         cr016       4  0.500000         0.500000      0.250000  0.333333   \n",
            "6         cr031       8  0.500000         0.500000      0.250000  0.333333   \n",
            "7         cr032       9  0.444444         0.500000      0.222222  0.307692   \n",
            "8        crs002      14  0.214286         0.500000      0.107143  0.176471   \n",
            "9        crs007      15  0.333333         0.500000      0.166667  0.250000   \n",
            "10       crs010      17  0.588235         0.611111      0.766667  0.529644   \n",
            "11       crs011      13  0.769231         0.500000      0.384615  0.434783   \n",
            "12       crs014      14  0.785714         0.500000      0.392857  0.440000   \n",
            "13       crs015       4  0.500000         0.500000      0.500000  0.500000   \n",
            "14       crs019       6  0.166667         0.083333      0.500000  0.142857   \n",
            "\n",
            "         auc            recall_per_class         precision_per_class  \\\n",
            "0   0.676471  [1.0, 0.23529411764705882]  [0.13333333333333333, 1.0]   \n",
            "1   0.200000                  [0.0, 0.8]   [0.0, 0.6666666666666666]   \n",
            "2        NaN                  [0.0, 0.5]                  [0.0, 1.0]   \n",
            "3   0.833333   [0.6666666666666666, 0.5]   [0.6666666666666666, 0.5]   \n",
            "4        NaN                 [0.25, 0.0]                  [1.0, 0.0]   \n",
            "5        NaN                  [0.0, 0.5]                  [0.0, 1.0]   \n",
            "6        NaN                  [0.5, 0.0]                  [1.0, 0.0]   \n",
            "7        NaN   [0.0, 0.4444444444444444]                  [0.0, 1.0]   \n",
            "8        NaN  [0.21428571428571427, 0.0]                  [1.0, 0.0]   \n",
            "9        NaN   [0.0, 0.3333333333333333]                  [0.0, 1.0]   \n",
            "10  0.833333   [1.0, 0.5333333333333333]   [0.2222222222222222, 1.0]   \n",
            "11       NaN   [0.7692307692307693, 0.0]                  [1.0, 0.0]   \n",
            "12       NaN   [0.7857142857142857, 0.0]                  [1.0, 0.0]   \n",
            "13  0.250000                  [0.5, 0.5]                  [0.5, 0.5]   \n",
            "14  0.200000                  [1.0, 0.0]  [0.16666666666666666, 0.0]   \n",
            "\n",
            "                                     confusion_matrix y_test_unique  \\\n",
            "0   [[1.0, 0.0], [0.7647058823529411, 0.2352941176...        [0, 1]   \n",
            "1                            [[0.0, 1.0], [0.2, 0.8]]        [0, 1]   \n",
            "2                            [[0.0, 0.0], [0.5, 0.5]]           [1]   \n",
            "3   [[0.6666666666666666, 0.3333333333333333], [0....        [0, 1]   \n",
            "4                          [[0.25, 0.75], [0.0, 0.0]]           [0]   \n",
            "5                            [[0.0, 0.0], [0.5, 0.5]]           [1]   \n",
            "6                            [[0.5, 0.5], [0.0, 0.0]]           [0]   \n",
            "7   [[0.0, 0.0], [0.5555555555555556, 0.4444444444...           [1]   \n",
            "8   [[0.21428571428571427, 0.7857142857142857], [0...           [0]   \n",
            "9   [[0.0, 0.0], [0.6666666666666666, 0.3333333333...           [1]   \n",
            "10  [[1.0, 0.0], [0.4666666666666667, 0.5333333333...        [0, 1]   \n",
            "11  [[0.7692307692307693, 0.23076923076923078], [0...           [0]   \n",
            "12  [[0.7857142857142857, 0.21428571428571427], [0...           [0]   \n",
            "13                           [[0.5, 0.5], [0.5, 0.5]]        [0, 1]   \n",
            "14                           [[1.0, 0.0], [1.0, 0.0]]        [0, 1]   \n",
            "\n",
            "   class_counts_train  pos_weight  best_threshold  train_best_f1  \\\n",
            "0        [63.0, 72.0]    0.875000            0.55       0.850746   \n",
            "1        [63.0, 84.0]    0.750000            0.55       0.902439   \n",
            "2        [65.0, 75.0]    0.866667            0.50       0.841379   \n",
            "3        [62.0, 87.0]    0.712644            0.50       0.883721   \n",
            "4        [61.0, 89.0]    0.685393            0.55       0.857143   \n",
            "5        [65.0, 85.0]    0.764706            0.55       0.871166   \n",
            "6        [57.0, 89.0]    0.640449            0.45       0.876404   \n",
            "7        [65.0, 80.0]    0.812500            0.50       0.867925   \n",
            "8        [51.0, 89.0]    0.573034            0.35       0.827907   \n",
            "9        [65.0, 74.0]    0.878378            0.50       0.804734   \n",
            "10       [63.0, 74.0]    0.851351            0.55       0.873239   \n",
            "11       [52.0, 89.0]    0.584270            0.50       0.921348   \n",
            "12       [51.0, 89.0]    0.573034            0.50       0.892655   \n",
            "13       [63.0, 87.0]    0.724138            0.50       0.891429   \n",
            "14       [64.0, 84.0]    0.761905            0.55       0.890244   \n",
            "\n",
            "                                     train_loss_curve  \\\n",
            "0   [0.6719490192554615, 0.6479902112925494, 0.627...   \n",
            "1   [0.572354587162433, 0.5683332209278937, 0.5494...   \n",
            "2   [0.6210175769669669, 0.5993969661848886, 0.586...   \n",
            "3   [0.599067290957342, 0.5783172061779355, 0.5566...   \n",
            "4   [0.5903940625985463, 0.5443071762720744, 0.524...   \n",
            "5   [0.596719680627187, 0.5870603585243225, 0.5511...   \n",
            "6   [0.5393550869536726, 0.5218365421033886, 0.496...   \n",
            "7   [0.6145019301052751, 0.5843710356745226, 0.601...   \n",
            "8   [0.5107259971754892, 0.524915863786425, 0.4956...   \n",
            "9   [0.7018090405052514, 0.6665031635503975, 0.655...   \n",
            "10  [0.6445148769956436, 0.6226157794033524, 0.606...   \n",
            "11  [0.5217218944366943, 0.48864497741063434, 0.46...   \n",
            "12  [0.5052476355007717, 0.48470770120620726, 0.48...   \n",
            "13  [0.567150571346283, 0.5271110184987386, 0.5266...   \n",
            "14  [0.5965072029345745, 0.5961522033085694, 0.552...   \n",
            "\n",
            "                                      test_loss_curve  \n",
            "0   [0.5741121768951416, 0.5852757190403185, 0.628...  \n",
            "1   [0.5283573865890503, 0.5300971269607544, 0.535...  \n",
            "2   [0.7067575454711914, 0.7246794104576111, 0.734...  \n",
            "3   [0.6531069874763489, 0.6061691045761108, 0.608...  \n",
            "4   [0.6361402273178101, 0.6789477467536926, 0.673...  \n",
            "5   [0.5025976300239563, 0.47103017568588257, 0.47...  \n",
            "6   [0.7125586867332458, 0.7592278718948364, 0.827...  \n",
            "7   [0.5124155282974243, 0.5612305998802185, 0.589...  \n",
            "8   [0.7122082710266113, 0.711523175239563, 0.6989...  \n",
            "9   [0.49303948879241943, 0.5539494752883911, 0.60...  \n",
            "10  [0.5784566472558414, 0.5768004200037788, 0.597...  \n",
            "11  [0.6995925307273865, 0.7301250100135803, 0.719...  \n",
            "12  [0.6365379691123962, 0.58372962474823, 0.58631...  \n",
            "13  [0.679036021232605, 0.7336740493774414, 0.7402...  \n",
            "14  [0.6399168968200684, 0.6159611344337463, 0.650...  \n",
            "\n",
            "Average metrics:\n",
            "accuracy           0.469275\n",
            "precision_macro    0.478519\n",
            "recall_macro       0.367743\n",
            "f1_macro           0.349103\n",
            "auc                0.498856\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_curves = np.array(loso_bin_cum[\"train_loss_curve\"].tolist())  # shape (S, E)\n",
        "test_curves  = np.array(loso_bin_cum[\"test_loss_curve\"].tolist())   # shape (S, E)\n",
        "\n",
        "# Compute mean across subjects\n",
        "train_mean = np.nanmean(train_curves, axis=0)\n",
        "test_mean  = np.nanmean(test_curves, axis=0)\n",
        "\n",
        "epochs = np.arange(1, len(train_mean)+1)"
      ],
      "metadata": {
        "id": "-8nEsssJ-_54"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plt.figure(figsize=(8,5))\n",
        "#plt.plot(epochs, train_mean, label=\"Train Loss (avg across subjects)\", linewidth=2)\n",
        "#plt.plot(epochs, test_mean,  label=\"Test Loss (avg across subjects)\", linewidth=2)\n",
        "\n",
        "#plt.xlabel(\"Epoch\")\n",
        "#plt.ylabel(\"Loss (BCEWithLogits)\")\n",
        "#plt.title(\"Global Train/Test Loss Curves  GRU (cumulative + prev EMA)\")\n",
        "#plt.legend()\n",
        "#plt.grid(True)\n",
        "#plt.show()"
      ],
      "metadata": {
        "id": "qM3H8vnN_FCy"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "HR + Prev EMA with fixed H"
      ],
      "metadata": {
        "id": "7Mf2yu6ULcy-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_dataset_binary_with_prev_ema(day_embeddings, ema_labels_bin, H=7):\n",
        "    \"\"\"\n",
        "    For each subject:\n",
        "      - Input X: (H, 32) embeddings for days [n-H, ..., n-1]\n",
        "      - Extra feature prev_ema: EMA_bin at day (n-1)\n",
        "      - Target y: EMA_bin at day n\n",
        "    Returns:\n",
        "      X: (N, H, 32)\n",
        "      prev_ema: (N,)\n",
        "      y: (N,)\n",
        "      subj_ids: (N,)\n",
        "    \"\"\"\n",
        "    X_list, prev_ema_list, y_list, subj_list = [], [], [], []\n",
        "\n",
        "    for subj in day_embeddings:\n",
        "        if subj not in ema_labels_bin:\n",
        "            continue\n",
        "\n",
        "        days = sorted(day_embeddings[subj].keys())\n",
        "\n",
        "        for i in range(H, len(days)):\n",
        "            day_n = days[i]          # prediction day\n",
        "            prev_days = days[i-H:i]  # H previous days\n",
        "            day_prev = days[i-1]     # immediate previous day\n",
        "\n",
        "            # need label at day n and day_prev\n",
        "            if day_n not in ema_labels_bin[subj]:\n",
        "                continue\n",
        "            if day_prev not in ema_labels_bin[subj]:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                seq = np.vstack([day_embeddings[subj][d] for d in prev_days])  # (H, 32)\n",
        "            except KeyError:\n",
        "                continue\n",
        "\n",
        "            X_list.append(seq)\n",
        "            y_list.append(ema_labels_bin[subj][day_n])\n",
        "            prev_ema_list.append(ema_labels_bin[subj][day_prev])\n",
        "            subj_list.append(subj)\n",
        "\n",
        "    X = np.array(X_list, dtype=np.float32)\n",
        "    prev_ema = np.array(prev_ema_list, dtype=np.float32)\n",
        "    y = np.array(y_list, dtype=int)\n",
        "    subj_ids = np.array(subj_list)\n",
        "\n",
        "    return X, prev_ema, y, subj_ids\n"
      ],
      "metadata": {
        "id": "Xl_qhx4pllWm"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "H = 7\n",
        "X_ema, prev_ema_arr, y_ema, subj_ema = build_dataset_binary_with_prev_ema(\n",
        "    day_embeddings, ema_labels_bin, H=H\n",
        ")\n",
        "print(X_ema.shape, prev_ema_arr.shape, y_ema.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3uFb5pOlpuB",
        "outputId": "c81b2b2e-57d2-44ff-9d71-961743eabc95"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(154, 7, 64) (154,) (154,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AffectSequenceDatasetBinWithEMA(Dataset):\n",
        "    \"\"\"\n",
        "    X: (N, H, 32)\n",
        "    prev_ema: (N,) scalar 0/1 (yesterday's affect)\n",
        "    y: (N,)\n",
        "    \"\"\"\n",
        "    def __init__(self, X, prev_ema, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)          # (N, H, 32)\n",
        "        self.prev_ema = torch.tensor(prev_ema, dtype=torch.float32)  # (N,)\n",
        "        self.y = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.prev_ema[idx], self.y[idx]\n"
      ],
      "metadata": {
        "id": "SXFXw1byl7yu"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GRUAffectClassifierWithEMA(nn.Module):\n",
        "    def __init__(self, latent_dim=64, hidden_dim=64, num_layers=1, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=latent_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim + 1, num_classes)  # +1 for prev_ema scalar\n",
        "\n",
        "    def forward(self, x, prev_ema):\n",
        "        \"\"\"\n",
        "        x: (batch, H, latent_dim)\n",
        "        prev_ema: (batch,) scalar 0/1\n",
        "        \"\"\"\n",
        "        out, _ = self.gru(x)         # (batch, H, hidden_dim)\n",
        "        h_last = out[:, -1, :]       # (batch, hidden_dim)\n",
        "        prev_ema = prev_ema.unsqueeze(-1)    # (batch, 1)\n",
        "        feat = torch.cat([h_last, prev_ema], dim=1)  # (batch, hidden_dim + 1)\n",
        "        logits = self.fc(feat)                   # (batch, num_classes)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "UHmE1iVVmInd"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_loso_gru_binary_with_prev_ema(\n",
        "    X, prev_ema, y, subj_ids,\n",
        "    latent_dim=64,\n",
        "    hidden_dim=64,\n",
        "    num_layers=1,\n",
        "    epochs=30,\n",
        "    batch_size=16,\n",
        "    device=None\n",
        "):\n",
        "    if device is None:\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    unique_subjs = np.unique(subj_ids)\n",
        "    results = []\n",
        "\n",
        "    for test_subj in unique_subjs:\n",
        "        test_mask = (subj_ids == test_subj)\n",
        "        train_mask = ~test_mask\n",
        "\n",
        "        X_train, prev_train, y_train = X[train_mask], prev_ema[train_mask], y[train_mask]\n",
        "        X_test,  prev_test,  y_test  = X[test_mask],  prev_ema[test_mask],  y[test_mask]\n",
        "\n",
        "        if len(y_test) < 2: #or len(np.unique(y_test)) < 2:\n",
        "            continue\n",
        "\n",
        "\n",
        "\n",
        "        train_ds = AffectSequenceDatasetBinWithEMA(X_train, prev_train, y_train)\n",
        "        test_ds  = AffectSequenceDatasetBinWithEMA(X_test,  prev_test,  y_test)\n",
        "\n",
        "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "        test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        # class weights\n",
        "        class_counts = np.bincount(y_train, minlength=2).astype(float)\n",
        "        class_counts[class_counts == 0.0] = 1.0\n",
        "        class_weights = 1.0 / class_counts\n",
        "        class_weights = class_weights * (2 / class_weights.sum())\n",
        "        weight_tensor = torch.tensor(class_weights, dtype=torch.float32, device=device)\n",
        "\n",
        "        model = GRUAffectClassifierWithEMA(\n",
        "            latent_dim=latent_dim,\n",
        "            hidden_dim=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            num_classes=2\n",
        "        ).to(device)\n",
        "\n",
        "        opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "        loss_fn = nn.CrossEntropyLoss(weight=weight_tensor)\n",
        "\n",
        "        # Train\n",
        "        for ep in range(epochs):\n",
        "            model.train()\n",
        "            for bx, bprev, by in train_loader:\n",
        "                bx, bprev, by = bx.to(device), bprev.to(device), by.to(device)\n",
        "                opt.zero_grad()\n",
        "                logits = model(bx, bprev)\n",
        "                loss = loss_fn(logits, by)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "        # Evaluate\n",
        "        model.eval()\n",
        "        all_logits = []\n",
        "        all_truth = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for bx, bprev, by in test_loader:\n",
        "                bx, bprev = bx.to(device), bprev.to(device)\n",
        "                logits = model(bx, bprev)\n",
        "                all_logits.append(logits.cpu())\n",
        "                all_truth.append(by)\n",
        "\n",
        "        all_logits = torch.cat(all_logits, dim=0)\n",
        "        all_truth = torch.cat(all_truth, dim=0).numpy()\n",
        "        probs = F.softmax(all_logits, dim=1).numpy()\n",
        "        preds = probs.argmax(axis=1)\n",
        "        pos_probs = probs[:, 1]\n",
        "\n",
        "        acc = accuracy_score(all_truth, preds)\n",
        "        f1_macro = f1_score(all_truth, preds, average=\"macro\", zero_division=0)\n",
        "        prec_macro = precision_score(all_truth, preds, average=\"macro\", zero_division=0)\n",
        "        rec_macro = recall_score(all_truth, preds, average=\"macro\", zero_division=0)\n",
        "\n",
        "        label_list = [0, 1]\n",
        "        rec_per_class = recall_score(\n",
        "            all_truth, preds,\n",
        "            average=None,\n",
        "            labels=label_list,\n",
        "            zero_division=0\n",
        "        )\n",
        "        prec_per_class = precision_score(\n",
        "            all_truth, preds,\n",
        "            average=None,\n",
        "            labels=label_list,\n",
        "            zero_division=0\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            auc = roc_auc_score(all_truth, pos_probs)\n",
        "        except ValueError:\n",
        "            auc = np.nan\n",
        "\n",
        "        cm = confusion_matrix(all_truth, preds, labels=label_list)\n",
        "        cm_norm = cm.astype('float') / cm.sum(axis = 1, keepdims=True)\n",
        "        cm_norm = np.nan_to_num(cm_norm)\n",
        "\n",
        "        results.append({\n",
        "            \"test_subject\": test_subj,\n",
        "            \"n_test\": len(all_truth),\n",
        "            \"accuracy\": acc,\n",
        "            \"precision_macro\": prec_macro,\n",
        "            \"recall_macro\": rec_macro,\n",
        "            \"f1_macro\": f1_macro,\n",
        "            \"auc\": auc,\n",
        "            \"recall_per_class\": rec_per_class.tolist(),\n",
        "            \"precision_per_class\": prec_per_class.tolist(),\n",
        "            \"confusion_matrix\": cm_norm.tolist(),\n",
        "            \"y_test_unique\": list(np.unique(all_truth)),\n",
        "            \"class_counts_train\": class_counts.tolist(),\n",
        "            \"class_weights\": class_weights.tolist(),\n",
        "        })\n",
        "\n",
        "\n",
        "    return pd.DataFrame(results)\n"
      ],
      "metadata": {
        "id": "3-DPDW44mPam"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "H = 7\n",
        "X_ema, prev_ema_arr, y_ema, subj_ema = build_dataset_binary_with_prev_ema(\n",
        "    day_embeddings, ema_labels_bin, H=H\n",
        ")\n",
        "latent_dim = X_ema.shape[2]\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "loso_gru_ema = evaluate_loso_gru_binary_with_prev_ema(\n",
        "    X_ema, prev_ema_arr, y_ema, subj_ema,\n",
        "    latent_dim=latent_dim,\n",
        "    hidden_dim=64,\n",
        "    num_layers=1,\n",
        "    epochs=30,\n",
        "    batch_size=16,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(loso_gru_ema)\n",
        "print(\"\\nAverage metrics across subjects (HR + prev EMA):\")\n",
        "print(loso_gru_ema[[\"accuracy\", \"precision_macro\", \"recall_macro\", \"f1_macro\", \"auc\"]].mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0iVOwy0mcds",
        "outputId": "f979ccc3-2afb-423d-bcb5-113541c53be0"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-4125017975.py:105: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis = 1, keepdims=True)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-4125017975.py:105: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis = 1, keepdims=True)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-4125017975.py:105: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis = 1, keepdims=True)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-4125017975.py:105: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis = 1, keepdims=True)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-4125017975.py:105: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis = 1, keepdims=True)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-4125017975.py:105: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis = 1, keepdims=True)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-4125017975.py:105: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis = 1, keepdims=True)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-4125017975.py:105: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis = 1, keepdims=True)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-4125017975.py:105: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis = 1, keepdims=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   test_subject  n_test  accuracy  precision_macro  recall_macro  f1_macro  \\\n",
            "0         cr001      19  0.473684         0.409091      0.264706  0.321429   \n",
            "1         cr003       7  0.714286         0.357143      0.500000  0.416667   \n",
            "2         cr005      14  0.357143         0.500000      0.178571  0.263158   \n",
            "3         cr006       5  0.400000         0.416667      0.416667  0.400000   \n",
            "4         cr011       4  0.250000         0.500000      0.125000  0.200000   \n",
            "5         cr016       4  1.000000         1.000000      1.000000  1.000000   \n",
            "6         cr031       8  0.625000         0.500000      0.312500  0.384615   \n",
            "7         cr032       9  0.777778         0.500000      0.388889  0.437500   \n",
            "8        crs002      14  0.642857         0.500000      0.321429  0.391304   \n",
            "9        crs007      15  0.266667         0.500000      0.133333  0.210526   \n",
            "10       crs010      17  0.411765         0.583333      0.666667  0.392857   \n",
            "11       crs011      13  0.230769         0.500000      0.115385  0.187500   \n",
            "12       crs014      14  0.571429         0.500000      0.285714  0.363636   \n",
            "13       crs015       4  0.250000         0.166667      0.250000  0.200000   \n",
            "14       crs019       6  0.500000         0.375000      0.300000  0.333333   \n",
            "\n",
            "         auc            recall_per_class         precision_per_class  \\\n",
            "0   0.323529   [0.0, 0.5294117647058824]   [0.0, 0.8181818181818182]   \n",
            "1   0.400000                  [0.0, 1.0]   [0.0, 0.7142857142857143]   \n",
            "2        NaN  [0.0, 0.35714285714285715]                  [0.0, 1.0]   \n",
            "3   0.333333   [0.3333333333333333, 0.5]   [0.5, 0.3333333333333333]   \n",
            "4        NaN                 [0.25, 0.0]                  [1.0, 0.0]   \n",
            "5        NaN                  [0.0, 1.0]                  [0.0, 1.0]   \n",
            "6        NaN                [0.625, 0.0]                  [1.0, 0.0]   \n",
            "7        NaN   [0.0, 0.7777777777777778]                  [0.0, 1.0]   \n",
            "8        NaN   [0.6428571428571429, 0.0]                  [1.0, 0.0]   \n",
            "9        NaN  [0.0, 0.26666666666666666]                  [0.0, 1.0]   \n",
            "10  0.733333   [1.0, 0.3333333333333333]  [0.16666666666666666, 1.0]   \n",
            "11       NaN  [0.23076923076923078, 0.0]                  [1.0, 0.0]   \n",
            "12       NaN   [0.5714285714285714, 0.0]                  [1.0, 0.0]   \n",
            "13  0.000000                  [0.5, 0.0]   [0.3333333333333333, 0.0]   \n",
            "14  0.600000                  [0.0, 0.6]                 [0.0, 0.75]   \n",
            "\n",
            "                                     confusion_matrix y_test_unique  \\\n",
            "0   [[0.0, 1.0], [0.47058823529411764, 0.529411764...        [0, 1]   \n",
            "1                            [[0.0, 1.0], [0.0, 1.0]]        [0, 1]   \n",
            "2   [[0.0, 0.0], [0.6428571428571429, 0.3571428571...           [1]   \n",
            "3   [[0.3333333333333333, 0.6666666666666666], [0....        [0, 1]   \n",
            "4                          [[0.25, 0.75], [0.0, 0.0]]           [0]   \n",
            "5                            [[0.0, 0.0], [0.0, 1.0]]           [1]   \n",
            "6                        [[0.625, 0.375], [0.0, 0.0]]           [0]   \n",
            "7   [[0.0, 0.0], [0.2222222222222222, 0.7777777777...           [1]   \n",
            "8   [[0.6428571428571429, 0.35714285714285715], [0...           [0]   \n",
            "9   [[0.0, 0.0], [0.7333333333333333, 0.2666666666...           [1]   \n",
            "10  [[1.0, 0.0], [0.6666666666666666, 0.3333333333...        [0, 1]   \n",
            "11  [[0.23076923076923078, 0.7692307692307693], [0...           [0]   \n",
            "12  [[0.5714285714285714, 0.42857142857142855], [0...           [0]   \n",
            "13                           [[0.5, 0.5], [1.0, 0.0]]        [0, 1]   \n",
            "14                           [[0.0, 1.0], [0.4, 0.6]]        [0, 1]   \n",
            "\n",
            "   class_counts_train                             class_weights  \n",
            "0        [63.0, 72.0]  [1.0666666666666667, 0.9333333333333333]  \n",
            "1        [63.0, 84.0]  [1.1428571428571428, 0.8571428571428571]  \n",
            "2        [65.0, 75.0]  [1.0714285714285714, 0.9285714285714286]  \n",
            "3        [62.0, 87.0]   [1.1677852348993287, 0.832214765100671]  \n",
            "4        [61.0, 89.0]  [1.1866666666666668, 0.8133333333333334]  \n",
            "5        [65.0, 85.0]  [1.1333333333333335, 0.8666666666666667]  \n",
            "6        [57.0, 89.0]  [1.2191780821917808, 0.7808219178082192]  \n",
            "7        [65.0, 80.0]    [1.103448275862069, 0.896551724137931]  \n",
            "8        [51.0, 89.0]  [1.2714285714285714, 0.7285714285714285]  \n",
            "9        [65.0, 74.0]   [1.064748201438849, 0.9352517985611511]  \n",
            "10       [63.0, 74.0]  [1.0802919708029197, 0.9197080291970804]  \n",
            "11       [52.0, 89.0]  [1.2624113475177308, 0.7375886524822696]  \n",
            "12       [51.0, 89.0]  [1.2714285714285714, 0.7285714285714285]  \n",
            "13       [63.0, 87.0]                              [1.16, 0.84]  \n",
            "14       [64.0, 84.0]   [1.135135135135135, 0.8648648648648648]  \n",
            "\n",
            "Average metrics across subjects (HR + prev EMA):\n",
            "accuracy           0.498092\n",
            "precision_macro    0.487193\n",
            "recall_macro       0.350591\n",
            "f1_macro           0.366835\n",
            "auc                0.398366\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-label Multi Neurons fixed H\n"
      ],
      "metadata": {
        "id": "VUmU5huOJMFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_ema_ml = df_ema.copy()\n",
        "\n",
        "# Making sure they are ints 0/1\n",
        "df_ema_ml[\"Positive\"] = df_ema_ml[\"Positive\"].astype(int)\n",
        "df_ema_ml[\"Negative\"] = df_ema_ml[\"Negative\"].astype(int)\n",
        "\n",
        "# Building dict: ema_labels_multi[subj][date] = np.array([neg, pos])\n",
        "ema_labels_multi = {}\n",
        "\n",
        "for subj, df_s in df_ema_ml.groupby(\"User_ID\"):\n",
        "    ema_labels_multi[subj] = {}\n",
        "    for _, row in df_s.iterrows():\n",
        "        d = pd.to_datetime(row[\"Date_only\"]).date()\n",
        "        neg = int(row[\"Negative\"])\n",
        "        pos = int(row[\"Positive\"])\n",
        "        ema_labels_multi[subj][d] = np.array([neg, pos], dtype=np.float32)\n"
      ],
      "metadata": {
        "id": "YqM9G9FoJdaB"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_dataset_multilabel(day_embeddings, ema_labels_multi, H=7):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      X: (N_samples, H, 32)\n",
        "      y: (N_samples, 2)   # [neg, pos]\n",
        "      subj_ids: (N_samples,)\n",
        "    \"\"\"\n",
        "    X_list, y_list, subj_list = [], [], []\n",
        "\n",
        "    for subj in day_embeddings:\n",
        "        if subj not in ema_labels_multi:\n",
        "            continue\n",
        "\n",
        "        days = sorted(day_embeddings[subj].keys())\n",
        "\n",
        "        for i in range(H, len(days)):\n",
        "            day_n = days[i]          # prediction day\n",
        "            prev_days = days[i-H:i]  # H previous days\n",
        "\n",
        "            if day_n not in ema_labels_multi[subj]:\n",
        "                continue\n",
        "\n",
        "            # Make sure we have embeddings for all previous days\n",
        "            try:\n",
        "                seq = np.vstack([day_embeddings[subj][d] for d in prev_days])  # (H, 32)\n",
        "            except KeyError:\n",
        "                continue\n",
        "\n",
        "            y_vec = ema_labels_multi[subj][day_n]   # (2,) [neg, pos]\n",
        "\n",
        "            X_list.append(seq)\n",
        "            y_list.append(y_vec)\n",
        "            subj_list.append(subj)\n",
        "\n",
        "    X = np.array(X_list, dtype=np.float32)     # (N, H, 32)\n",
        "    y = np.array(y_list, dtype=np.float32)     # (N, 2)\n",
        "    subj_ids = np.array(subj_list)\n",
        "\n",
        "    return X, y, subj_ids\n",
        "\n",
        "H = 7  # same horizon you used before\n",
        "X_ml, y_ml, subj_ml = build_dataset_multilabel(day_embeddings, ema_labels_multi, H=H)\n",
        "print(X_ml.shape, y_ml.shape, subj_ml.shape)\n",
        "# e.g. (N, 2, 32), (N, 2), (N,)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zW3v08S6JqhH",
        "outputId": "694acf40-b753-4a8b-c1e2-86e8ba345258"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(242, 7, 64) (242, 2) (242,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class AffectSequenceDatasetMultiLabel(Dataset):\n",
        "    \"\"\"\n",
        "    X: (N, H, 32)\n",
        "    y: (N, 2)  -> [neg, pos]\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)  # float for BCE\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n"
      ],
      "metadata": {
        "id": "Chn5DYfxKO8j"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class GRUAffectMultiLabel(nn.Module):\n",
        "    def __init__(self, latent_dim=64, hidden_dim=64, num_layers=1, num_labels=2):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=latent_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim, num_labels)  # 2 labels: [neg, pos]\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (batch, H, latent_dim)\n",
        "        returns: logits of shape (batch, 2)\n",
        "        \"\"\"\n",
        "        out, _ = self.gru(x)        # (batch, H, hidden_dim)\n",
        "        h_last = out[:, -1, :]      # (batch, hidden_dim)\n",
        "        logits = self.fc(h_last)    # (batch, 2)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "slzdmJMxKTG9"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, precision_score, recall_score,\n",
        "    confusion_matrix\n",
        ")\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def confusion_normalized(cm):\n",
        "    \"\"\"\n",
        "    Row-normalize a confusion matrix.\n",
        "    cm: (2,2) array\n",
        "    returns (2,2) float array\n",
        "    \"\"\"\n",
        "    cm = cm.astype(float)\n",
        "    row_sums = cm.sum(axis=1, keepdims=True)\n",
        "    row_sums[row_sums == 0] = 1.0\n",
        "    return cm / row_sums\n",
        "\n",
        "\n",
        "def evaluate_loso_gru_multilabel(\n",
        "    X, y, subj_ids,\n",
        "    latent_dim=None,\n",
        "    hidden_dim=64,\n",
        "    num_layers=1,\n",
        "    epochs=30,\n",
        "    batch_size=16,\n",
        "    device=None\n",
        "):\n",
        "    \"\"\"\n",
        "    X: (N, H, latent_dim)\n",
        "    y: (N, 2)  -> [neg, pos]\n",
        "    subj_ids: (N,)\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    import torch\n",
        "\n",
        "    if device is None:\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    if latent_dim is None:\n",
        "        latent_dim = X.shape[2]\n",
        "\n",
        "    unique_subjs = np.unique(subj_ids)\n",
        "    results = []\n",
        "\n",
        "    for test_subj in unique_subjs:\n",
        "        test_mask = (subj_ids == test_subj)\n",
        "        train_mask = ~test_mask\n",
        "\n",
        "        X_train, y_train = X[train_mask], y[train_mask]\n",
        "        X_test,  y_test  = X[test_mask],  y[test_mask]\n",
        "\n",
        "        if len(y_test) < 2:\n",
        "            continue\n",
        "\n",
        "        #  NORMALIZE (train only)\n",
        "        N_train, H, D = X_train.shape\n",
        "        N_test = X_test.shape[0]\n",
        "\n",
        "        X_train_flat = X_train.reshape(N_train, -1)\n",
        "        X_test_flat  = X_test.reshape(N_test,  -1)\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train_flat)\n",
        "        X_test_scaled  = scaler.transform(X_test_flat)\n",
        "\n",
        "        X_train = X_train_scaled.reshape(N_train, H, D)\n",
        "        X_test  = X_test_scaled.reshape(N_test,  H, D)\n",
        "\n",
        "\n",
        "        # Datasets\n",
        "        train_ds = AffectSequenceDatasetMultiLabel(X_train, y_train)\n",
        "        test_ds  = AffectSequenceDatasetMultiLabel(X_test,  y_test)\n",
        "\n",
        "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "        test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        # class weights (pos_weight)\n",
        "        pos_counts = y_train.sum(axis=0)\n",
        "        neg_counts = len(y_train) - pos_counts\n",
        "\n",
        "        # avoid zero division\n",
        "        pos_counts = np.where(pos_counts == 0, 1.0, pos_counts)\n",
        "        neg_counts = np.where(neg_counts == 0, 1.0, neg_counts)\n",
        "\n",
        "        raw_pw = neg_counts / pos_counts\n",
        "        alpha = 0.5\n",
        "        pos_weight = 1.0 + alpha * (raw_pw - 1.0)\n",
        "\n",
        "        pos_weight_t = torch.tensor(pos_weight, dtype=torch.float32, device=device)\n",
        "\n",
        "        # Model\n",
        "        model = GRUAffectMultiLabel(\n",
        "            latent_dim=latent_dim,\n",
        "            hidden_dim=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            num_labels=2\n",
        "        ).to(device)\n",
        "\n",
        "        opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "        loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight_t)\n",
        "\n",
        "        # TRAIN\n",
        "        for ep in range(epochs):\n",
        "            model.train()\n",
        "            for bx, by in train_loader:\n",
        "                bx, by = bx.to(device), by.to(device)\n",
        "                opt.zero_grad()\n",
        "                logits = model(bx)\n",
        "                loss = loss_fn(logits, by)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "        # TRAIN PRED FOR THRESHOLD TUNING\n",
        "        model.eval()\n",
        "        train_logits = []\n",
        "        train_truth = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for bx, by in train_loader:\n",
        "                bx = bx.to(device)\n",
        "                logits = model(bx)\n",
        "                train_logits.append(logits.cpu())\n",
        "                train_truth.append(by)\n",
        "\n",
        "        train_logits = torch.cat(train_logits, dim=0)\n",
        "        train_truth = torch.cat(train_truth, dim=0).numpy()\n",
        "        train_probs = torch.sigmoid(train_logits).numpy()\n",
        "\n",
        "        # Tune thresholds (per label)\n",
        "        def find_best_threshold(y_true, y_prob, steps=17):\n",
        "            best_t = 0.5\n",
        "            best_f1 = 0\n",
        "            for t in np.linspace(0.1, 0.9, steps):\n",
        "                y_pred = (y_prob >= t).astype(int)\n",
        "                f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "                if f1 > best_f1:\n",
        "                    best_f1 = f1\n",
        "                    best_t = t\n",
        "            return best_t\n",
        "\n",
        "        thr_neg = find_best_threshold(train_truth[:,0], train_probs[:,0])\n",
        "        thr_pos = find_best_threshold(train_truth[:,1], train_probs[:,1])\n",
        "\n",
        "        # TEST PRED\n",
        "        test_logits = []\n",
        "        test_truth = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for bx, by in test_loader:\n",
        "                bx = bx.to(device)\n",
        "                logits = model(bx)\n",
        "                test_logits.append(logits.cpu())\n",
        "                test_truth.append(by)\n",
        "\n",
        "        test_logits = torch.cat(test_logits, dim=0)\n",
        "        test_truth = torch.cat(test_truth, dim=0).numpy()\n",
        "\n",
        "        probs = torch.sigmoid(test_logits).numpy()\n",
        "        preds = np.zeros_like(probs, dtype=int)\n",
        "\n",
        "        preds[:,0] = (probs[:,0] >= thr_neg).astype(int)\n",
        "        preds[:,1] = (probs[:,1] >= thr_pos).astype(int)\n",
        "\n",
        "        metrics = {}\n",
        "        raw_cm_dict = {}\n",
        "        norm_cm_dict = {}\n",
        "\n",
        "        # Per label metrics + confusion matrices\n",
        "        for j, name in enumerate([\"neg\", \"pos\"]):\n",
        "            y_true_j = test_truth[:,j]\n",
        "            y_pred_j = preds[:,j]\n",
        "\n",
        "            # Metrics\n",
        "            metrics[f\"acc_{name}\"]  = accuracy_score(y_true_j, y_pred_j)\n",
        "            metrics[f\"prec_{name}\"] = precision_score(y_true_j, y_pred_j, zero_division=0)\n",
        "            metrics[f\"rec_{name}\"]  = recall_score(y_true_j, y_pred_j, zero_division=0)\n",
        "            metrics[f\"f1_{name}\"]   = f1_score(y_true_j, y_pred_j, zero_division=0)\n",
        "\n",
        "            # Confusion matrix (2x2)\n",
        "            cm = confusion_matrix(y_true_j, y_pred_j, labels=[0,1])\n",
        "            cm_norm = confusion_normalized(cm)\n",
        "\n",
        "            raw_cm_dict[f\"confusion_{name}\"] = cm.tolist()\n",
        "            norm_cm_dict[f\"confusion_norm_{name}\"] = cm_norm.tolist()\n",
        "\n",
        "        # Macro\n",
        "        metrics[\"acc_macro\"]  = 0.5*(metrics[\"acc_neg\"]+metrics[\"acc_pos\"])\n",
        "        metrics[\"prec_macro\"] = 0.5*(metrics[\"prec_neg\"]+metrics[\"prec_pos\"])\n",
        "        metrics[\"rec_macro\"]  = 0.5*(metrics[\"rec_neg\"]+metrics[\"rec_pos\"])\n",
        "        metrics[\"f1_macro\"]   = 0.5*(metrics[\"f1_neg\"]+metrics[\"f1_pos\"])\n",
        "\n",
        "        results.append({\n",
        "            \"test_subject\": test_subj,\n",
        "            \"n_test\": len(test_truth),\n",
        "            **metrics,\n",
        "            **raw_cm_dict,\n",
        "            **norm_cm_dict,\n",
        "            \"pos_weight\": pos_weight.tolist(),\n",
        "            \"thr_neg\": float(thr_neg),\n",
        "            \"thr_pos\": float(thr_pos)\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n"
      ],
      "metadata": {
        "id": "iNNxoS4aKmoy"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "latent_dim = X_ml.shape[2]\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "loso_ml = evaluate_loso_gru_multilabel(\n",
        "    X_ml, y_ml, subj_ml,\n",
        "    latent_dim=latent_dim,\n",
        "    hidden_dim=64,\n",
        "    num_layers=1,\n",
        "    epochs=30,\n",
        "    batch_size=16,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(loso_ml)\n",
        "print(\"\\nAverage metrics:\")\n",
        "print(loso_ml[[\"acc_macro\", \"prec_macro\", \"rec_macro\", \"f1_macro\"]].mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nWbrBcSK8U9",
        "outputId": "5e120979-f91a-4d1b-ee23-9d3610b4e03d"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   test_subject  n_test   acc_neg  prec_neg   rec_neg    f1_neg   acc_pos  \\\n",
            "0         cr001      22  0.590909  0.000000  0.000000  0.000000  0.727273   \n",
            "1         cr002      19  0.473684  0.000000  0.000000  0.000000  0.578947   \n",
            "2         cr003       7  0.428571  0.333333  1.000000  0.500000  0.428571   \n",
            "3         cr004       3  0.666667  0.666667  1.000000  0.800000  1.000000   \n",
            "4         cr005      16  0.812500  0.000000  0.000000  0.000000  0.687500   \n",
            "5         cr006      14  0.500000  0.000000  0.000000  0.000000  0.500000   \n",
            "6         cr011      11  0.636364  1.000000  0.636364  0.777778  0.545455   \n",
            "7         cr016      10  1.000000  0.000000  0.000000  0.000000  0.600000   \n",
            "8         cr031      24  0.833333  0.904762  0.904762  0.904762  0.416667   \n",
            "9         cr032      13  0.692308  0.500000  0.500000  0.500000  0.923077   \n",
            "10       crs002      22  0.454545  1.000000  0.454545  0.625000  0.318182   \n",
            "11       crs007      15  0.733333  0.000000  0.000000  0.000000  0.000000   \n",
            "12       crs010      21  0.285714  0.250000  1.000000  0.400000  0.285714   \n",
            "13       crs011      16  0.750000  1.000000  0.750000  0.857143  0.187500   \n",
            "14       crs014      14  0.428571  1.000000  0.428571  0.600000  0.500000   \n",
            "15       crs015       8  0.625000  0.714286  0.833333  0.769231  0.625000   \n",
            "16       crs019       6  0.333333  0.200000  1.000000  0.333333  0.166667   \n",
            "\n",
            "    prec_pos   rec_pos    f1_pos  ...  prec_macro  rec_macro  f1_macro  \\\n",
            "0   0.882353  0.789474  0.833333  ...    0.441176   0.394737  0.416667   \n",
            "1   0.000000  0.000000  0.000000  ...    0.000000   0.000000  0.000000   \n",
            "2   0.000000  0.000000  0.000000  ...    0.166667   0.500000  0.250000   \n",
            "3   1.000000  1.000000  1.000000  ...    0.833333   1.000000  0.900000   \n",
            "4   0.916667  0.733333  0.814815  ...    0.458333   0.366667  0.407407   \n",
            "5   0.636364  0.700000  0.666667  ...    0.318182   0.350000  0.333333   \n",
            "6   0.428571  0.750000  0.545455  ...    0.714286   0.693182  0.661616   \n",
            "7   0.750000  0.750000  0.750000  ...    0.375000   0.375000  0.375000   \n",
            "8   0.444444  0.307692  0.363636  ...    0.674603   0.606227  0.634199   \n",
            "9   1.000000  0.923077  0.960000  ...    0.750000   0.711538  0.730000   \n",
            "10  0.214286  0.428571  0.285714  ...    0.607143   0.441558  0.455357   \n",
            "11  0.000000  0.000000  0.000000  ...    0.000000   0.000000  0.000000   \n",
            "12  0.666667  0.235294  0.347826  ...    0.458333   0.617647  0.373913   \n",
            "13  0.000000  0.000000  0.000000  ...    0.500000   0.375000  0.428571   \n",
            "14  0.000000  0.000000  0.000000  ...    0.500000   0.214286  0.300000   \n",
            "15  0.333333  0.500000  0.400000  ...    0.523810   0.666667  0.584615   \n",
            "16  0.000000  0.000000  0.000000  ...    0.100000   0.500000  0.166667   \n",
            "\n",
            "         confusion_neg      confusion_pos  \\\n",
            "0    [[13, 7], [2, 0]]  [[1, 2], [4, 15]]   \n",
            "1    [[9, 10], [0, 0]]  [[11, 8], [0, 0]]   \n",
            "2     [[1, 4], [0, 2]]   [[3, 0], [4, 0]]   \n",
            "3     [[0, 1], [0, 2]]   [[0, 0], [0, 3]]   \n",
            "4    [[13, 3], [0, 0]]  [[0, 1], [4, 11]]   \n",
            "5     [[7, 3], [4, 0]]   [[0, 4], [3, 7]]   \n",
            "6     [[0, 0], [4, 7]]   [[3, 4], [1, 3]]   \n",
            "7    [[10, 0], [0, 0]]   [[0, 2], [2, 6]]   \n",
            "8    [[1, 2], [2, 19]]   [[6, 5], [9, 4]]   \n",
            "9     [[7, 2], [2, 2]]  [[0, 0], [1, 12]]   \n",
            "10  [[0, 0], [12, 10]]  [[4, 11], [4, 3]]   \n",
            "11   [[11, 4], [0, 0]]  [[0, 0], [15, 0]]   \n",
            "12   [[1, 15], [0, 5]]  [[2, 2], [13, 4]]   \n",
            "13   [[0, 0], [4, 12]]  [[3, 13], [0, 0]]   \n",
            "14    [[0, 0], [8, 6]]   [[7, 7], [0, 0]]   \n",
            "15    [[0, 2], [1, 5]]   [[4, 2], [1, 1]]   \n",
            "16    [[1, 4], [0, 1]]   [[1, 0], [5, 0]]   \n",
            "\n",
            "                                   confusion_norm_neg  \\\n",
            "0                          [[0.65, 0.35], [1.0, 0.0]]   \n",
            "1   [[0.47368421052631576, 0.5263157894736842], [0...   \n",
            "2                            [[0.2, 0.8], [0.0, 1.0]]   \n",
            "3                            [[0.0, 1.0], [0.0, 1.0]]   \n",
            "4                      [[0.8125, 0.1875], [0.0, 0.0]]   \n",
            "5                            [[0.7, 0.3], [1.0, 0.0]]   \n",
            "6   [[0.0, 0.0], [0.36363636363636365, 0.636363636...   \n",
            "7                            [[1.0, 0.0], [0.0, 0.0]]   \n",
            "8   [[0.3333333333333333, 0.6666666666666666], [0....   \n",
            "9   [[0.7777777777777778, 0.2222222222222222], [0....   \n",
            "10  [[0.0, 0.0], [0.5454545454545454, 0.4545454545...   \n",
            "11  [[0.7333333333333333, 0.26666666666666666], [0...   \n",
            "12                     [[0.0625, 0.9375], [0.0, 1.0]]   \n",
            "13                         [[0.0, 0.0], [0.25, 0.75]]   \n",
            "14  [[0.0, 0.0], [0.5714285714285714, 0.4285714285...   \n",
            "15  [[0.0, 1.0], [0.16666666666666666, 0.833333333...   \n",
            "16                           [[0.2, 0.8], [0.0, 1.0]]   \n",
            "\n",
            "                                   confusion_norm_pos  \\\n",
            "0   [[0.3333333333333333, 0.6666666666666666], [0....   \n",
            "1   [[0.5789473684210527, 0.42105263157894735], [0...   \n",
            "2                            [[1.0, 0.0], [1.0, 0.0]]   \n",
            "3                            [[0.0, 0.0], [0.0, 1.0]]   \n",
            "4   [[0.0, 1.0], [0.26666666666666666, 0.733333333...   \n",
            "5                            [[0.0, 1.0], [0.3, 0.7]]   \n",
            "6   [[0.42857142857142855, 0.5714285714285714], [0...   \n",
            "7                          [[0.0, 1.0], [0.25, 0.75]]   \n",
            "8   [[0.5454545454545454, 0.45454545454545453], [0...   \n",
            "9   [[0.0, 0.0], [0.07692307692307693, 0.923076923...   \n",
            "10  [[0.26666666666666666, 0.7333333333333333], [0...   \n",
            "11                           [[0.0, 0.0], [1.0, 0.0]]   \n",
            "12  [[0.5, 0.5], [0.7647058823529411, 0.2352941176...   \n",
            "13                     [[0.1875, 0.8125], [0.0, 0.0]]   \n",
            "14                           [[0.5, 0.5], [0.0, 0.0]]   \n",
            "15  [[0.6666666666666666, 0.3333333333333333], [0....   \n",
            "16                           [[1.0, 0.0], [1.0, 0.0]]   \n",
            "\n",
            "                                  pos_weight thr_neg  thr_pos  \n",
            "0     [1.0185184478759766, 0.94017094373703]    0.35     0.30  \n",
            "1   [1.0136363506317139, 0.8198529481887817]    0.35     0.60  \n",
            "2   [1.0879629850387573, 0.8901515007019043]    0.35     0.50  \n",
            "3   [1.1064815521240234, 0.8984962701797485]    0.35     0.65  \n",
            "4   [1.0272727012634277, 0.9338842630386353]    0.30     0.45  \n",
            "5   [1.0754716396331787, 0.9047619104385376]    0.30     0.60  \n",
            "6                [1.1666667461395264, 0.875]    0.55     0.35  \n",
            "7              [1.0545454025268555, 0.90625]    0.45     0.50  \n",
            "8   [1.2247190475463867, 0.8861788511276245]    0.20     0.65  \n",
            "9   [1.0801887512207031, 0.9308943152427673]    0.25     0.75  \n",
            "10                [1.25, 0.8527131676673889]    0.55     0.65  \n",
            "11   [1.031818151473999, 0.9380165338516235]    0.30     0.65  \n",
            "12  [1.0523810386657715, 0.9285714626312256]    0.25     0.70  \n",
            "13  [1.2021276950836182, 0.8308823108673096]    0.35     0.50  \n",
            "14              [1.1875, 0.8382352590560913]    0.60     0.55  \n",
            "15               [1.125, 0.8731343150138855]    0.25     0.60  \n",
            "16   [1.082568883895874, 0.9007633924484253]    0.25     0.60  \n",
            "\n",
            "[17 rows x 21 columns]\n",
            "\n",
            "Average metrics:\n",
            "acc_macro     0.551041\n",
            "prec_macro    0.436522\n",
            "rec_macro     0.459559\n",
            "f1_macro      0.412785\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-label with cumulative n-1 days HR data"
      ],
      "metadata": {
        "id": "iaq54uEqM05D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_dataset_cumulative_multilabel(day_embeddings, ema_labels_multi):\n",
        "    \"\"\"\n",
        "    For each subject and each day_n (from 2nd day onward):\n",
        "      X_seq = embeddings for all days before day_n: [day_1, ..., day_{n-1}]\n",
        "      y_vec = EMA multilabel for day_n: [neg, pos]\n",
        "    Returns:\n",
        "      X_list: list of np.arrays with shape (T_i, D), T_i varies by sample\n",
        "      y_arr:  (N, 2)\n",
        "      subj_arr: (N,)\n",
        "    \"\"\"\n",
        "    X_list = []\n",
        "    y_list = []\n",
        "    subj_list = []\n",
        "\n",
        "    for subj in day_embeddings:\n",
        "        if subj not in ema_labels_multi:\n",
        "            continue\n",
        "\n",
        "        days = sorted(day_embeddings[subj].keys())\n",
        "\n",
        "        # start at i=7, because i=0 has no history\n",
        "        for i in range(7, len(days)):\n",
        "            day_n = days[i]\n",
        "            prev_day = days[i-1]\n",
        "            history_days = days[:i]   # all days up to day_{i-1}\n",
        "\n",
        "            # need EMA label for the target day\n",
        "            if day_n not in ema_labels_multi[subj]:\n",
        "                continue\n",
        "            if prev_day not in ema_labels_multi[subj]:\n",
        "                continue\n",
        "\n",
        "            # build sequence of embeddings for all previous days\n",
        "            seq = np.vstack([day_embeddings[subj][d] for d in history_days])  # (i, D)\n",
        "\n",
        "            y_vec = ema_labels_multi[subj][day_n]   # shape (2,)\n",
        "\n",
        "            X_list.append(seq)\n",
        "            y_list.append(y_vec)\n",
        "            subj_list.append(subj)\n",
        "\n",
        "    X_list = np.array(X_list, dtype=object)  # object array since seq lengths differ\n",
        "    y_arr = np.array(y_list, dtype=np.float32)\n",
        "    subj_arr = np.array(subj_list)\n",
        "\n",
        "    return X_list, y_arr, subj_arr\n"
      ],
      "metadata": {
        "id": "XjwK4zTOcQD_"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_cum, y_cum, subj_cum = build_dataset_cumulative_multilabel(day_embeddings, ema_labels_multi)\n"
      ],
      "metadata": {
        "id": "khxnvKtecWJE"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class AffectSequenceDatasetMultiLabelVar(Dataset):\n",
        "    def __init__(self, X_list, y):\n",
        "        # X_list: array/list of arrays with shape (T_i, D)\n",
        "        self.X = [torch.tensor(x, dtype=torch.float32) for x in X_list]\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "\n",
        "def collate_varlen_multilabel(batch):\n",
        "    \"\"\"\n",
        "    batch: list of (seq, y) where seq is (T_i, D)\n",
        "    Returns:\n",
        "      padded: (B, T_max, D)\n",
        "      y_batch: (B, 2)\n",
        "      lengths: (B,) original sequence lengths\n",
        "    \"\"\"\n",
        "    seqs, ys = zip(*batch)\n",
        "    lengths = [s.shape[0] for s in seqs]\n",
        "    B = len(seqs)\n",
        "    D = seqs[0].shape[1]\n",
        "    T_max = max(lengths)\n",
        "\n",
        "    padded = torch.zeros(B, T_max, D, dtype=torch.float32)\n",
        "    for i, s in enumerate(seqs):\n",
        "        T_i = s.shape[0]\n",
        "        # left-pad so the most recent day is at the end\n",
        "        padded[i, T_max - T_i:, :] = s\n",
        "\n",
        "    y_batch = torch.stack(ys, dim=0)\n",
        "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
        "\n",
        "    return padded, y_batch, lengths\n"
      ],
      "metadata": {
        "id": "PWIML0oScehi"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "class GRUAffectMultiLabel(nn.Module):\n",
        "    def __init__(self, latent_dim=64, hidden_dim=64, num_layers=1, num_labels=2):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=latent_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(hidden_dim, num_labels)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        \"\"\"\n",
        "        x: (B, T_max, D)\n",
        "        lengths: (B,) actual sequence lengths\n",
        "        \"\"\"\n",
        "        # pack padded sequence so GRU ignores left padding\n",
        "        packed = pack_padded_sequence(\n",
        "            x, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
        "        )\n",
        "        packed_out, _ = self.gru(packed)\n",
        "        out, _ = pad_packed_sequence(packed_out, batch_first=True)  # (B, T_eff, H)\n",
        "\n",
        "        # last valid timestep for each sequence\n",
        "        idx = (lengths - 1).unsqueeze(1).unsqueeze(2).expand(-1, 1, out.size(2))\n",
        "        last_h = out.gather(1, idx).squeeze(1)  # (B, hidden_dim)\n",
        "\n",
        "        last_h = self.dropout(last_h)\n",
        "        logits = self.fc(last_h)               # (B, 2)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "HWGTxKGCcjum"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, precision_score, recall_score,\n",
        "    confusion_matrix\n",
        ")\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def confusion_normalized(cm):\n",
        "    \"\"\"\n",
        "    Row-normalize a confusion matrix.\n",
        "    cm: (2,2) array\n",
        "    returns (2,2) float array\n",
        "    \"\"\"\n",
        "    cm = cm.astype(float)\n",
        "    row_sums = cm.sum(axis=1, keepdims=True)\n",
        "    row_sums[row_sums == 0] = 1.0\n",
        "    return cm / row_sums\n",
        "\n",
        "\n",
        "def evaluate_loso_gru_multilabel(\n",
        "    X, y, subj_ids,\n",
        "    latent_dim=None,\n",
        "    hidden_dim=64,\n",
        "    num_layers=1,\n",
        "    epochs=30,\n",
        "    batch_size=16,\n",
        "    device=None\n",
        "):\n",
        "    \"\"\"\n",
        "    X: list/array of length N, each element has shape (T_i, latent_dim)\n",
        "    y: (N, 2) multilabel targets [neg, pos]\n",
        "    subj_ids: (N,)\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    if latent_dim is None:\n",
        "        # infer from the first sample\n",
        "        latent_dim = X[0].shape[1]\n",
        "\n",
        "    unique_subjs = np.unique(subj_ids)\n",
        "    results = []\n",
        "\n",
        "    for test_subj in unique_subjs:\n",
        "        test_mask = (subj_ids == test_subj)\n",
        "        train_mask = ~test_mask\n",
        "\n",
        "        if not np.any(test_mask):\n",
        "            continue\n",
        "\n",
        "        # index arrays for train/test\n",
        "        train_idx = np.where(train_mask)[0]\n",
        "        test_idx  = np.where(test_mask)[0]\n",
        "\n",
        "        # split X into lists of sequences\n",
        "        X_train_list = [X[i] for i in train_idx]\n",
        "        X_test_list  = [X[i] for i in test_idx]\n",
        "\n",
        "        y_train = y[train_mask]\n",
        "        y_test  = y[test_mask]\n",
        "\n",
        "        if len(y_test) < 2:\n",
        "            continue\n",
        "\n",
        "        #  NORMALIZE (train only)\n",
        "        # stack all timesteps from training sequences to fit scaler\n",
        "        concat_train = np.vstack(X_train_list)   # (sum_T_train, D)\n",
        "        scaler = StandardScaler().fit(concat_train)\n",
        "\n",
        "        X_train_scaled = [scaler.transform(seq) for seq in X_train_list]\n",
        "        X_test_scaled  = [scaler.transform(seq) for seq in X_test_list]\n",
        "\n",
        "\n",
        "        # Datasets & loaders (variable length)\n",
        "        train_ds = AffectSequenceDatasetMultiLabelVar(X_train_scaled, y_train)\n",
        "        test_ds  = AffectSequenceDatasetMultiLabelVar(X_test_scaled,  y_test)\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            train_ds, batch_size=batch_size,\n",
        "            shuffle=True, collate_fn=collate_varlen_multilabel\n",
        "        )\n",
        "        test_loader  = DataLoader(\n",
        "            test_ds, batch_size=batch_size,\n",
        "            shuffle=False, collate_fn=collate_varlen_multilabel\n",
        "        )\n",
        "\n",
        "        # class weights (pos_weight)\n",
        "        pos_counts = y_train.sum(axis=0)           # (2,)\n",
        "        neg_counts = len(y_train) - pos_counts\n",
        "\n",
        "        pos_counts = np.where(pos_counts == 0, 1.0, pos_counts)\n",
        "        neg_counts = np.where(neg_counts == 0, 1.0, neg_counts)\n",
        "\n",
        "        raw_pw = neg_counts / pos_counts          # (2,)\n",
        "        alpha = 0.5\n",
        "        pos_weight = 1.0 + alpha * (raw_pw - 1.0)\n",
        "\n",
        "        pos_weight_t = torch.tensor(pos_weight, dtype=torch.float32, device=device)\n",
        "\n",
        "        #  Model\n",
        "        model = GRUAffectMultiLabel(\n",
        "            latent_dim=latent_dim,\n",
        "            hidden_dim=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            num_labels=2\n",
        "        ).to(device)\n",
        "\n",
        "        opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "        loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight_t)\n",
        "\n",
        "        # TRAIN\n",
        "        for ep in range(epochs):\n",
        "            model.train()\n",
        "            for bx, by, lengths in train_loader:\n",
        "                bx, by, lengths = bx.to(device), by.to(device), lengths.to(device)\n",
        "                opt.zero_grad()\n",
        "                logits = model(bx, lengths)   # (B, 2)\n",
        "                loss = loss_fn(logits, by)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "        # TRAIN PRED FOR THRESHOLD TUNING\n",
        "        model.eval()\n",
        "        train_logits = []\n",
        "        train_truth = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for bx, by, lengths in train_loader:\n",
        "                bx, by, lengths = bx.to(device), by.to(device), lengths.to(device)\n",
        "                logits = model(bx, lengths)\n",
        "                train_logits.append(logits.cpu())\n",
        "                train_truth.append(by)\n",
        "\n",
        "        train_logits = torch.cat(train_logits, dim=0).cpu()       # (N_train_fold, 2)\n",
        "        train_truth = torch.cat(train_truth, dim=0).cpu().numpy() # (N_train_fold, 2)\n",
        "        train_probs = torch.sigmoid(train_logits).cpu().numpy()   # (N_train_fold, 2)\n",
        "\n",
        "        # Tune thresholds (per label)\n",
        "        def find_best_threshold(y_true, y_prob, steps=17):\n",
        "            best_t = 0.5\n",
        "            best_f1 = 0.0\n",
        "            for t in np.linspace(0.1, 0.9, steps):\n",
        "                y_pred = (y_prob >= t).astype(int)\n",
        "                f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "                if f1 > best_f1:\n",
        "                    best_f1 = f1\n",
        "                    best_t = t\n",
        "            return best_t\n",
        "\n",
        "        thr_neg = find_best_threshold(train_truth[:, 0], train_probs[:, 0])\n",
        "        thr_pos = find_best_threshold(train_truth[:, 1], train_probs[:, 1])\n",
        "\n",
        "        # TEST PRED\n",
        "        test_logits = []\n",
        "        test_truth = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for bx, by, lengths in test_loader:\n",
        "                bx, by, lengths = bx.to(device), by.to(device), lengths.to(device)\n",
        "                logits = model(bx, lengths)\n",
        "                test_logits.append(logits.cpu())\n",
        "                test_truth.append(by)\n",
        "\n",
        "        test_logits = torch.cat(test_logits, dim=0).cpu()\n",
        "        test_truth = torch.cat(test_truth, dim=0).cpu().numpy()\n",
        "        probs = torch.sigmoid(test_logits).cpu().numpy()   # (N_test, 2)\n",
        "\n",
        "        preds = np.zeros_like(probs, dtype=int)\n",
        "        preds[:, 0] = (probs[:, 0] >= thr_neg).astype(int)\n",
        "        preds[:, 1] = (probs[:, 1] >= thr_pos).astype(int)\n",
        "\n",
        "        #Metrics & confusion matrices\n",
        "        metrics = {}\n",
        "        raw_cm_dict = {}\n",
        "        norm_cm_dict = {}\n",
        "\n",
        "        for j, name in enumerate([\"neg\", \"pos\"]):\n",
        "            y_true_j = test_truth[:, j]\n",
        "            y_pred_j = preds[:, j]\n",
        "\n",
        "            metrics[f\"acc_{name}\"]  = accuracy_score(y_true_j, y_pred_j)\n",
        "            metrics[f\"prec_{name}\"] = precision_score(y_true_j, y_pred_j, zero_division=0)\n",
        "            metrics[f\"rec_{name}\"]  = recall_score(y_true_j, y_pred_j, zero_division=0)\n",
        "            metrics[f\"f1_{name}\"]   = f1_score(y_true_j, y_pred_j, zero_division=0)\n",
        "\n",
        "            cm = confusion_matrix(y_true_j, y_pred_j, labels=[0, 1])\n",
        "            cm_norm = confusion_normalized(cm)\n",
        "\n",
        "            raw_cm_dict[f\"confusion_{name}\"] = cm.tolist()\n",
        "            norm_cm_dict[f\"confusion_norm_{name}\"] = cm_norm.tolist()\n",
        "\n",
        "        metrics[\"acc_macro\"]  = 0.5 * (metrics[\"acc_neg\"]  + metrics[\"acc_pos\"])\n",
        "        metrics[\"prec_macro\"] = 0.5 * (metrics[\"prec_neg\"] + metrics[\"prec_pos\"])\n",
        "        metrics[\"rec_macro\"]  = 0.5 * (metrics[\"rec_neg\"]  + metrics[\"rec_pos\"])\n",
        "        metrics[\"f1_macro\"]   = 0.5 * (metrics[\"f1_neg\"]   + metrics[\"f1_pos\"])\n",
        "\n",
        "        results.append({\n",
        "            \"test_subject\": test_subj,\n",
        "            \"n_test\": len(test_truth),\n",
        "            **metrics,\n",
        "            **raw_cm_dict,\n",
        "            **norm_cm_dict,\n",
        "            \"pos_weight\": pos_weight.tolist(),\n",
        "            \"thr_neg\": float(thr_neg),\n",
        "            \"thr_pos\": float(thr_pos),\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yTyOstfMc5Pd"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "loso_cum = evaluate_loso_gru_multilabel(\n",
        "    X_cum, y_cum, subj_cum,\n",
        "    latent_dim=X_cum[0].shape[1],   # or omit, it'll infer\n",
        "    hidden_dim=64,\n",
        "    num_layers=1,\n",
        "    epochs=30,\n",
        "    batch_size=16,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(loso_cum)\n",
        "print(\"\\nAverage metrics:\")\n",
        "print(loso_cum[[\"acc_macro\", \"prec_macro\", \"rec_macro\", \"f1_macro\"]].mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rC3uQ6sngusg",
        "outputId": "723edbf7-51fd-4743-90b7-b94491909393"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   test_subject  n_test   acc_neg  prec_neg   rec_neg    f1_neg   acc_pos  \\\n",
            "0         cr001      22  0.090909  0.090909  1.000000  0.166667  0.772727   \n",
            "1         cr002       7  0.285714  0.000000  0.000000  0.000000  0.000000   \n",
            "2         cr003       7  0.285714  0.285714  1.000000  0.444444  0.285714   \n",
            "3         cr005      16  0.000000  0.000000  0.000000  0.000000  0.937500   \n",
            "4         cr006       5  0.400000  0.500000  0.333333  0.400000  0.400000   \n",
            "5         cr011       9  0.444444  1.000000  0.444444  0.615385  0.333333   \n",
            "6         cr016       4  0.000000  0.000000  0.000000  0.000000  0.750000   \n",
            "7         cr031      14  0.928571  0.928571  1.000000  0.962963  0.428571   \n",
            "8         cr032      12  0.500000  0.200000  0.333333  0.250000  1.000000   \n",
            "9        crs002      22  1.000000  1.000000  1.000000  1.000000  0.545455   \n",
            "10       crs007      15  0.266667  0.000000  0.000000  0.000000  0.466667   \n",
            "11       crs010      18  0.222222  0.176471  1.000000  0.300000  0.500000   \n",
            "12       crs011      13  0.923077  1.000000  0.923077  0.960000  0.230769   \n",
            "13       crs014      14  1.000000  1.000000  1.000000  1.000000  0.285714   \n",
            "14       crs015       4  0.500000  0.500000  1.000000  0.666667  0.750000   \n",
            "15       crs019       6  0.333333  0.200000  1.000000  0.333333  0.833333   \n",
            "\n",
            "    prec_pos   rec_pos    f1_pos  ...  prec_macro  rec_macro  f1_macro  \\\n",
            "0   0.850000  0.894737  0.871795  ...    0.470455   0.947368  0.519231   \n",
            "1   0.000000  0.000000  0.000000  ...    0.000000   0.000000  0.000000   \n",
            "2   0.333333  0.250000  0.285714  ...    0.309524   0.625000  0.365079   \n",
            "3   0.937500  1.000000  0.967742  ...    0.468750   0.500000  0.483871   \n",
            "4   0.400000  1.000000  0.571429  ...    0.450000   0.666667  0.485714   \n",
            "5   0.375000  0.750000  0.500000  ...    0.687500   0.597222  0.557692   \n",
            "6   1.000000  0.666667  0.800000  ...    0.500000   0.333333  0.400000   \n",
            "7   0.428571  0.428571  0.428571  ...    0.678571   0.714286  0.695767   \n",
            "8   1.000000  1.000000  1.000000  ...    0.600000   0.666667  0.625000   \n",
            "9   0.384615  0.714286  0.500000  ...    0.692308   0.857143  0.750000   \n",
            "10  1.000000  0.466667  0.636364  ...    0.500000   0.233333  0.318182   \n",
            "11  0.777778  0.500000  0.608696  ...    0.477124   0.750000  0.454348   \n",
            "12  0.000000  0.000000  0.000000  ...    0.500000   0.461538  0.480000   \n",
            "13  0.000000  0.000000  0.000000  ...    0.500000   0.500000  0.500000   \n",
            "14  1.000000  0.500000  0.666667  ...    0.750000   0.750000  0.666667   \n",
            "15  0.833333  1.000000  0.909091  ...    0.516667   1.000000  0.621212   \n",
            "\n",
            "        confusion_neg      confusion_pos  \\\n",
            "0   [[0, 20], [0, 2]]  [[0, 3], [2, 17]]   \n",
            "1    [[2, 5], [0, 0]]   [[0, 7], [0, 0]]   \n",
            "2    [[0, 5], [0, 2]]   [[1, 2], [3, 1]]   \n",
            "3   [[0, 16], [0, 0]]  [[0, 1], [0, 15]]   \n",
            "4    [[1, 1], [2, 1]]   [[0, 3], [0, 2]]   \n",
            "5    [[0, 0], [5, 4]]   [[0, 5], [1, 3]]   \n",
            "6    [[0, 4], [0, 0]]   [[1, 0], [1, 2]]   \n",
            "7   [[0, 1], [0, 13]]   [[3, 4], [4, 3]]   \n",
            "8    [[5, 4], [2, 1]]  [[0, 0], [0, 12]]   \n",
            "9   [[0, 0], [0, 22]]   [[7, 8], [2, 5]]   \n",
            "10  [[4, 11], [0, 0]]   [[0, 0], [8, 7]]   \n",
            "11  [[1, 14], [0, 3]]   [[2, 2], [7, 7]]   \n",
            "12  [[0, 0], [1, 12]]  [[3, 10], [0, 0]]   \n",
            "13  [[0, 0], [0, 14]]  [[4, 10], [0, 0]]   \n",
            "14   [[0, 2], [0, 2]]   [[2, 0], [1, 1]]   \n",
            "15   [[1, 4], [0, 1]]   [[0, 1], [0, 5]]   \n",
            "\n",
            "                                   confusion_norm_neg  \\\n",
            "0                            [[0.0, 1.0], [0.0, 1.0]]   \n",
            "1   [[0.2857142857142857, 0.7142857142857143], [0....   \n",
            "2                            [[0.0, 1.0], [0.0, 1.0]]   \n",
            "3                            [[0.0, 1.0], [0.0, 0.0]]   \n",
            "4   [[0.5, 0.5], [0.6666666666666666, 0.3333333333...   \n",
            "5   [[0.0, 0.0], [0.5555555555555556, 0.4444444444...   \n",
            "6                            [[0.0, 1.0], [0.0, 0.0]]   \n",
            "7                            [[0.0, 1.0], [0.0, 1.0]]   \n",
            "8   [[0.5555555555555556, 0.4444444444444444], [0....   \n",
            "9                            [[0.0, 0.0], [0.0, 1.0]]   \n",
            "10  [[0.26666666666666666, 0.7333333333333333], [0...   \n",
            "11  [[0.06666666666666667, 0.9333333333333333], [0...   \n",
            "12  [[0.0, 0.0], [0.07692307692307693, 0.923076923...   \n",
            "13                           [[0.0, 0.0], [0.0, 1.0]]   \n",
            "14                           [[0.0, 1.0], [0.0, 1.0]]   \n",
            "15                           [[0.2, 0.8], [0.0, 1.0]]   \n",
            "\n",
            "                                   confusion_norm_pos  \\\n",
            "0   [[0.0, 1.0], [0.10526315789473684, 0.894736842...   \n",
            "1                            [[0.0, 1.0], [0.0, 0.0]]   \n",
            "2   [[0.3333333333333333, 0.6666666666666666], [0....   \n",
            "3                            [[0.0, 1.0], [0.0, 1.0]]   \n",
            "4                            [[0.0, 1.0], [0.0, 1.0]]   \n",
            "5                          [[0.0, 1.0], [0.25, 0.75]]   \n",
            "6   [[1.0, 0.0], [0.3333333333333333, 0.6666666666...   \n",
            "7   [[0.42857142857142855, 0.5714285714285714], [0...   \n",
            "8                            [[0.0, 0.0], [0.0, 1.0]]   \n",
            "9   [[0.4666666666666667, 0.5333333333333333], [0....   \n",
            "10  [[0.0, 0.0], [0.5333333333333333, 0.4666666666...   \n",
            "11                           [[0.5, 0.5], [0.5, 0.5]]   \n",
            "12  [[0.23076923076923078, 0.7692307692307693], [0...   \n",
            "13  [[0.2857142857142857, 0.7142857142857143], [0....   \n",
            "14                           [[1.0, 0.0], [0.5, 0.5]]   \n",
            "15                           [[0.0, 1.0], [0.0, 1.0]]   \n",
            "\n",
            "                                  pos_weight thr_neg  thr_pos  \n",
            "0   [0.9823529720306396, 0.9175823926925659]    0.25     0.40  \n",
            "1    [1.045976996421814, 0.8272727131843567]    0.25     0.30  \n",
            "2    [1.070588231086731, 0.8584905862808228]    0.30     0.45  \n",
            "3   [0.9942528605461121, 0.9105262756347656]    0.30     0.15  \n",
            "4   [1.0952380895614624, 0.8518518209457397]    0.45     0.45  \n",
            "5    [1.1538461446762085, 0.849056601524353]    0.25     0.45  \n",
            "6    [1.063218355178833, 0.8644859790802002]    0.35     0.50  \n",
            "7   [1.1824324131011963, 0.8495146036148071]    0.25     0.45  \n",
            "8   [1.0535714626312256, 0.9030612111091614]    0.45     0.30  \n",
            "9    [1.284615397453308, 0.8106796145439148]    0.45     0.30  \n",
            "10                 [1.0, 0.9157894849777222]    0.45     0.35  \n",
            "11            [1.0178570747375488, 0.890625]    0.40     0.45  \n",
            "12   [1.1891891956329346, 0.800000011920929]    0.40     0.50  \n",
            "13  [1.1986300945281982, 0.7954545021057129]    0.20     0.45  \n",
            "14  [1.0882353782653809, 0.8564814925193787]    0.20     0.40  \n",
            "15  [1.0639533996582031, 0.8714286088943481]    0.40     0.40  \n",
            "\n",
            "[16 rows x 21 columns]\n",
            "\n",
            "Average metrics:\n",
            "acc_macro     0.490639\n",
            "prec_macro    0.506306\n",
            "rec_macro     0.600160\n",
            "f1_macro      0.495173\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM VERSION OF HR ONLY"
      ],
      "metadata": {
        "id": "NG-ICAoAQJT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMAffectBinary(nn.Module):\n",
        "    def __init__(self, latent_dim=64, hidden_dim=64, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=latent_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(hidden_dim, 1)  # single logit for BCE\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        \"\"\"\n",
        "        x:       (B, T_max, D)\n",
        "        lengths: (B,)\n",
        "        \"\"\"\n",
        "        packed = pack_padded_sequence(\n",
        "            x, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
        "        )\n",
        "        # LSTM returns (output, (h_n, c_n))\n",
        "        packed_out, _ = self.lstm(packed)\n",
        "        out, _ = pad_packed_sequence(packed_out, batch_first=True)  # (B, T_eff, H)\n",
        "\n",
        "        # last valid timestep per sample (same as in GRU version)\n",
        "        idx = (lengths - 1).unsqueeze(1).unsqueeze(2).expand(-1, 1, out.size(2))\n",
        "        last_h = out.gather(1, idx).squeeze(1)  # (B, hidden_dim)\n",
        "\n",
        "        last_h = self.dropout(last_h)\n",
        "        logit = self.fc(last_h).squeeze(1)      # (B,)\n",
        "        return logit\n"
      ],
      "metadata": {
        "id": "lGOabxs5QM8G"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    f1_score,\n",
        "    recall_score,\n",
        "    precision_score,\n",
        "    roc_auc_score,\n",
        "    confusion_matrix\n",
        ")\n",
        "\n",
        "def evaluate_loso_lstm_binary_cumulative_hr(\n",
        "    X_list, y, subj_ids,\n",
        "    latent_dim=64,\n",
        "    hidden_dim=64,\n",
        "    num_layers=1,\n",
        "    epochs=30,\n",
        "    batch_size=16,\n",
        "    device=None\n",
        "):\n",
        "    \"\"\"\n",
        "    X_list: object array/list of length N, each element (T_i, D)\n",
        "    y:      (N,) 0/1\n",
        "    subj_ids: (N,)\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    unique_subjs = np.unique(subj_ids)\n",
        "    results = []\n",
        "\n",
        "    for test_subj in unique_subjs:\n",
        "        test_mask = (subj_ids == test_subj)\n",
        "        train_mask = ~test_mask\n",
        "\n",
        "        X_train_list = X_list[train_mask]\n",
        "        X_test_list  = X_list[test_mask]\n",
        "        y_train      = y[train_mask]\n",
        "        y_test       = y[test_mask]\n",
        "\n",
        "        # need at least 2 test samples\n",
        "        if len(y_test) < 2:\n",
        "            continue\n",
        "\n",
        "        #  NORMALIZATION (on training only)\n",
        "        # compute mean/std over all timepoints and dims in training\n",
        "        all_train_frames = np.vstack(X_train_list)  # (sum T_i, D)\n",
        "        mean = all_train_frames.mean(axis=0, keepdims=True)  # (1, D)\n",
        "        std = all_train_frames.std(axis=0, keepdims=True)\n",
        "        std[std == 0] = 1.0\n",
        "\n",
        "        def normalize_seq_list(XL):\n",
        "            out = []\n",
        "            for arr in XL:\n",
        "                arr_norm = (arr - mean) / std\n",
        "                out.append(arr_norm.astype(np.float32))\n",
        "            return np.array(out, dtype=object)\n",
        "\n",
        "        X_train_list_norm = normalize_seq_list(X_train_list)\n",
        "        X_test_list_norm  = normalize_seq_list(X_test_list)\n",
        "\n",
        "\n",
        "        # datasets/loaders\n",
        "        train_ds = AffectSequenceDatasetBin_Var(X_train_list_norm, y_train)\n",
        "        test_ds  = AffectSequenceDatasetBin_Var(X_test_list_norm,  y_test)\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            train_ds, batch_size=batch_size, shuffle=True,\n",
        "            collate_fn=collate_varlen_bin\n",
        "        )\n",
        "        test_loader = DataLoader(\n",
        "            test_ds, batch_size=batch_size, shuffle=False,\n",
        "            collate_fn=collate_varlen_bin\n",
        "        )\n",
        "\n",
        "        # class imbalance for BCE pos_weight\n",
        "        class_counts = np.bincount(y_train.astype(int), minlength=2).astype(float)\n",
        "        if class_counts[1] == 0:\n",
        "            pos_weight_val = 1.0\n",
        "        else:\n",
        "            pos_weight_val = class_counts[0] / class_counts[1]\n",
        "        pos_weight = torch.tensor([pos_weight_val], dtype=torch.float32, device=device)\n",
        "\n",
        "        # latent_dim from first sequence\n",
        "        latent_dim_here = X_train_list_norm[0].shape[1]\n",
        "\n",
        "        model = LSTMAffectBinary(\n",
        "              latent_dim=latent_dim_here,\n",
        "              hidden_dim=hidden_dim,\n",
        "              num_layers=num_layers\n",
        "                                ).to(device)\n",
        "\n",
        "        opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "        loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "        # TRAIN\n",
        "        for ep in range(epochs):\n",
        "            model.train()\n",
        "            for bx, by, lengths in train_loader:\n",
        "                bx = bx.to(device)         # (B, T_max, D)\n",
        "                by = by.to(device)         # (B,) float\n",
        "                lengths = lengths.to(device)\n",
        "\n",
        "                opt.zero_grad()\n",
        "                logits = model(bx, lengths)  # (B,)\n",
        "                loss = loss_fn(logits, by)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "        # TRAIN PREDICTIONS FOR THRESHOLD\n",
        "        model.eval()\n",
        "        train_logits_all = []\n",
        "        train_truth_all  = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for bx, by, lengths in train_loader:\n",
        "                bx = bx.to(device)\n",
        "                lengths = lengths.to(device)\n",
        "                logits = model(bx, lengths)\n",
        "                train_logits_all.append(logits.cpu())\n",
        "                train_truth_all.append(by)\n",
        "\n",
        "        train_logits_all = torch.cat(train_logits_all, dim=0).numpy()\n",
        "        train_truth_all  = torch.cat(train_truth_all, dim=0).numpy()\n",
        "        train_probs_all  = 1.0 / (1.0 + np.exp(-train_logits_all))  # sigmoid\n",
        "\n",
        "        best_thr, best_f1_train = find_best_threshold(train_truth_all, train_probs_all)\n",
        "\n",
        "        #  TEST PREDICTIONS\n",
        "        test_logits_all = []\n",
        "        test_truth_all  = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for bx, by, lengths in test_loader:\n",
        "                bx = bx.to(device)\n",
        "                lengths = lengths.to(device)\n",
        "                logits = model(bx, lengths)\n",
        "                test_logits_all.append(logits.cpu())\n",
        "                test_truth_all.append(by)\n",
        "\n",
        "        test_logits_all = torch.cat(test_logits_all, dim=0).numpy()\n",
        "        test_truth_all  = torch.cat(test_truth_all, dim=0).numpy()\n",
        "        test_probs_all  = 1.0 / (1.0 + np.exp(-test_logits_all))\n",
        "\n",
        "        preds = (test_probs_all >= best_thr).astype(int)\n",
        "        y_int = test_truth_all.astype(int)\n",
        "\n",
        "        # metrics\n",
        "        acc = accuracy_score(y_int, preds)\n",
        "        f1_macro = f1_score(y_int, preds, average=\"macro\", zero_division=0)\n",
        "        prec_macro = precision_score(y_int, preds, average=\"macro\", zero_division=0)\n",
        "        rec_macro = recall_score(y_int, preds, average=\"macro\", zero_division=0)\n",
        "\n",
        "        rec_per_class = recall_score(\n",
        "            y_int, preds,\n",
        "            average=None,\n",
        "            labels=[0, 1],\n",
        "            zero_division=0\n",
        "        )\n",
        "        prec_per_class = precision_score(\n",
        "            y_int, preds,\n",
        "            average=None,\n",
        "            labels=[0, 1],\n",
        "            zero_division=0\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            auc = roc_auc_score(y_int, test_probs_all)\n",
        "        except ValueError:\n",
        "            auc = np.nan\n",
        "\n",
        "        cm = confusion_matrix(y_int, preds, labels=[0, 1])\n",
        "        cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
        "        cm_norm = np.nan_to_num(cm_norm)\n",
        "\n",
        "        results.append({\n",
        "            \"test_subject\": test_subj,\n",
        "            \"n_test\": len(y_int),\n",
        "            \"accuracy\": acc,\n",
        "            \"precision_macro\": prec_macro,\n",
        "            \"recall_macro\": rec_macro,\n",
        "            \"f1_macro\": f1_macro,\n",
        "            \"auc\": auc,\n",
        "            \"recall_per_class\": rec_per_class.tolist(),\n",
        "            \"precision_per_class\": prec_per_class.tolist(),\n",
        "            \"confusion_matrix\": cm_norm.tolist(),\n",
        "            \"y_test_unique\": list(np.unique(y_int)),\n",
        "            \"class_counts_train\": class_counts.tolist(),\n",
        "            \"pos_weight\": float(pos_weight_val),\n",
        "            \"best_threshold\": float(best_thr),\n",
        "            \"train_best_f1\": float(best_f1_train),\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results)\n"
      ],
      "metadata": {
        "id": "BSDYN5dpRkJi"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build cumulative HR-only dataset\n",
        "X_hr_cum, y_hr_cum, subj_hr_cum = build_dataset_binary_cumulative_hr(\n",
        "    day_embeddings, ema_labels_bin\n",
        ")\n",
        "\n",
        "set_seed(42)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "loso_hr_cum = evaluate_loso_lstm_binary_cumulative_hr(\n",
        "    X_hr_cum, y_hr_cum, subj_hr_cum,\n",
        "    latent_dim=X_hr_cum[0].shape[1],   # D from first sequence\n",
        "    hidden_dim=64,\n",
        "    num_layers=1,\n",
        "    epochs=30,\n",
        "    batch_size=16,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(loso_hr_cum)\n",
        "print(\"\\nAverage metrics:\")\n",
        "print(loso_hr_cum[[\"accuracy\", \"precision_macro\", \"recall_macro\", \"f1_macro\", \"auc\"]].mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NMpooL1R6YL",
        "outputId": "c6de52a1-1744-40e9-b550-c5db579ef54d"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-4214987100.py:171: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-4214987100.py:171: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-4214987100.py:171: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-4214987100.py:171: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-4214987100.py:171: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-4214987100.py:171: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-4214987100.py:171: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-4214987100.py:171: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-4214987100.py:171: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   test_subject  n_test  accuracy  precision_macro  recall_macro  f1_macro  \\\n",
            "0         cr001      19  0.315789         0.566667      0.617647  0.308123   \n",
            "1         cr003       7  0.428571         0.458333      0.450000  0.416667   \n",
            "2         cr005      14  0.500000         0.500000      0.250000  0.333333   \n",
            "3         cr006       5  0.400000         0.200000      0.500000  0.285714   \n",
            "4         cr011       4  0.000000         0.000000      0.000000  0.000000   \n",
            "5         cr016       4  0.500000         0.500000      0.250000  0.333333   \n",
            "6         cr031       8  0.000000         0.000000      0.000000  0.000000   \n",
            "7         cr032       9  1.000000         1.000000      1.000000  1.000000   \n",
            "8        crs002      14  0.571429         0.500000      0.285714  0.363636   \n",
            "9        crs007      15  1.000000         1.000000      1.000000  1.000000   \n",
            "10       crs010      17  0.529412         0.600000      0.733333  0.484848   \n",
            "11       crs011      13  0.000000         0.000000      0.000000  0.000000   \n",
            "12       crs014      14  0.000000         0.000000      0.000000  0.000000   \n",
            "13       crs015       4  0.750000         0.833333      0.750000  0.733333   \n",
            "14       crs019       6  0.833333         0.416667      0.500000  0.454545   \n",
            "\n",
            "         auc            recall_per_class         precision_per_class  \\\n",
            "0   0.382353  [1.0, 0.23529411764705882]  [0.13333333333333333, 1.0]   \n",
            "1   0.700000                  [0.5, 0.4]  [0.25, 0.6666666666666666]   \n",
            "2        NaN                  [0.0, 0.5]                  [0.0, 1.0]   \n",
            "3   0.500000                  [0.0, 1.0]                  [0.0, 0.4]   \n",
            "4        NaN                  [0.0, 0.0]                  [0.0, 0.0]   \n",
            "5        NaN                  [0.0, 0.5]                  [0.0, 1.0]   \n",
            "6        NaN                  [0.0, 0.0]                  [0.0, 0.0]   \n",
            "7        NaN                  [0.0, 1.0]                  [0.0, 1.0]   \n",
            "8        NaN   [0.5714285714285714, 0.0]                  [1.0, 0.0]   \n",
            "9        NaN                  [0.0, 1.0]                  [0.0, 1.0]   \n",
            "10  0.933333   [1.0, 0.4666666666666667]                  [0.2, 1.0]   \n",
            "11       NaN                  [0.0, 0.0]                  [0.0, 0.0]   \n",
            "12       NaN                  [0.0, 0.0]                  [0.0, 0.0]   \n",
            "13  1.000000                  [1.0, 0.5]   [0.6666666666666666, 1.0]   \n",
            "14  1.000000                  [0.0, 1.0]   [0.0, 0.8333333333333334]   \n",
            "\n",
            "                                     confusion_matrix y_test_unique  \\\n",
            "0   [[1.0, 0.0], [0.7647058823529411, 0.2352941176...        [0, 1]   \n",
            "1                            [[0.5, 0.5], [0.6, 0.4]]        [0, 1]   \n",
            "2                            [[0.0, 0.0], [0.5, 0.5]]           [1]   \n",
            "3                            [[0.0, 1.0], [0.0, 1.0]]        [0, 1]   \n",
            "4                            [[0.0, 1.0], [0.0, 0.0]]           [0]   \n",
            "5                            [[0.0, 0.0], [0.5, 0.5]]           [1]   \n",
            "6                            [[0.0, 1.0], [0.0, 0.0]]           [0]   \n",
            "7                            [[0.0, 0.0], [0.0, 1.0]]           [1]   \n",
            "8   [[0.5714285714285714, 0.42857142857142855], [0...           [0]   \n",
            "9                            [[0.0, 0.0], [0.0, 1.0]]           [1]   \n",
            "10  [[1.0, 0.0], [0.5333333333333333, 0.4666666666...        [0, 1]   \n",
            "11                           [[0.0, 1.0], [0.0, 0.0]]           [0]   \n",
            "12                           [[0.0, 1.0], [0.0, 0.0]]           [0]   \n",
            "13                           [[1.0, 0.0], [0.5, 0.5]]        [0, 1]   \n",
            "14                           [[0.0, 1.0], [0.0, 1.0]]        [0, 1]   \n",
            "\n",
            "   class_counts_train  pos_weight  best_threshold  train_best_f1  \n",
            "0        [63.0, 72.0]    0.875000            0.50       0.813559  \n",
            "1        [63.0, 84.0]    0.750000            0.30       0.821782  \n",
            "2        [65.0, 75.0]    0.866667            0.30       0.777202  \n",
            "3        [62.0, 87.0]    0.712644            0.25       0.820755  \n",
            "4        [61.0, 89.0]    0.685393            0.40       0.820276  \n",
            "5        [65.0, 85.0]    0.764706            0.40       0.805825  \n",
            "6        [57.0, 89.0]    0.640449            0.15       0.827907  \n",
            "7        [65.0, 80.0]    0.812500            0.15       0.776699  \n",
            "8        [51.0, 89.0]    0.573034            0.40       0.816901  \n",
            "9        [65.0, 74.0]    0.878378            0.35       0.804348  \n",
            "10       [63.0, 74.0]    0.851351            0.30       0.778947  \n",
            "11       [52.0, 89.0]    0.584270            0.25       0.835681  \n",
            "12       [51.0, 89.0]    0.573034            0.25       0.851675  \n",
            "13       [63.0, 87.0]    0.724138            0.15       0.813084  \n",
            "14       [64.0, 84.0]    0.761905            0.35       0.815534  \n",
            "\n",
            "Average metrics:\n",
            "accuracy           0.455236\n",
            "precision_macro    0.438333\n",
            "recall_macro       0.422446\n",
            "f1_macro           0.380902\n",
            "auc                0.752614\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM Version of HR + Prev EMA with cumulative n-1 HR data\n"
      ],
      "metadata": {
        "id": "iO1KZ22MwWqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMAffectBinaryWithEMA(nn.Module):\n",
        "    def __init__(self, latent_dim=64, hidden_dim=64, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=latent_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(hidden_dim + 1, 1)  # +1 for prev_ema\n",
        "\n",
        "    def forward(self, x, lengths, prev_ema):\n",
        "        packed = pack_padded_sequence(\n",
        "            x, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
        "        )\n",
        "        packed_out, _ = self.lstm(packed)\n",
        "        out, _ = pad_packed_sequence(packed_out, batch_first=True)\n",
        "\n",
        "        idx = (lengths - 1).unsqueeze(1).unsqueeze(2).expand(-1, 1, out.size(2))\n",
        "        last_h = out.gather(1, idx).squeeze(1)   # (B, hidden_dim)\n",
        "\n",
        "        last_h = self.dropout(last_h)\n",
        "        prev_ema = prev_ema.unsqueeze(1)         # (B, 1)\n",
        "        h_cat = torch.cat([last_h, prev_ema], dim=1)  # (B, hidden_dim+1)\n",
        "\n",
        "        logit = self.fc(h_cat).squeeze(1)        # (B,)\n",
        "        return logit\n"
      ],
      "metadata": {
        "id": "bYK4Umg7wgIs"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, precision_score, recall_score,\n",
        "    roc_auc_score, confusion_matrix\n",
        ")\n",
        "\n",
        "def evaluate_loso_lstm_binary_with_prev_ema_threshold_varlen(\n",
        "    X_list, prev_ema, y, subj_ids,\n",
        "    latent_dim=64,\n",
        "    hidden_dim=64,\n",
        "    num_layers=1,\n",
        "    epochs=30,\n",
        "    batch_size=16,\n",
        "    device=None\n",
        "):\n",
        "    \"\"\"\n",
        "    X_list: np.array (dtype=object) of length N, each element array(T_i, D)\n",
        "    prev_ema: (N,)\n",
        "    y: (N,)  0/1\n",
        "    subj_ids: (N,)\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    unique_subjs = np.unique(subj_ids)\n",
        "    results = []\n",
        "\n",
        "    for test_subj in unique_subjs:\n",
        "        test_mask = (subj_ids == test_subj)\n",
        "        train_mask = ~test_mask\n",
        "\n",
        "        X_train_list = X_list[train_mask]\n",
        "        prev_train   = prev_ema[train_mask]\n",
        "        y_train      = y[train_mask]\n",
        "\n",
        "        X_test_list  = X_list[test_mask]\n",
        "        prev_test    = prev_ema[test_mask]\n",
        "        y_test       = y[test_mask]\n",
        "\n",
        "        # need at least 2 test samples\n",
        "        if len(y_test) < 2:\n",
        "            continue\n",
        "\n",
        "        #  datasets / loaders\n",
        "        train_ds = AffectSequenceDatasetBinWithEMA_Var(X_train_list, prev_train, y_train)\n",
        "        test_ds  = AffectSequenceDatasetBinWithEMA_Var(X_test_list,  prev_test,  y_test)\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            train_ds, batch_size=batch_size, shuffle=True,\n",
        "            collate_fn=collate_varlen_bin_with_ema\n",
        "        )\n",
        "        test_loader  = DataLoader(\n",
        "            test_ds, batch_size=batch_size, shuffle=False,\n",
        "            collate_fn=collate_varlen_bin_with_ema\n",
        "        )\n",
        "\n",
        "        #  class imbalance (pos_weight for BCE)\n",
        "        class_counts = np.bincount(y_train.astype(int), minlength=2).astype(float)\n",
        "        # class_counts[0] = #neg, class_counts[1] = #pos\n",
        "        if class_counts[1] == 0:\n",
        "            pos_weight_val = 1.0\n",
        "        else:\n",
        "            pos_weight_val = class_counts[0] / class_counts[1]\n",
        "\n",
        "        pos_weight = torch.tensor([pos_weight_val], dtype=torch.float32, device=device)\n",
        "\n",
        "        # model / loss\n",
        "        # infer latent_dim from first sequence\n",
        "        if isinstance(X_list[0], np.ndarray):\n",
        "            latent_dim_here = X_list[0].shape[1]\n",
        "        else:\n",
        "            latent_dim_here = latent_dim\n",
        "\n",
        "        model = LSTMAffectBinaryWithEMA(\n",
        "              latent_dim=latent_dim_here,\n",
        "              hidden_dim=hidden_dim,\n",
        "              num_layers=num_layers\n",
        "                                ).to(device)\n",
        "\n",
        "        opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "        loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "        #  TRAIN\n",
        "        for ep in range(epochs):\n",
        "            model.train()\n",
        "            for bx, bprev, by, lengths in train_loader:\n",
        "                bx = bx.to(device)             # (B, T_max, D)\n",
        "                bprev = bprev.to(device)       # (B,)\n",
        "                by = by.to(device)             # (B,) float 0/1\n",
        "                lengths = lengths.to(device)\n",
        "\n",
        "                opt.zero_grad()\n",
        "                logits = model(bx, lengths, bprev)  # (B,)\n",
        "                loss = loss_fn(logits, by)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "        #  TRAIN PREDICTIONS FOR THRESHOLD TUNING\n",
        "        model.eval()\n",
        "        train_logits_all = []\n",
        "        train_truth_all  = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for bx, bprev, by, lengths in train_loader:\n",
        "                bx = bx.to(device)\n",
        "                bprev = bprev.to(device)\n",
        "                lengths = lengths.to(device)\n",
        "                logits = model(bx, lengths, bprev)  # (B,)\n",
        "                train_logits_all.append(logits.cpu())\n",
        "                train_truth_all.append(by)\n",
        "\n",
        "        train_logits_all = torch.cat(train_logits_all, dim=0).numpy()   # (N_train,)\n",
        "        train_truth_all  = torch.cat(train_truth_all, dim=0).numpy()    # (N_train,)\n",
        "        train_probs_all  = 1.0 / (1.0 + np.exp(-train_logits_all))      # sigmoid\n",
        "\n",
        "        best_thr, best_f1_train = find_best_threshold(train_truth_all, train_probs_all)\n",
        "\n",
        "        #  TEST PREDICTIONS\n",
        "        test_logits_all = []\n",
        "        test_truth_all  = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for bx, bprev, by, lengths in test_loader:\n",
        "                bx = bx.to(device)\n",
        "                bprev = bprev.to(device)\n",
        "                lengths = lengths.to(device)\n",
        "                logits = model(bx, lengths, bprev)\n",
        "                test_logits_all.append(logits.cpu())\n",
        "                test_truth_all.append(by)\n",
        "\n",
        "        test_logits_all = torch.cat(test_logits_all, dim=0).numpy()  # (N_test,)\n",
        "        test_truth_all  = torch.cat(test_truth_all, dim=0).numpy()   # (N_test,)\n",
        "        test_probs_all  = 1.0 / (1.0 + np.exp(-test_logits_all))\n",
        "\n",
        "        preds = (test_probs_all >= best_thr).astype(int)\n",
        "        y_int = test_truth_all.astype(int)\n",
        "\n",
        "        # metrics\n",
        "        acc = accuracy_score(y_int, preds)\n",
        "        f1_macro = f1_score(y_int, preds, average=\"macro\", zero_division=0)\n",
        "        prec_macro = precision_score(y_int, preds, average=\"macro\", zero_division=0)\n",
        "        rec_macro = recall_score(y_int, preds, average=\"macro\", zero_division=0)\n",
        "\n",
        "        rec_per_class = recall_score(\n",
        "            y_int, preds,\n",
        "            average=None,\n",
        "            labels=[0, 1],\n",
        "            zero_division=0\n",
        "        )\n",
        "        prec_per_class = precision_score(\n",
        "            y_int, preds,\n",
        "            average=None,\n",
        "            labels=[0, 1],\n",
        "            zero_division=0\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            auc = roc_auc_score(y_int, test_probs_all)\n",
        "        except ValueError:\n",
        "            auc = np.nan\n",
        "\n",
        "        cm = confusion_matrix(y_int, preds, labels=[0, 1])\n",
        "        cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
        "        cm_norm = np.nan_to_num(cm_norm)\n",
        "\n",
        "        results.append({\n",
        "            \"test_subject\": test_subj,\n",
        "            \"n_test\": len(y_int),\n",
        "            \"accuracy\": acc,\n",
        "            \"precision_macro\": prec_macro,\n",
        "            \"recall_macro\": rec_macro,\n",
        "            \"f1_macro\": f1_macro,\n",
        "            \"auc\": auc,\n",
        "            \"recall_per_class\": rec_per_class.tolist(),\n",
        "            \"precision_per_class\": prec_per_class.tolist(),\n",
        "            \"confusion_matrix\": cm_norm.tolist(),\n",
        "            \"y_test_unique\": list(np.unique(y_int)),\n",
        "            \"class_counts_train\": class_counts.tolist(),\n",
        "            \"pos_weight\": float(pos_weight_val),\n",
        "            \"best_threshold\": float(best_thr),\n",
        "            \"train_best_f1\": float(best_f1_train),\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KdRkxvHcx-mL"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_bin, prev_bin, y_bin, subj_bin = build_dataset_binary_cumulative_with_prev_ema(\n",
        "    day_embeddings, ema_labels_bin\n",
        ")\n",
        "\n",
        "set_seed(42)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "loso_bin_cum = evaluate_loso_lstm_binary_with_prev_ema_threshold_varlen(\n",
        "    X_bin, prev_bin, y_bin, subj_bin,\n",
        "    latent_dim=X_bin[0].shape[1],   # D from first sequence\n",
        "    hidden_dim=64,\n",
        "    num_layers=1,\n",
        "    epochs=30,\n",
        "    batch_size=16,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(loso_bin_cum)\n",
        "\n",
        "print(\"\\nAverage metrics:\")\n",
        "print(loso_bin_cum[[\"accuracy\", \"precision_macro\", \"recall_macro\", \"f1_macro\", \"auc\"]].mean())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "715170c8-e735-465d-9350-ca5ea6b496f9",
        "id": "aeY93D0kyq3v"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-3061278158.py:162: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-3061278158.py:162: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-3061278158.py:162: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-3061278158.py:162: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-3061278158.py:162: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-3061278158.py:162: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-3061278158.py:162: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-3061278158.py:162: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-3061278158.py:162: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   test_subject  n_test  accuracy  precision_macro  recall_macro  f1_macro  \\\n",
            "0         cr001      19  0.315789         0.566667      0.617647  0.308123   \n",
            "1         cr003       7  0.571429         0.541667      0.550000  0.533333   \n",
            "2         cr005      14  0.357143         0.500000      0.178571  0.263158   \n",
            "3         cr006       5  0.600000         0.750000      0.666667  0.583333   \n",
            "4         cr011       4  0.000000         0.000000      0.000000  0.000000   \n",
            "5         cr016       4  0.500000         0.500000      0.250000  0.333333   \n",
            "6         cr031       8  0.875000         0.500000      0.437500  0.466667   \n",
            "7         cr032       9  0.444444         0.500000      0.222222  0.307692   \n",
            "8        crs002      14  1.000000         1.000000      1.000000  1.000000   \n",
            "9        crs007      15  0.266667         0.500000      0.133333  0.210526   \n",
            "10       crs010      17  0.529412         0.600000      0.733333  0.484848   \n",
            "11       crs011      13  0.307692         0.500000      0.153846  0.235294   \n",
            "12       crs014      14  0.928571         0.500000      0.464286  0.481481   \n",
            "13       crs015       4  0.500000         0.250000      0.500000  0.333333   \n",
            "14       crs019       6  0.166667         0.250000      0.100000  0.142857   \n",
            "\n",
            "         auc            recall_per_class         precision_per_class  \\\n",
            "0   0.352941  [1.0, 0.23529411764705882]  [0.13333333333333333, 1.0]   \n",
            "1   0.600000                  [0.5, 0.6]  [0.3333333333333333, 0.75]   \n",
            "2        NaN  [0.0, 0.35714285714285715]                  [0.0, 1.0]   \n",
            "3   0.666667   [0.3333333333333333, 1.0]                  [1.0, 0.5]   \n",
            "4        NaN                  [0.0, 0.0]                  [0.0, 0.0]   \n",
            "5        NaN                  [0.0, 0.5]                  [0.0, 1.0]   \n",
            "6        NaN                [0.875, 0.0]                  [1.0, 0.0]   \n",
            "7        NaN   [0.0, 0.4444444444444444]                  [0.0, 1.0]   \n",
            "8        NaN                  [1.0, 0.0]                  [1.0, 0.0]   \n",
            "9        NaN  [0.0, 0.26666666666666666]                  [0.0, 1.0]   \n",
            "10  0.733333   [1.0, 0.4666666666666667]                  [0.2, 1.0]   \n",
            "11       NaN   [0.3076923076923077, 0.0]                  [1.0, 0.0]   \n",
            "12       NaN   [0.9285714285714286, 0.0]                  [1.0, 0.0]   \n",
            "13  0.500000                  [1.0, 0.0]                  [0.5, 0.0]   \n",
            "14  0.000000                  [0.0, 0.2]                  [0.0, 0.5]   \n",
            "\n",
            "                                     confusion_matrix y_test_unique  \\\n",
            "0   [[1.0, 0.0], [0.7647058823529411, 0.2352941176...        [0, 1]   \n",
            "1                            [[0.5, 0.5], [0.4, 0.6]]        [0, 1]   \n",
            "2   [[0.0, 0.0], [0.6428571428571429, 0.3571428571...           [1]   \n",
            "3   [[0.3333333333333333, 0.6666666666666666], [0....        [0, 1]   \n",
            "4                            [[0.0, 1.0], [0.0, 0.0]]           [0]   \n",
            "5                            [[0.0, 0.0], [0.5, 0.5]]           [1]   \n",
            "6                        [[0.875, 0.125], [0.0, 0.0]]           [0]   \n",
            "7   [[0.0, 0.0], [0.5555555555555556, 0.4444444444...           [1]   \n",
            "8                            [[1.0, 0.0], [0.0, 0.0]]           [0]   \n",
            "9   [[0.0, 0.0], [0.7333333333333333, 0.2666666666...           [1]   \n",
            "10  [[1.0, 0.0], [0.5333333333333333, 0.4666666666...        [0, 1]   \n",
            "11  [[0.3076923076923077, 0.6923076923076923], [0....           [0]   \n",
            "12  [[0.9285714285714286, 0.07142857142857142], [0...           [0]   \n",
            "13                           [[1.0, 0.0], [1.0, 0.0]]        [0, 1]   \n",
            "14                           [[0.0, 1.0], [0.8, 0.2]]        [0, 1]   \n",
            "\n",
            "   class_counts_train  pos_weight  best_threshold  train_best_f1  \n",
            "0        [63.0, 72.0]    0.875000             0.5       0.857143  \n",
            "1        [63.0, 84.0]    0.750000             0.5       0.874251  \n",
            "2        [65.0, 75.0]    0.866667             0.5       0.841379  \n",
            "3        [62.0, 87.0]    0.712644             0.5       0.902857  \n",
            "4        [61.0, 89.0]    0.685393             0.5       0.860215  \n",
            "5        [65.0, 85.0]    0.764706             0.5       0.887574  \n",
            "6        [57.0, 89.0]    0.640449             0.5       0.862069  \n",
            "7        [65.0, 80.0]    0.812500             0.5       0.808219  \n",
            "8        [51.0, 89.0]    0.573034             0.5       0.877193  \n",
            "9        [65.0, 74.0]    0.878378             0.5       0.816568  \n",
            "10       [63.0, 74.0]    0.851351             0.5       0.792208  \n",
            "11       [52.0, 89.0]    0.584270             0.5       0.849741  \n",
            "12       [51.0, 89.0]    0.573034             0.5       0.885057  \n",
            "13       [63.0, 87.0]    0.724138             0.5       0.845238  \n",
            "14       [64.0, 84.0]    0.761905             0.5       0.880952  \n",
            "\n",
            "Average metrics:\n",
            "accuracy           0.490854\n",
            "precision_macro    0.497222\n",
            "recall_macro       0.400494\n",
            "f1_macro           0.378932\n",
            "auc                0.475490\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM with Fixed H- HR only"
      ],
      "metadata": {
        "id": "5Dr19sRgzZpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMAffectClassifier(nn.Module):\n",
        "    def __init__(self, latent_dim=64, hidden_dim=64, num_layers=1, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=latent_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (batch, H, latent_dim)\n",
        "        \"\"\"\n",
        "        out, _ = self.lstm(x)           # (batch, H, hidden_dim)\n",
        "        h_last = out[:, -1, :]          # last timestep\n",
        "        logits = self.fc(h_last)        # (batch, num_classes)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "6Ci4ORoczhRI"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_loso_lstm_binary(\n",
        "    X, y, subj_ids,\n",
        "    latent_dim=64,\n",
        "    hidden_dim=64,\n",
        "    num_layers=1,\n",
        "    epochs=30,\n",
        "    batch_size=16,\n",
        "    device=None\n",
        "):\n",
        "    if device is None:\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    unique_subjs = np.unique(subj_ids)\n",
        "    results = []\n",
        "\n",
        "    for test_subj in unique_subjs:\n",
        "        test_mask = (subj_ids == test_subj)\n",
        "        train_mask = ~test_mask\n",
        "\n",
        "        X_train, y_train = X[train_mask], y[train_mask]\n",
        "        X_test,  y_test  = X[test_mask],  y[test_mask]\n",
        "\n",
        "        # we skip if test subject has < 2 samples OR only 1 class (AUC would be degenerate)\n",
        "        if len(y_test) < 2: #or len(np.unique(y_test)) < 2\n",
        "            continue\n",
        "\n",
        "        N_train,H, D = X_train.shape\n",
        "        N_test = X_test.shape[0]\n",
        "\n",
        "        X_train_flat = X_train.reshape(N_train, -1)\n",
        "        X_test_flat = X_test.reshape(N_test, -1)\n",
        "\n",
        "        scalar= StandardScaler()\n",
        "        X_train_scaled = scalar.fit_transform(X_train_flat)\n",
        "        X_test_scaled = scalar.transform(X_test_flat)\n",
        "\n",
        "        #reshape back to (N, H, latent_dim)\n",
        "        X_train = X_train_scaled.reshape(N_train, H, D)\n",
        "        X_test = X_test_scaled.reshape(N_test, H, D)\n",
        "\n",
        "        train_ds = AffectSequenceDatasetBin(X_train, y_train)\n",
        "        test_ds  = AffectSequenceDatasetBin(X_test,  y_test)\n",
        "\n",
        "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "        test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        #  class weights (to handle imbalance)\n",
        "        class_counts = np.bincount(y_train, minlength=2).astype(float)\n",
        "        class_counts[class_counts == 0.0] = 1.0\n",
        "        class_weights = 1.0 / class_counts\n",
        "        class_weights = class_weights * (2 / class_weights.sum())\n",
        "        weight_tensor = torch.tensor(class_weights, dtype=torch.float32, device=device)\n",
        "\n",
        "        model = LSTMAffectClassifier(\n",
        "            latent_dim=latent_dim,\n",
        "            hidden_dim=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            num_classes=2\n",
        "        ).to(device)\n",
        "\n",
        "        opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "        loss_fn = nn.CrossEntropyLoss(weight=weight_tensor)\n",
        "\n",
        "        #  Train\n",
        "        for ep in range(epochs):\n",
        "            model.train()\n",
        "            for bx, by in train_loader:\n",
        "                bx, by = bx.to(device), by.to(device)\n",
        "                opt.zero_grad()\n",
        "                logits = model(bx)              # (batch, 2)\n",
        "                loss = loss_fn(logits, by)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "        # Evaluate\n",
        "        model.eval()\n",
        "        all_logits = []\n",
        "        all_truth = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for bx, by in test_loader:\n",
        "                bx = bx.to(device)\n",
        "                logits = model(bx)\n",
        "                all_logits.append(logits.cpu())\n",
        "                all_truth.append(by)\n",
        "\n",
        "        all_logits = torch.cat(all_logits, dim=0)        # (N_test, 2)\n",
        "        all_truth = torch.cat(all_truth, dim=0).numpy()  # (N_test,)\n",
        "        probs = F.softmax(all_logits, dim=1).numpy()     # (N_test, 2)\n",
        "        preds = probs.argmax(axis=1)                     # (N_test,)\n",
        "        pos_probs = probs[:, 1]                          # for AUC\n",
        "\n",
        "        # Metrics\n",
        "        acc = accuracy_score(all_truth, preds)\n",
        "        f1_macro = f1_score(all_truth, preds, average=\"macro\", zero_division=0)\n",
        "        prec_macro = precision_score(all_truth, preds, average=\"macro\", zero_division=0)\n",
        "        rec_macro = recall_score(all_truth, preds, average=\"macro\", zero_division=0)\n",
        "\n",
        "        # per-class recall [0, 1]\n",
        "        label_list = [0, 1]\n",
        "        rec_per_class = recall_score(\n",
        "            all_truth, preds,\n",
        "            average=None,\n",
        "            labels=label_list,\n",
        "            zero_division=0\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            auc = roc_auc_score(all_truth, pos_probs)\n",
        "        except ValueError:\n",
        "            auc = np.nan\n",
        "\n",
        "        # Confusion matrix\n",
        "        cm = confusion_matrix(all_truth, preds, labels=label_list)\n",
        "\n",
        "        results.append({\n",
        "            \"test_subject\": test_subj,\n",
        "            \"n_test\": len(all_truth),\n",
        "            \"accuracy\": acc,\n",
        "            \"precision_macro\": prec_macro,\n",
        "            \"recall_macro\": rec_macro,\n",
        "            \"f1_macro\": f1_macro,\n",
        "            \"auc\": auc,\n",
        "            \"recall_per_class\": rec_per_class.tolist(),\n",
        "            \"confusion_matrix\": cm.tolist(),\n",
        "            \"y_test_unique\": list(np.unique(all_truth)),\n",
        "            \"class_counts_train\": class_counts.tolist(),\n",
        "            \"class_weights\": class_weights.tolist(),\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results)"
      ],
      "metadata": {
        "id": "CsG5P13R0wp6"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "latent_dim = X_mc.shape[2]   # 64\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "loso_lstm = evaluate_loso_lstm_binary(\n",
        "    X_mc, y_mc, subj_mc,\n",
        "    latent_dim=latent_dim,\n",
        "    hidden_dim=64,\n",
        "    num_layers=1,\n",
        "    epochs=30,\n",
        "    batch_size=16,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(loso_lstm)\n",
        "print(\"\\nAverage metrics across subjects:\")\n",
        "print(loso_lstm[[\"accuracy\", \"precision_macro\", \"recall_macro\", \"f1_macro\", \"auc\"]].mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlvVmPNy1H8Q",
        "outputId": "1baf7446-a877-44f9-ed97-dbd99d724b71"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   test_subject  n_test  accuracy  precision_macro  recall_macro  f1_macro  \\\n",
            "0         cr001      21  0.476190         0.416667      0.263158  0.322581   \n",
            "1         cr003       7  0.285714         0.142857      0.500000  0.222222   \n",
            "2         cr004       2  0.500000         0.250000      0.500000  0.333333   \n",
            "3         cr005      15  0.866667         0.500000      0.433333  0.464286   \n",
            "4         cr006      14  0.285714         0.250000      0.200000  0.222222   \n",
            "5         cr011       7  0.285714         0.500000      0.142857  0.222222   \n",
            "6         cr016      10  1.000000         1.000000      1.000000  1.000000   \n",
            "7         cr031      19  0.789474         0.604167      0.604167  0.604167   \n",
            "8         cr032      11  0.818182         0.500000      0.409091  0.450000   \n",
            "9        crs002      17  0.529412         0.500000      0.264706  0.346154   \n",
            "10       crs007      15  0.066667         0.500000      0.033333  0.062500   \n",
            "11       crs010      20  0.300000         0.588235      0.588235  0.300000   \n",
            "12       crs011      16  0.500000         0.500000      0.250000  0.333333   \n",
            "13       crs014      14  0.642857         0.500000      0.321429  0.391304   \n",
            "14       crs015       8  0.750000         0.666667      0.666667  0.666667   \n",
            "15       crs019       6  0.500000         0.375000      0.300000  0.333333   \n",
            "\n",
            "         auc             recall_per_class   confusion_matrix y_test_unique  \\\n",
            "0   0.421053    [0.0, 0.5263157894736842]  [[0, 2], [9, 10]]        [0, 1]   \n",
            "1   0.800000                   [1.0, 0.0]   [[2, 0], [5, 0]]        [0, 1]   \n",
            "2   1.000000                   [0.0, 1.0]   [[0, 1], [0, 1]]        [0, 1]   \n",
            "3        NaN    [0.0, 0.8666666666666667]  [[0, 0], [2, 13]]           [1]   \n",
            "4   0.150000                   [0.0, 0.4]   [[0, 4], [6, 4]]        [0, 1]   \n",
            "5        NaN    [0.2857142857142857, 0.0]   [[2, 5], [0, 0]]           [0]   \n",
            "6        NaN                   [0.0, 1.0]  [[0, 0], [0, 10]]           [1]   \n",
            "7   0.708333  [0.875, 0.3333333333333333]  [[14, 2], [2, 1]]        [0, 1]   \n",
            "8        NaN    [0.0, 0.8181818181818182]   [[0, 0], [2, 9]]           [1]   \n",
            "9        NaN    [0.5294117647058824, 0.0]   [[9, 8], [0, 0]]           [0]   \n",
            "10       NaN   [0.0, 0.06666666666666667]  [[0, 0], [14, 1]]           [1]   \n",
            "11  0.372549   [1.0, 0.17647058823529413]  [[3, 0], [14, 3]]        [0, 1]   \n",
            "12       NaN                   [0.5, 0.0]   [[8, 8], [0, 0]]           [0]   \n",
            "13       NaN    [0.6428571428571429, 0.0]   [[9, 5], [0, 0]]           [0]   \n",
            "14  0.666667    [0.8333333333333334, 0.5]   [[5, 1], [1, 1]]        [0, 1]   \n",
            "15  0.600000                   [0.0, 0.6]   [[0, 1], [2, 3]]        [0, 1]   \n",
            "\n",
            "   class_counts_train                             class_weights  \n",
            "0        [87.0, 95.0]   [1.043956043956044, 0.9560439560439562]  \n",
            "1       [87.0, 109.0]  [1.1122448979591837, 0.8877551020408163]  \n",
            "2       [88.0, 113.0]  [1.1243781094527363, 0.8756218905472636]  \n",
            "3        [89.0, 99.0]    [1.053191489361702, 0.946808510638298]  \n",
            "4       [85.0, 104.0]  [1.1005291005291005, 0.8994708994708995]  \n",
            "5       [82.0, 114.0]    [1.163265306122449, 0.836734693877551]  \n",
            "6       [89.0, 104.0]    [1.077720207253886, 0.922279792746114]  \n",
            "7       [73.0, 111.0]  [1.2065217391304348, 0.7934782608695653]  \n",
            "8       [89.0, 103.0]  [1.0729166666666667, 0.9270833333333334]  \n",
            "9       [72.0, 114.0]  [1.2258064516129032, 0.7741935483870968]  \n",
            "10       [89.0, 99.0]    [1.053191489361702, 0.946808510638298]  \n",
            "11       [86.0, 97.0]  [1.0601092896174864, 0.9398907103825138]  \n",
            "12      [73.0, 114.0]  [1.2192513368983957, 0.7807486631016043]  \n",
            "13      [75.0, 114.0]  [1.2063492063492063, 0.7936507936507935]  \n",
            "14      [83.0, 112.0]  [1.1487179487179489, 0.8512820512820513]  \n",
            "15      [88.0, 109.0]  [1.1065989847715736, 0.8934010152284264]  \n",
            "\n",
            "Average metrics across subjects:\n",
            "accuracy           0.537287\n",
            "precision_macro    0.487100\n",
            "recall_macro       0.404811\n",
            "f1_macro           0.392145\n",
            "auc                0.589825\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM With Fixed H- HR+Prev EMA"
      ],
      "metadata": {
        "id": "7U4YBd0C2pnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class LSTMAffectClassifierWithEMA(nn.Module):\n",
        "    def __init__(self, latent_dim=64, hidden_dim=64, num_layers=1, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=latent_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        # +1 to concatenate prev_ema as extra feature\n",
        "        self.fc = nn.Linear(hidden_dim + 1, num_classes)\n",
        "\n",
        "    def forward(self, x, prev_ema):\n",
        "        \"\"\"\n",
        "        x:        (batch, H, latent_dim)\n",
        "        prev_ema: (batch,)  scalar 0/1 for previous day's affect\n",
        "        \"\"\"\n",
        "        out, (h_n, c_n) = self.lstm(x)        # out: (B, H, hidden_dim), h_n: (num_layers, B, hidden_dim)\n",
        "        h_last = out[:, -1, :]                # last time step (B, hidden_dim)\n",
        "        h_last = self.dropout(h_last)\n",
        "\n",
        "        prev_ema = prev_ema.unsqueeze(1)      # (B, 1)\n",
        "        h_cat = torch.cat([h_last, prev_ema], dim=1)  # (B, hidden_dim + 1)\n",
        "\n",
        "        logits = self.fc(h_cat)               # (B, num_classes)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "ad8YMTDHHOm0"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM = 64\n",
        "\n",
        "def build_dataset_binary_fixedH_with_prev_ema(day_embeddings, ema_labels_bin, H=7):\n",
        "    \"\"\"\n",
        "    For each subject:\n",
        "      - Input X: (H, D) embeddings for days [n-H, ..., n-1]\n",
        "      - prev_ema: EMA_bin at day (n-1)\n",
        "      - Target y: EMA_bin at day n\n",
        "\n",
        "    Returns:\n",
        "      X:        (N, H, D)\n",
        "      prev_ema: (N,)\n",
        "      y:        (N,)\n",
        "      subj_ids: (N,)\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "\n",
        "    X_list, prev_ema_list, y_list, subj_list = [], [], [], []\n",
        "\n",
        "    for subj in day_embeddings:\n",
        "        if subj not in ema_labels_bin:\n",
        "            continue\n",
        "\n",
        "        days = sorted(day_embeddings[subj].keys())\n",
        "\n",
        "        for i in range(H, len(days)):\n",
        "            day_n = days[i]          # prediction day\n",
        "            prev_days = days[i-H:i]  # H previous days\n",
        "            day_prev = days[i-1]     # immediate previous day\n",
        "\n",
        "            # need labels for day_n and day_prev\n",
        "            if day_n not in ema_labels_bin[subj]:\n",
        "                continue\n",
        "            if day_prev not in ema_labels_bin[subj]:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                seq = np.vstack([day_embeddings[subj][d] for d in prev_days])  # (H, D)\n",
        "            except KeyError:\n",
        "                continue\n",
        "\n",
        "            X_list.append(seq)\n",
        "            y_list.append(ema_labels_bin[subj][day_n])\n",
        "            prev_ema_list.append(ema_labels_bin[subj][day_prev])\n",
        "            subj_list.append(subj)\n",
        "\n",
        "    X = np.array(X_list, dtype=np.float32)\n",
        "    prev_ema = np.array(prev_ema_list, dtype=np.float32)\n",
        "    y = np.array(y_list, dtype=int)\n",
        "    subj_ids = np.array(subj_list)\n",
        "\n",
        "    return X, prev_ema, y, subj_ids\n"
      ],
      "metadata": {
        "id": "bfBWPpHLKnY0"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, precision_score, recall_score,\n",
        "    roc_auc_score, confusion_matrix\n",
        ")\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def find_best_threshold(y_true, y_prob, steps=17):\n",
        "    \"\"\"\n",
        "    y_true: (N,) binary 0/1\n",
        "    y_prob: (N,) predicted prob of class 1\n",
        "    \"\"\"\n",
        "    best_t = 0.5\n",
        "    best_f1 = -1.0\n",
        "    for t in np.linspace(0.1, 0.9, steps):\n",
        "        y_pred = (y_prob >= t).astype(int)\n",
        "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            best_t = t\n",
        "    return best_t, best_f1\n",
        "\n",
        "\n",
        "def evaluate_loso_lstm_binary_with_prev_ema_fixedH(\n",
        "    X, prev_ema, y, subj_ids,\n",
        "    latent_dim=64,\n",
        "    hidden_dim=64,\n",
        "    num_layers=1,\n",
        "    epochs=30,\n",
        "    batch_size=16,\n",
        "    device=None\n",
        "):\n",
        "    \"\"\"\n",
        "    X:        (N, H, D)  HR embeddings\n",
        "    prev_ema: (N,)       previous day's EMA (0/1)\n",
        "    y:        (N,)       target EMA (0/1)\n",
        "    subj_ids: (N,)       subject ids\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    unique_subjs = np.unique(subj_ids)\n",
        "    results = []\n",
        "\n",
        "    for test_subj in unique_subjs:\n",
        "        test_mask = (subj_ids == test_subj)\n",
        "        train_mask = ~test_mask\n",
        "\n",
        "        X_train, prev_train, y_train = X[train_mask], prev_ema[train_mask], y[train_mask]\n",
        "        X_test,  prev_test,  y_test  = X[test_mask],  prev_ema[test_mask],  y[test_mask]\n",
        "\n",
        "        # skip tiny test sets\n",
        "        if len(y_test) < 2:\n",
        "            continue\n",
        "\n",
        "        # Normalize features (train only)\n",
        "        N_train, H, D = X_train.shape\n",
        "        N_test = X_test.shape[0]\n",
        "\n",
        "        X_train_flat = X_train.reshape(N_train, -1)\n",
        "        X_test_flat  = X_test.reshape(N_test,  -1)\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train_flat)\n",
        "        X_test_scaled  = scaler.transform(X_test_flat)\n",
        "\n",
        "        X_train = X_train_scaled.reshape(N_train, H, D)\n",
        "        X_test  = X_test_scaled.reshape(N_test,  H, D)\n",
        "\n",
        "        train_ds = AffectSequenceDatasetBinWithEMA(X_train, prev_train, y_train)\n",
        "        test_ds  = AffectSequenceDatasetBinWithEMA(X_test,  prev_test,  y_test)\n",
        "\n",
        "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "        test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        #  class weights (handle imbalance)\n",
        "        class_counts = np.bincount(y_train, minlength=2).astype(float)\n",
        "        class_counts[class_counts == 0.0] = 1.0\n",
        "        class_weights = 1.0 / class_counts\n",
        "        class_weights = class_weights * (2 / class_weights.sum())\n",
        "        weight_tensor = torch.tensor(class_weights, dtype=torch.float32, device=device)\n",
        "\n",
        "        # Model\n",
        "        model = LSTMAffectClassifierWithEMA(\n",
        "            latent_dim=latent_dim,\n",
        "            hidden_dim=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            num_classes=2\n",
        "        ).to(device)\n",
        "\n",
        "        opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "        loss_fn = nn.CrossEntropyLoss(weight=weight_tensor)\n",
        "\n",
        "        # Train\n",
        "        for ep in range(epochs):\n",
        "            model.train()\n",
        "            for bx, bprev, by in train_loader:\n",
        "                bx, bprev, by = bx.to(device), bprev.to(device), by.to(device)\n",
        "                opt.zero_grad()\n",
        "                logits = model(bx, bprev)        # (B, 2)\n",
        "                loss = loss_fn(logits, by)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "        # Train predictions for threshold tuning\n",
        "        model.eval()\n",
        "        train_logits_all = []\n",
        "        train_truth_all  = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for bx, bprev, by in train_loader:\n",
        "                bx, bprev = bx.to(device), bprev.to(device)\n",
        "                logits = model(bx, bprev)\n",
        "                train_logits_all.append(logits.cpu())\n",
        "                train_truth_all.append(by)\n",
        "\n",
        "        train_logits_all = torch.cat(train_logits_all, dim=0).cpu()        # (N_train_fold, 2)\n",
        "        train_truth_all  = torch.cat(train_truth_all, dim=0).cpu().numpy() # (N_train_fold,)\n",
        "        train_probs_all  = F.softmax(train_logits_all, dim=1).cpu().numpy()[:, 1]\n",
        "\n",
        "        best_thr, best_f1_train = find_best_threshold(train_truth_all, train_probs_all)\n",
        "\n",
        "        # Test predictions\n",
        "        test_logits_all = []\n",
        "        test_truth_all  = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for bx, bprev, by in test_loader:\n",
        "                bx, bprev = bx.to(device), bprev.to(device)\n",
        "                logits = model(bx, bprev)\n",
        "                test_logits_all.append(logits.cpu())\n",
        "                test_truth_all.append(by)\n",
        "\n",
        "        test_logits_all = torch.cat(test_logits_all, dim=0).cpu()\n",
        "        test_truth_all  = torch.cat(test_truth_all, dim=0).cpu().numpy()\n",
        "        test_probs_all  = F.softmax(test_logits_all, dim=1).cpu().numpy()[:, 1]\n",
        "\n",
        "        preds = (test_probs_all >= best_thr).astype(int)\n",
        "\n",
        "        # Metrics\n",
        "        acc = accuracy_score(test_truth_all, preds)\n",
        "        f1_macro = f1_score(test_truth_all, preds, average=\"macro\", zero_division=0)\n",
        "        prec_macro = precision_score(test_truth_all, preds, average=\"macro\", zero_division=0)\n",
        "        rec_macro = recall_score(test_truth_all, preds, average=\"macro\", zero_division=0)\n",
        "\n",
        "        label_list = [0, 1]\n",
        "        rec_per_class = recall_score(\n",
        "            test_truth_all, preds,\n",
        "            average=None,\n",
        "            labels=label_list,\n",
        "            zero_division=0\n",
        "        )\n",
        "        prec_per_class = precision_score(\n",
        "            test_truth_all, preds,\n",
        "            average=None,\n",
        "            labels=label_list,\n",
        "            zero_division=0\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            auc = roc_auc_score(test_truth_all, test_probs_all)\n",
        "        except ValueError:\n",
        "            auc = np.nan\n",
        "\n",
        "        cm = confusion_matrix(test_truth_all, preds, labels=label_list)\n",
        "        cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
        "        cm_norm = np.nan_to_num(cm_norm)\n",
        "\n",
        "        results.append({\n",
        "            \"test_subject\": test_subj,\n",
        "            \"n_test\": len(test_truth_all),\n",
        "            \"accuracy\": acc,\n",
        "            \"precision_macro\": prec_macro,\n",
        "            \"recall_macro\": rec_macro,\n",
        "            \"f1_macro\": f1_macro,\n",
        "            \"auc\": auc,\n",
        "            \"recall_per_class\": rec_per_class.tolist(),\n",
        "            \"precision_per_class\": prec_per_class.tolist(),\n",
        "            \"confusion_matrix\": cm_norm.tolist(),\n",
        "            \"y_test_unique\": list(np.unique(test_truth_all)),\n",
        "            \"class_counts_train\": class_counts.tolist(),\n",
        "            \"class_weights\": class_weights.tolist(),\n",
        "            \"best_threshold\": float(best_thr),\n",
        "            \"train_best_f1\": float(best_f1_train),\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results)\n"
      ],
      "metadata": {
        "id": "OJAAyGb3LCn9"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build fixed-H dataset (e.g., H = 7 days history)\n",
        "H = 7\n",
        "X_H, prev_H, y_H, subj_H = build_dataset_binary_fixedH_with_prev_ema(\n",
        "    day_embeddings, ema_labels_bin, H=H\n",
        ")\n",
        "\n",
        "set_seed(42)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "latent_dim = X_H.shape[2]  # 32 or 64 depending on autoencoder\n",
        "\n",
        "loso_lstm_H = evaluate_loso_lstm_binary_with_prev_ema_fixedH(\n",
        "    X_H, prev_H, y_H, subj_H,\n",
        "    latent_dim=latent_dim,\n",
        "    hidden_dim=64,\n",
        "    num_layers=1,\n",
        "    epochs=30,\n",
        "    batch_size=16,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(loso_lstm_H)\n",
        "print(\"\\nAverage metrics:\")\n",
        "print(loso_lstm_H[[\"accuracy\",\"precision_macro\",\"recall_macro\",\"f1_macro\",\"auc\"]].mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJtIYXMoLM5N",
        "outputId": "3ad001cc-b22b-416c-97bc-d83578289955"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-2411081043.py:170: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-2411081043.py:170: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-2411081043.py:170: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-2411081043.py:170: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-2411081043.py:170: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-2411081043.py:170: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-2411081043.py:170: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-2411081043.py:170: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-2411081043.py:170: RuntimeWarning: invalid value encountered in divide\n",
            "  cm_norm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   test_subject  n_test  accuracy  precision_macro  recall_macro  f1_macro  \\\n",
            "0         cr001      19  0.526316         0.416667      0.294118  0.344828   \n",
            "1         cr003       7  0.285714         0.142857      0.500000  0.222222   \n",
            "2         cr005      14  0.928571         0.500000      0.464286  0.481481   \n",
            "3         cr006       5  0.200000         0.125000      0.250000  0.166667   \n",
            "4         cr011       4  1.000000         1.000000      1.000000  1.000000   \n",
            "5         cr016       4  1.000000         1.000000      1.000000  1.000000   \n",
            "6         cr031       8  0.625000         0.500000      0.312500  0.384615   \n",
            "7         cr032       9  1.000000         1.000000      1.000000  1.000000   \n",
            "8        crs002      14  0.428571         0.500000      0.214286  0.300000   \n",
            "9        crs007      15  0.200000         0.500000      0.100000  0.166667   \n",
            "10       crs010      17  0.235294         0.566667      0.566667  0.235294   \n",
            "11       crs011      13  0.307692         0.500000      0.153846  0.235294   \n",
            "12       crs014      14  0.785714         0.500000      0.392857  0.440000   \n",
            "13       crs015       4  0.750000         0.833333      0.750000  0.733333   \n",
            "14       crs019       6  0.666667         0.666667      0.800000  0.625000   \n",
            "\n",
            "         auc            recall_per_class         precision_per_class  \\\n",
            "0   0.235294   [0.0, 0.5882352941176471]   [0.0, 0.8333333333333334]   \n",
            "1   0.600000                  [1.0, 0.0]   [0.2857142857142857, 0.0]   \n",
            "2        NaN   [0.0, 0.9285714285714286]                  [0.0, 1.0]   \n",
            "3   0.000000                  [0.0, 0.5]                 [0.0, 0.25]   \n",
            "4        NaN                  [1.0, 0.0]                  [1.0, 0.0]   \n",
            "5        NaN                  [0.0, 1.0]                  [0.0, 1.0]   \n",
            "6        NaN                [0.625, 0.0]                  [1.0, 0.0]   \n",
            "7        NaN                  [0.0, 1.0]                  [0.0, 1.0]   \n",
            "8        NaN  [0.42857142857142855, 0.0]                  [1.0, 0.0]   \n",
            "9        NaN                  [0.0, 0.2]                  [0.0, 1.0]   \n",
            "10  0.333333  [1.0, 0.13333333333333333]  [0.13333333333333333, 1.0]   \n",
            "11       NaN   [0.3076923076923077, 0.0]                  [1.0, 0.0]   \n",
            "12       NaN   [0.7857142857142857, 0.0]                  [1.0, 0.0]   \n",
            "13  0.500000                  [1.0, 0.5]   [0.6666666666666666, 1.0]   \n",
            "14  1.000000                  [1.0, 0.6]   [0.3333333333333333, 1.0]   \n",
            "\n",
            "                                     confusion_matrix y_test_unique  \\\n",
            "0   [[0.0, 1.0], [0.4117647058823529, 0.5882352941...        [0, 1]   \n",
            "1                            [[1.0, 0.0], [1.0, 0.0]]        [0, 1]   \n",
            "2   [[0.0, 0.0], [0.07142857142857142, 0.928571428...           [1]   \n",
            "3                            [[0.0, 1.0], [0.5, 0.5]]        [0, 1]   \n",
            "4                            [[1.0, 0.0], [0.0, 0.0]]           [0]   \n",
            "5                            [[0.0, 0.0], [0.0, 1.0]]           [1]   \n",
            "6                        [[0.625, 0.375], [0.0, 0.0]]           [0]   \n",
            "7                            [[0.0, 0.0], [0.0, 1.0]]           [1]   \n",
            "8   [[0.42857142857142855, 0.5714285714285714], [0...           [0]   \n",
            "9                            [[0.0, 0.0], [0.8, 0.2]]           [1]   \n",
            "10  [[1.0, 0.0], [0.8666666666666667, 0.1333333333...        [0, 1]   \n",
            "11  [[0.3076923076923077, 0.6923076923076923], [0....           [0]   \n",
            "12  [[0.7857142857142857, 0.21428571428571427], [0...           [0]   \n",
            "13                           [[1.0, 0.0], [0.5, 0.5]]        [0, 1]   \n",
            "14                           [[1.0, 0.0], [0.4, 0.6]]        [0, 1]   \n",
            "\n",
            "   class_counts_train                             class_weights  \\\n",
            "0        [63.0, 72.0]  [1.0666666666666667, 0.9333333333333333]   \n",
            "1        [63.0, 84.0]  [1.1428571428571428, 0.8571428571428571]   \n",
            "2        [65.0, 75.0]  [1.0714285714285714, 0.9285714285714286]   \n",
            "3        [62.0, 87.0]   [1.1677852348993287, 0.832214765100671]   \n",
            "4        [61.0, 89.0]  [1.1866666666666668, 0.8133333333333334]   \n",
            "5        [65.0, 85.0]  [1.1333333333333335, 0.8666666666666667]   \n",
            "6        [57.0, 89.0]  [1.2191780821917808, 0.7808219178082192]   \n",
            "7        [65.0, 80.0]    [1.103448275862069, 0.896551724137931]   \n",
            "8        [51.0, 89.0]  [1.2714285714285714, 0.7285714285714285]   \n",
            "9        [65.0, 74.0]   [1.064748201438849, 0.9352517985611511]   \n",
            "10       [63.0, 74.0]  [1.0802919708029197, 0.9197080291970804]   \n",
            "11       [52.0, 89.0]  [1.2624113475177308, 0.7375886524822696]   \n",
            "12       [51.0, 89.0]  [1.2714285714285714, 0.7285714285714285]   \n",
            "13       [63.0, 87.0]                              [1.16, 0.84]   \n",
            "14       [64.0, 84.0]   [1.135135135135135, 0.8648648648648648]   \n",
            "\n",
            "    best_threshold  train_best_f1  \n",
            "0             0.10       1.000000  \n",
            "1             0.40       0.994083  \n",
            "2             0.35       0.986667  \n",
            "3             0.75       1.000000  \n",
            "4             0.35       0.994413  \n",
            "5             0.45       0.994152  \n",
            "6             0.75       1.000000  \n",
            "7             0.45       0.987654  \n",
            "8             0.75       1.000000  \n",
            "9             0.65       1.000000  \n",
            "10            0.20       0.993289  \n",
            "11            0.35       1.000000  \n",
            "12            0.65       1.000000  \n",
            "13            0.40       0.994286  \n",
            "14            0.40       0.994083  \n",
            "\n",
            "Average metrics:\n",
            "accuracy           0.595969\n",
            "precision_macro    0.583413\n",
            "recall_macro       0.519904\n",
            "f1_macro           0.489027\n",
            "auc                0.444771\n",
            "dtype: float64\n"
          ]
        }
      ]
    }
  ]
}