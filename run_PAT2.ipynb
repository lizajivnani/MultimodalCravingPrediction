{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4530867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from sys import exit as e\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score\n",
    "import torch.optim as optim\n",
    "import torch.cuda.amp\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4e59bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch tasks will run on: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "# Define the device: use CUDA if available, otherwise fall back to CPU\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    print(f\"PyTorch tasks will run on: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(\"PyTorch tasks will run on: CPU (GPU not available or detected)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29430ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "MAX_CONTEXT_LENGTH = 7  # look back these many days of fitbit HR for context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a678b2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data\n",
    "\n",
    "df_hr = pd.read_csv('../data/Combined_Fitbit/Filtered_Fitbit_HR.csv')\n",
    "df_ema = pd.read_csv('../data/EMA/Filtered_EMA_Q1_Q4.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8d6c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dict for easy lookup: {(participant_id, date): hr_data_array}\n",
    "hr_data_dict = {}\n",
    "for index, row in df_hr.iterrows():\n",
    "    participant_id = row['subject']\n",
    "    date, time = pd.to_datetime(row['datetime']).date(), pd.to_datetime(row['datetime']).time()\n",
    "\n",
    "    hr_value = row['value']\n",
    "    key = (participant_id, date)\n",
    "    if key not in hr_data_dict:\n",
    "        hr_data_dict[key] = np.zeros((1440,)) \n",
    "    \n",
    "    minute_of_day = time.hour * 60 + time.minute\n",
    "    hr_data_dict[key][minute_of_day] = hr_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2fdc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_data_dict2 = hr_data_dict.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472510d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_data_dict = hr_data_dict2.copy()\n",
    "\n",
    "# Per user normalization\n",
    "\n",
    "per_user_data = df_hr.groupby('subject')\n",
    "\n",
    "# compute per user mean and std dev\n",
    "per_sub_mean = per_user_data['value'].mean()\n",
    "per_sub_std = per_user_data['value'].std()\n",
    "\n",
    "# normalize\n",
    "for sub, sub_mean in per_sub_mean.items():\n",
    "\n",
    "    sub_std = per_sub_std[sub]\n",
    "\n",
    "    for key in hr_data_dict:\n",
    "        if sub in key:\n",
    "            # only normalize if non-zero\n",
    "            hr_data_dict[key] = np.where(hr_data_dict[key] != 0, \n",
    "                                        (hr_data_dict[key] - sub_mean) / sub_std, \n",
    "                                        0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861efb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.45133713 -0.54642306 -0.64150898 ... -0.16607936 -0.35625121\n",
      " -0.35625121]\n"
     ]
    }
   ],
   "source": [
    "# demo usage\n",
    "\n",
    "participant_id = 'CR001'\n",
    "date = pd.to_datetime('2021-07-28').date()\n",
    "\n",
    "# Get the array for that participant and date\n",
    "hr_array = hr_data_dict[(participant_id, date)]\n",
    "print(hr_array)  # This will print the HR data array for the specified participant and date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f799e247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X: (BATCH_SIZE, MAX_CONTEXT_LENGTH, 1440, 1)\n",
    "# Y: (N, 4) where N is total number of EMA samples\n",
    "\n",
    "\n",
    "\n",
    "X_list, Y_list, sub_list = [], [], []\n",
    "\n",
    "for idx, row in df_ema.iterrows():\n",
    "    participant = row['User_ID']\n",
    "    target_date = pd.to_datetime(row['datetime']).date()\n",
    "\n",
    "    # Create an empty block for this person: Shape (7, 1440, 1)\n",
    "    sample_block = np.zeros((MAX_CONTEXT_LENGTH, 1440, 1))\n",
    "\n",
    "    for i in range(MAX_CONTEXT_LENGTH):\n",
    "        look_back_date = target_date - timedelta(days=(i+1))\n",
    "\n",
    "        if (participant, look_back_date) in hr_data_dict:\n",
    "            sample_block[i, :, 0] = hr_data_dict[(participant, look_back_date)]\n",
    "\n",
    "\n",
    "        X_list.append(sample_block)\n",
    "        Y_list.append([row['Q1'], row['Q2'], row['Q3'], row['Q4']])\n",
    "        sub_list.append(participant)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0882d2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final = np.stack(X_list)\n",
    "Y_final = np.array(Y_list)\n",
    "\n",
    "X_final = X_final.astype(np.float32)\n",
    "Y_final = Y_final.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af96a3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FitbitEMADataset(Dataset):\n",
    "    def __init__(self, x_data, y_data):\n",
    "        # Store the data as Tensors\n",
    "        self.x_data = torch.from_numpy(x_data)\n",
    "        self.y_data = torch.from_numpy(y_data)\n",
    "        \n",
    "    def __len__(self):\n",
    "        # Tells the loader how many samples total\n",
    "        return len(self.y_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 1. Grab the feature and label\n",
    "        x = self.x_data[idx]  # Shape: (7, 1440, 1)\n",
    "        y = self.y_data[idx]  # Shape: (4,)\n",
    "        \n",
    "        # 2. Create the Masks dynamically\n",
    "        # -------------------------------\n",
    "        \n",
    "        # Mask A: The Day Mask (Shape: 7)\n",
    "        # If the whole day is 0s, it's a padding day.\n",
    "        # We check if the sum of absolute values in a day is 0.\n",
    "        # (This works because you normalized data, so an actual full day of 0.0 is statistically impossible)\n",
    "        day_sum = torch.sum(torch.abs(x), dim=(1, 2)) # Sum across 1440 mins and 1 channel\n",
    "        day_mask = (day_sum == 0) # True if day is empty, False if data exists\n",
    "        \n",
    "        # Mask B: The Minute Mask (Shape: 7, 1440)\n",
    "        # We need to flatten the last dim first -> (7, 1440)\n",
    "        # True if value is 0 (missing), False if real\n",
    "        min_mask = (x.squeeze(-1) == 0)\n",
    "        \n",
    "        return {\n",
    "            'x': x,\n",
    "            'y': y,\n",
    "            'day_mask': day_mask,\n",
    "            'min_mask': min_mask\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dace15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Batch Shape: torch.Size([32, 7, 1440, 1])\n",
      "Label Batch Shape: torch.Size([32, 4])\n",
      "Day Mask Shape:    torch.Size([32, 7])\n",
      "Min Mask Shape:    torch.Size([32, 7, 1440])\n"
     ]
    }
   ],
   "source": [
    "# 1. Instantiate the Dataset\n",
    "dataset = FitbitEMADataset(X_final, Y_final)\n",
    "\n",
    "# 2. Create the Loader\n",
    "train_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=32,      # The B in your dimensions\n",
    "    shuffle=True,       # Shuffle only for Training!\n",
    "    num_workers=0       # Set to 2 or 4 if using Linux/Colab\n",
    ")\n",
    "\n",
    "# --- Test it out ---\n",
    "# Let's grab one batch to verify shapes\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "print(\"Input Batch Shape:\", batch['x'].shape)          # Should be [32, 7, 1440, 1]\n",
    "print(\"Label Batch Shape:\", batch['y'].shape)          # Should be [32, 4]\n",
    "print(\"Day Mask Shape:   \", batch['day_mask'].shape)   # Should be [32, 7]\n",
    "print(\"Min Mask Shape:   \", batch['min_mask'].shape)   # Should be [32, 7, 1440]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19b750d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Samples: 9058\n",
      "Testing Samples:  2163\n",
      "Unique Users in Train: 18\n",
      "Unique Users in Test:  5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Setup your lists (assuming you have these)\n",
    "# X_list: List of (7, 1440, 1) arrays\n",
    "# Y_list: List of (4,) arrays\n",
    "# groups: List of user_ids corresponding to each sample (e.g. ['P1', 'P1', 'P2'...])\n",
    "\n",
    "X_all = np.stack(X_list).astype(np.float32)\n",
    "Y_all = np.array(Y_list).astype(np.float32)\n",
    "groups = np.array(sub_list) \n",
    "\n",
    "# 2. Perform the Split\n",
    "# We use GroupShuffleSplit to ensure users don't leak across sets\n",
    "splitter = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, test_idx = next(splitter.split(X_all, Y_all, groups))\n",
    "\n",
    "# 3. Create the final arrays\n",
    "X_train, X_test = X_all[train_idx], X_all[test_idx]\n",
    "Y_train, Y_test = Y_all[train_idx], Y_all[test_idx]\n",
    "\n",
    "print(f\"Training Samples: {len(X_train)}\")\n",
    "print(f\"Testing Samples:  {len(X_test)}\")\n",
    "print(f\"Unique Users in Train: {len(np.unique(groups[train_idx]))}\")\n",
    "print(f\"Unique Users in Test:  {len(np.unique(groups[test_idx]))}\")\n",
    "\n",
    "# 4. Create your Datasets/Loaders (using the class we defined earlier)\n",
    "train_dataset = FitbitEMADataset(X_train, Y_train)\n",
    "test_dataset  = FitbitEMADataset(X_test, Y_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader  = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fdf110",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ==========================================\n",
    "# 1. THE POSITION \"BADGE\" FACTORY\n",
    "# ==========================================\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        # Create a matrix of [max_len, d_model] to hold the badges\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # Calculate the \"speed\" of the clock hands\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        # Fill even columns with Sin, odd columns with Cos\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Add batch dimension: (1, max_len, d_model)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        # Register as part of model state, but not learnable\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (Batch, Seq_Len, Dim)\n",
    "        # Slice the 'pe' matrix to match the length of x and add it\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "# ==========================================\n",
    "# 2. THE MAIN HIERARCHICAL MODEL\n",
    "# ==========================================\n",
    "class HierarchicalPAT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # --- Config ---\n",
    "        self.d_model = 64\n",
    "        self.nhead = 4\n",
    "        self.dim_feedforward = 128\n",
    "        \n",
    "        # --- LEVEL 1: MINUTE ENCODER (PAT) ---\n",
    "        self.input_proj = nn.Linear(1, self.d_model) # 1 HR val -> 64 features\n",
    "        self.min_pos_encoder = PositionalEncoding(self.d_model, max_len=1441)\n",
    "        self.min_cls_token = nn.Parameter(torch.randn(1, 1, self.d_model)) # The Day Secretary\n",
    "        \n",
    "        min_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.d_model, \n",
    "            nhead=self.nhead, \n",
    "            dim_feedforward=self.dim_feedforward, \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.min_transformer = nn.TransformerEncoder(min_layer, num_layers=2)\n",
    "\n",
    "        # --- LEVEL 2: DAY ENCODER (History Aggregator) ---\n",
    "        self.day_pos_encoder = PositionalEncoding(self.d_model, max_len=8)\n",
    "        self.day_cls_token = nn.Parameter(torch.randn(1, 1, self.d_model)) # The Week Secretary\n",
    "        \n",
    "        day_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.d_model, nhead=self.nhead, \n",
    "            dim_feedforward=self.dim_feedforward, batch_first=True\n",
    "        )\n",
    "        self.day_transformer = nn.TransformerEncoder(day_layer, num_layers=2)\n",
    "\n",
    "        # --- LEVEL 3: HEAD ---\n",
    "        self.head = nn.Linear(self.d_model, 4) # Predict Q1, Q2, Q3, Q4\n",
    "\n",
    "    def forward(self, x, min_mask, day_mask):\n",
    "        \"\"\"\n",
    "        x: (Batch, 7, 1440, 1)\n",
    "        min_mask: (Batch, 7, 1440)  -> True where minute is missing\n",
    "        day_mask: (Batch, 7)        -> True where day is missing\n",
    "        \"\"\"\n",
    "        batch_size, num_days, num_mins, _ = x.shape\n",
    "        \n",
    "        # ---------------------------------------------------------\n",
    "        # STAGE 1: Process Minutes (Flatten Batch & Days)\n",
    "        # ---------------------------------------------------------\n",
    "        x_flat = x.view(batch_size * num_days, num_mins, 1)         # (224, 1440, 1)\n",
    "        min_mask_flat = min_mask.view(batch_size * num_days, num_mins)\n",
    "        \n",
    "        # Project & Add CLS\n",
    "        x_emb = self.input_proj(x_flat)                             # (224, 1440, 64)\n",
    "        cls_tokens = self.min_cls_token.expand(batch_size * num_days, -1, -1)\n",
    "        x_emb = torch.cat((cls_tokens, x_emb), dim=1)               # (224, 1441, 64)\n",
    "        \n",
    "        # Add Time Badges (Minute 0 vs Minute 500)\n",
    "        x_emb = self.min_pos_encoder(x_emb)\n",
    "        \n",
    "        # Update Mask for CLS (Always keep Index 0)\n",
    "        cls_mask_col = torch.zeros((batch_size * num_days, 1), dtype=torch.bool, device=x.device)\n",
    "        min_mask_with_cls = torch.cat((cls_mask_col, min_mask_flat), dim=1)\n",
    "        \n",
    "        # Run Minute Transformer\n",
    "        day_embeddings = self.min_transformer(x_emb, src_key_padding_mask=min_mask_with_cls)\n",
    "        \n",
    "        # Extract Summary (Index 0)\n",
    "        day_vectors = day_embeddings[:, 0, :]                       # (224, 64)\n",
    "        \n",
    "        # ---------------------------------------------------------\n",
    "        # STAGE 2: Process Days (Un-flatten & Aggregate)\n",
    "        # ---------------------------------------------------------\n",
    "        day_vectors = day_vectors.view(batch_size, num_days, self.d_model) # (32, 7, 64)\n",
    "        \n",
    "        # Add CLS Token for the Week\n",
    "        cls_tokens_day = self.day_cls_token.expand(batch_size, -1, -1)\n",
    "        day_seq = torch.cat((cls_tokens_day, day_vectors), dim=1)   # (32, 8, 64)\n",
    "        \n",
    "        # Add Recency Badges (Yesterday vs Last Week)\n",
    "        day_seq = self.day_pos_encoder(day_seq)\n",
    "        \n",
    "        # Update Mask for CLS (Always keep Index 0)\n",
    "        cls_day_mask = torch.zeros((batch_size, 1), dtype=torch.bool, device=x.device)\n",
    "        day_mask_with_cls = torch.cat((cls_day_mask, day_mask), dim=1)\n",
    "        \n",
    "        # Run Day Transformer\n",
    "        final_seq = self.day_transformer(day_seq, src_key_padding_mask=day_mask_with_cls)\n",
    "        \n",
    "        # Extract Final Summary (Index 0)\n",
    "        final_vector = final_seq[:, 0, :]                           # (32, 64)\n",
    "        \n",
    "        # ---------------------------------------------------------\n",
    "        # STAGE 3: Prediction\n",
    "        # ---------------------------------------------------------\n",
    "        logits = self.head(final_vector)                            # (32, 4)\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a4b8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape:  torch.Size([32, 7, 1440, 1])\n",
      "Output Shape: torch.Size([32, 4])\n",
      "Success! The pipes are connected.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 4. QUICK TEST RUN\n",
    "# ==========================================\n",
    "\n",
    "# Create Dummy Data (Batch of 32, 7 days, 1440 mins)\n",
    "dummy_x = np.random.randn(32, 7, 1440, 1).astype(np.float32)\n",
    "# Simulate missing data (set some zeros)\n",
    "dummy_x[:, 3:, :, :] = 0  # Last 4 days are missing for everyone\n",
    "dummy_y = np.random.randint(0, 2, (32, 4)).astype(np.float32)\n",
    "\n",
    "# Init Dataset & Loader\n",
    "dataset = FitbitEMADataset(dummy_x, dummy_y)\n",
    "loader = DataLoader(dataset, batch_size=32)\n",
    "batch = next(iter(loader))\n",
    "\n",
    "# Init Model\n",
    "model = HierarchicalPAT()\n",
    "\n",
    "# Run Forward Pass\n",
    "logits = model(batch['x'], batch['min_mask'], batch['day_mask'])\n",
    "\n",
    "print(\"Input Shape: \", batch['x'].shape)     # [32, 7, 1440, 1]\n",
    "print(\"Output Shape:\", logits.shape)         # [32, 4]\n",
    "print(\"Success! The pipes are connected.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crave",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
