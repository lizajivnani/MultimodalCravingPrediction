{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96df9b2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#mounts from google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789008ac",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#cell for data loading\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "csv_path = '/content/structural-survey/CRAVE_scores.csv'\n",
    "structural_dir = '/content/structural-survey/Structural'\n",
    "\n",
    "if os.path.exists(csv_path) and os.path.exists(structural_dir):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    #get all .nii files\n",
    "    nii_files = [f for f in os.listdir(structural_dir) if f.endswith('.nii')]\n",
    "\n",
    "    #map subject ids to file paths\n",
    "    subject_to_path = {}\n",
    "    for filename in nii_files:\n",
    "        \n",
    "        temp_name = filename.replace('wc0c', '', 1) if filename.startswith('wc0c') else filename\n",
    "        #split at 1st underscore\n",
    "        subject_id = temp_name.split('_')[0]\n",
    "        subject_to_path[subject_id] = os.path.join(structural_dir, filename)\n",
    "\n",
    "    #find column corresponding to subj ids\n",
    "    matched_col = None\n",
    "    extracted_ids_set = set(subject_to_path.keys())\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_values = set(df[col].astype(str))\n",
    "        if len(col_values.intersection(extracted_ids_set)) > 0:\n",
    "            matched_col = col\n",
    "            break\n",
    "\n",
    "    if matched_col:\n",
    "        df = df[df[matched_col].astype(str).isin(extracted_ids_set)].copy()\n",
    "        df['file_path'] = df[matched_col].astype(str).map(subject_to_path)\n",
    "        for col in ['CAMS', 'QIDS', 'GAD']:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "        df['CAMS_binary'] = (df['CAMS'] >= 26).astype(int)\n",
    "        df['QIDS_binary'] = (df['QIDS'] >= 11).astype(int)\n",
    "        df['GAD_binary'] = (df['GAD'] >= 11).astype(int)\n",
    "\n",
    "        print(\"Data loaded and targets created successfully. (Prints hidden)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434ef733",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## ----------***This is not used****-----------\n",
    "#pca analysis\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import recall_score # Ensure recall_score is imported\n",
    "\n",
    "#standardize\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# no truncation to see variance curve\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_scaled)\n",
    "\n",
    "#visualize explained variance\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(np.cumsum(pca_full.explained_variance_ratio_), marker='o', linestyle='--')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('PCA: Explained Variance by Components')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "#apply pca w 95% of variance\n",
    "pca = PCA(n_components=0.95)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"Original Feature Shape: {X.shape}\")\n",
    "print(f\"Reduced PCA Shape: {X_pca.shape}\")\n",
    "print(f\"Kept {X_pca.shape[1]} components to explain 95% of variance.\")\n",
    "\n",
    "#re-train classifiers on pca data\n",
    "print(\"\\n--- Retraining on PCA Features ---\")\n",
    "\n",
    "results_store_pca = {}\n",
    "\n",
    "for target in targets:\n",
    "    print(f\"\\n=== Target: {target} (PCA) ===\")\n",
    "    results_store_pca[target] = {}\n",
    "\n",
    "    if target not in df_filtered.columns:\n",
    "        continue\n",
    "\n",
    "    y = df_filtered[target].values\n",
    "\n",
    "    for name, clf in classifiers.items():\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        try:\n",
    "            y_pred = cross_val_predict(clf, X_pca, y, cv=cv)\n",
    "\n",
    "            acc = accuracy_score(y, y_pred)\n",
    "            f1 = f1_score(y, y_pred, zero_division=0)\n",
    "            recall = recall_score(y, y_pred, zero_division=0)\n",
    "            cm = confusion_matrix(y, y_pred)\n",
    "\n",
    "            print(f\"{name}: Accuracy={acc:.4f}, F1={f1:.4f}, Recall={recall:.4f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error {name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95e1e84",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#load model and extract features\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import nibabel as nib\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, confusion_matrix\n",
    "from google.colab import drive\n",
    "\n",
    "#must install alive_progress dependency for this to work\n",
    "required = {'alive_progress'}\n",
    "installed = {pkg.key for pkg in pkg_resources.working_set}\n",
    "missing = required - installed\n",
    "if missing:\n",
    "    print(\"Installing missing packages:\", missing)\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', *missing])\n",
    "\n",
    "base_search_path = '/content/'\n",
    "zip_path = '/content/gdrive/MyDrive/structural-survey.zip'\n",
    "\n",
    "#debugging\n",
    "key_file = 'AD_pretrained_utilities.py'\n",
    "file_found = False\n",
    "for root, dirs, files in os.walk(base_search_path):\n",
    "    if key_file in files:\n",
    "        file_found = True\n",
    "        break\n",
    "\n",
    "if not file_found:\n",
    "    print(\"Dataset files not found. Attempting to restore...\")\n",
    "    if not os.path.exists('/content/gdrive'):\n",
    "        print(\"Mounting Google Drive...\")\n",
    "        drive.mount('/content/gdrive')\n",
    "\n",
    "    if os.path.exists(zip_path):\n",
    "        print(f\"Unzipping '{zip_path}'...\")\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(base_search_path)\n",
    "        print(\"Unzip complete.\")\n",
    "    else:\n",
    "        print(f\"Warning: Zip file not found at {zip_path}.\")\n",
    "\n",
    "#model utilities\n",
    "model_dir = None\n",
    "for root, dirs, files in os.walk(base_search_path):\n",
    "    if key_file in files:\n",
    "        model_dir = root\n",
    "        print(f\"Found {key_file} in {model_dir}\")\n",
    "        break\n",
    "\n",
    "if model_dir:\n",
    "    if model_dir not in sys.path:\n",
    "        sys.path.append(model_dir)\n",
    "        print(f\"Added {model_dir} to sys.path\")\n",
    "else:\n",
    "    raise RuntimeError(f\"Could not locate {key_file} in {base_search_path}\")\n",
    "\n",
    "if 'X' not in globals() or 'df_filtered' not in globals():\n",
    "    print(\"Variables 'X' or 'df_filtered' missing. Regenerating...\")\n",
    "    csv_path = None\n",
    "    structural_dir = None\n",
    "\n",
    "    for root, dirs, files in os.walk(base_search_path):\n",
    "        if 'CRAVE_scores.csv' in files:\n",
    "            csv_path = os.path.join(root, 'CRAVE_scores.csv')\n",
    "        if 'Structural' in dirs:\n",
    "            structural_dir = os.path.join(root, 'Structural')\n",
    "\n",
    "    if csv_path and structural_dir:\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        nii_files = [f for f in os.listdir(structural_dir) if f.endswith('.nii')]\n",
    "        subject_to_path = {}\n",
    "        for fname in nii_files:\n",
    "            sid = fname.replace('wc0c', '', 1).split('_')[0] if fname.startswith('wc0c') else fname.split('_')[0]\n",
    "            subject_to_path[sid] = os.path.join(structural_dir, fname)\n",
    "\n",
    "        #filter\n",
    "        extracted_ids = set(subject_to_path.keys())\n",
    "        matched_col = None\n",
    "        for col in df.columns:\n",
    "            if len(set(df[col].astype(str)).intersection(extracted_ids)) > 0:\n",
    "                matched_col = col\n",
    "                break\n",
    "\n",
    "        if matched_col:\n",
    "            df = df[df[matched_col].astype(str).isin(extracted_ids)].copy()\n",
    "            df['file_path'] = df[matched_col].astype(str).map(subject_to_path)\n",
    "\n",
    "            for c, thresh in [('CAMS', 26), ('QIDS', 11), ('GAD', 11)]:\n",
    "                if c in df.columns:\n",
    "                    df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "                    df[f'{c}_binary'] = (df[c] >= thresh).astype(int)\n",
    "\n",
    "            #run model\n",
    "            try:\n",
    "                from AD_pretrained_utilities import CNN, CNN_8CL_B\n",
    "                device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                weights_path = os.path.join(model_dir, 'AD_pretrained_weights.pt')\n",
    "\n",
    "                if os.path.exists(weights_path):\n",
    "                    param = CNN_8CL_B()\n",
    "                    model = CNN(param)\n",
    "                    model.load_state_dict(torch.load(weights_path, map_location=device))\n",
    "                    model.to(device)\n",
    "                    model.eval()\n",
    "\n",
    "                    features_list = []\n",
    "                    def hook(module, input, output):\n",
    "                        features_list.append(input[0].detach().cpu().numpy())\n",
    "                    if hasattr(model, 'f') and len(model.f) > 0:\n",
    "                        model.f[-1].register_forward_hook(hook)\n",
    "\n",
    "                    X_list = []\n",
    "                    valid_indices = []\n",
    "                    target_shape = (73, 96, 96)\n",
    "\n",
    "                    print(f\"Processing {len(df)} subjects...\")\n",
    "                    for idx, row in df.iterrows():\n",
    "                        try:\n",
    "                            img = nib.load(row['file_path'])\n",
    "                            data = img.get_fdata()\n",
    "                            data = np.nan_to_num(data)\n",
    "                            if data.shape != target_shape:\n",
    "                                zoom = [t/s for t, s in zip(target_shape, data.shape)]\n",
    "                                data = ndimage.zoom(data, zoom, order=1)\n",
    "                            if data.max() > data.min():\n",
    "                                data = (data - data.min()) / (data.max() - data.min())\n",
    "\n",
    "                            inp = torch.from_numpy(data).float().unsqueeze(0).unsqueeze(0).to(device)\n",
    "                            features_list = []\n",
    "                            with torch.no_grad():\n",
    "                                _ = model(inp)\n",
    "                            if features_list:\n",
    "                                X_list.append(features_list[0].flatten())\n",
    "                                valid_indices.append(idx)\n",
    "                        except Exception:\n",
    "                            pass\n",
    "\n",
    "                    if X_list:\n",
    "                        X = np.vstack(X_list)\n",
    "                        df_filtered = df.loc[valid_indices].copy()\n",
    "                        print(f\"X shape: {X.shape}\")\n",
    "                    else:\n",
    "                        raise RuntimeError(\"No features extracted\")\n",
    "                else:\n",
    "                    raise RuntimeError(\"Weights not found\")\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Feature extraction failed: {e}\")\n",
    "        else:\n",
    "            raise RuntimeError(\"No matching subject IDs\")\n",
    "    else:\n",
    "        raise RuntimeError(\"CSV or Structural dir not found\")\n",
    "\n",
    "#training\n",
    "print(\"\\nTraining...\")\n",
    "classifiers = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'SVM': SVC(kernel='linear', random_state=42),\n",
    "    'MLP': MLPClassifier(hidden_layer_sizes=(64,), max_iter=1000, random_state=42)\n",
    "}\n",
    "results_store = {}\n",
    "targets = ['CAMS_binary', 'QIDS_binary', 'GAD_binary']\n",
    "\n",
    "if 'X' in globals() and 'df_filtered' in globals():\n",
    "    for target in targets:\n",
    "        print(f\"\\nTarget: {target}\")\n",
    "        results_store[target] = {}\n",
    "        if target in df_filtered.columns:\n",
    "            y = df_filtered[target].values\n",
    "            if len(y) >= 5:\n",
    "                for name, clf in classifiers.items():\n",
    "                    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "                    try:\n",
    "                        y_pred = cross_val_predict(clf, X, y, cv=cv)\n",
    "                        acc = accuracy_score(y, y_pred)\n",
    "                        f1 = f1_score(y, y_pred, zero_division=0)\n",
    "                        recall = recall_score(y, y_pred, zero_division=0)\n",
    "                        cm = confusion_matrix(y, y_pred)\n",
    "                        results_store[target][name] = {'accuracy': acc, 'f1_score': f1, 'recall': recall}\n",
    "                        print(f\"{name}: Acc={acc:.4f}, F1={f1:.4f}, Recall={recall:.4f}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error {name}: {e}\")\n",
    "            else:\n",
    "                print(\"Not enough data\")\n",
    "else:\n",
    "    print(\"Data missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8be20f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#feature selection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "#rf, logreg, mlp\n",
    "req_models = {\n",
    "    'Statistical (LogReg)': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'MLP': MLPClassifier(hidden_layer_sizes=(64,), max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "print(\"\\n--- Running Feature Importance Selection ---\\n\")\n",
    "\n",
    "results_store_selection = {}\n",
    "\n",
    "for target in targets:\n",
    "    print(f\"\\n=== Target: {target} ===\")\n",
    "    results_store_selection[target] = {}\n",
    "\n",
    "    if target not in df_filtered.columns:\n",
    "        continue\n",
    "\n",
    "    y = df_filtered[target].values\n",
    "\n",
    "    #find feature importance using rf\n",
    "    #fit new rf for selection\n",
    "    selector_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    selector_clf.fit(X, y)\n",
    "\n",
    "    #importance scores, sorted descending\n",
    "    importances = selector_clf.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    #top 5 features for visualization\n",
    "    top_k = 5\n",
    "    top_indices = indices[:top_k]\n",
    "    print(f\"Top {top_k} Features selected: {top_indices}\")\n",
    "\n",
    "    #new X w/ only top 5 features\n",
    "    X_selected = X[:, top_indices]\n",
    "\n",
    "    #train models on these features\n",
    "    for name, clf in req_models.items():\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        try:\n",
    "            y_pred = cross_val_predict(clf, X_selected, y, cv=cv)\n",
    "\n",
    "            #metrics\n",
    "            acc = accuracy_score(y, y_pred)\n",
    "            f1 = f1_score(y, y_pred, zero_division=0)\n",
    "            recall = recall_score(y, y_pred, zero_division=0)\n",
    "\n",
    "            print(f\"{name}: Accuracy={acc:.4f}, F1={f1:.4f}, Recall={recall:.4f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error {name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc6fb8f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#important\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, recall_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "classifiers = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'SVM': SVC(kernel='linear', random_state=42),\n",
    "    'MLP': MLPClassifier(hidden_layer_sizes=(64,), max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "targets = ['CAMS_binary', 'QIDS_binary', 'GAD_binary']\n",
    "metrics_list = []\n",
    "\n",
    "if 'X' in globals() and 'df_filtered' in globals():\n",
    "    #3x3 confusion matrix\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "    plt.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "    for i, target in enumerate(targets):\n",
    "        if target not in df_filtered.columns:\n",
    "            print(f\"Target {target} missing.\")\n",
    "            continue\n",
    "\n",
    "        y = df_filtered[target].values\n",
    "\n",
    "        #debugging - skip if not enough samples\n",
    "        if len(y) < 5:\n",
    "             print(f\"Not enough samples for {target}\")\n",
    "             continue\n",
    "\n",
    "        for j, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "            ax = axes[i, j]\n",
    "\n",
    "            #cross-validation predictions\n",
    "            cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            try:\n",
    "                y_pred = cross_val_predict(clf, X, y, cv=cv)\n",
    "\n",
    "                #metrics\n",
    "                acc = accuracy_score(y, y_pred)\n",
    "                f1 = f1_score(y, y_pred, zero_division=0)\n",
    "                rec = recall_score(y, y_pred, zero_division=0)\n",
    "\n",
    "                metrics_list.append({\n",
    "                    'Target': target,\n",
    "                    'Model': clf_name,\n",
    "                    'Accuracy': acc,\n",
    "                    'F1 Score': f1,\n",
    "                    'Recall': rec\n",
    "                })\n",
    "\n",
    "                #confusion matrix\n",
    "                cm = confusion_matrix(y, y_pred)\n",
    "                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, cbar=False,\n",
    "                            xticklabels=['Low', 'High'], yticklabels=['Low', 'High'])\n",
    "                ax.set_title(f'{target}\\n{clf_name}')\n",
    "                ax.set_ylabel('True Label')\n",
    "                ax.set_xlabel('Predicted Label')\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {clf_name} for {target}: {e}\")\n",
    "                ax.text(0.5, 0.5, \"Error\", ha='center', va='center')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    #metrics df\n",
    "    metrics_df = pd.DataFrame(metrics_list)\n",
    "    print(\"\\nPerformance Metrics Summary:\")\n",
    "    display(metrics_df)\n",
    "\n",
    "'''\n",
    "    # comment this out, this was only for visualization at first\n",
    "    #f1 score bar chart\n",
    "    if not metrics_df.empty:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.barplot(data=metrics_df, x='Target', y='F1 Score', hue='Model', palette='viridis')\n",
    "        plt.title('Comparison of F1 Scores across Targets and Models')\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "'''\n",
    "else:\n",
    "    print(\"Data (X or df_filtered) is missing. Please run the previous feature extraction step.\") #debugging"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
